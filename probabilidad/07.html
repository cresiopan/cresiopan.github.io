<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-03-06 Wed 18:28 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Procesos de Poisson</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/res/org.css"/>
                                        <script type="text/javascript" src="/res/org-info.js">

<script type="text/javascript" src="/style/org-info.js">
/**
 *
 * @source: /style/org-info.js
 *
 * @licstart  The following is the entire license notice for the
 *  JavaScript code in /style/org-info.js.
 *
 * Copyright (C) 2012-2018 Free Software Foundation, Inc.
 *
 *
 * The JavaScript code in this tag is free software: you can
 * redistribute it and/or modify it under the terms of the GNU
 * General Public License (GNU GPL) as published by the Free Software
 * Foundation, either version 3 of the License, or (at your option)
 * any later version.  The code is distributed WITHOUT ANY WARRANTY;
 * without even the implied warranty of MERCHANTABILITY or FITNESS
 * FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.
 *
 * As additional permission under GNU GPL version 3 section 7, you
 * may distribute non-source (e.g., minimized or compacted) forms of
 * that code without the copy of the GNU GPL normally required by
 * section 4, provided you include this license notice and a URL
 * through which recipients can access the Corresponding Source.
 *
 * @licend  The above is the entire license notice
 * for the JavaScript code in /style/org-info.js.
 *
 */
</script>

<script type="text/javascript">

/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/

<!--/*--><![CDATA[/*><!--*/
org_html_manager.set("TOC_DEPTH", "2");
org_html_manager.set("LINK_HOME", "0");
org_html_manager.set("LINK_UP", "0");
org_html_manager.set("LOCAL_TOC", "0");
org_html_manager.set("VIEW_BUTTONS", "0");
org_html_manager.set("MOUSE_HINT", "underline");
org_html_manager.set("FIXED_TOC", "0");
org_html_manager.set("TOC", "1");
org_html_manager.set("VIEW", "showall");
org_html_manager.setup();  // activate after the parameters are set
/*]]>*///-->
</script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Procesos de Poisson</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org13b8fa2">Proceso puntual de Poisson</a>
<ul>
<li><a href="#orgfe705a9">Procesos puntuales</a>
<ul>
<li>
<ul>
<li><a href="#orge773ddc">Definición 1.1 (Proceso puntual aleatorio)</a></li>
<li><a href="#org077164e">Observación 1.2.</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org3bb4c7e">Procesos de Poisson</a>
<ul>
<li>
<ul>
<li><a href="#orgca15f94">Definición 1.3 (Proceso de Poisson)</a></li>
<li><a href="#orgb944bbf">Nota Bene</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org85bc904">Construcción</a>
<ul>
<li>
<ul>
<li><a href="#orgc81c7ea">Teorema 1.4.</a></li>
<li><a href="#orgc3be120">Demostración</a></li>
<li><a href="#org736014f">Teorema 1.5</a></li>
<li><a href="#orga97b68d">Demostración</a></li>
<li><a href="#org30256b7">Ejemplo 1.6</a></li>
<li><a href="#orgb73a8fc">Ejercicios adicionales</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgc401cee">Distribución condicional de los tiempos de llegada</a>
<ul>
<li>
<ul>
<li><a href="#orgccfbd64">Teorema 1.7 (Propiedad condicional)</a></li>
<li><a href="#orgdf1a9c1">Demostración</a></li>
<li><a href="#orgb90a757">Nota Bene</a></li>
<li><a href="#orgd54c38b">Ejemplo 1.8 (Insectos en un asado)</a></li>
<li><a href="#org30d4d6d">Solución</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org1d02c7c">Coloración y adelgazamiento de procesos de Poisson</a>
<ul>
<li>
<ul>
<li><a href="#orgaedc17e">Teorema 1.9 (Coloración).</a></li>
<li><a href="#orgbc2cf71">Demostración</a></li>
<li><a href="#org131e6c6">Ejemplo 1.10 (Insectos en un asado)</a></li>
<li><a href="#orgb70d3d8">Ejercicios adicionales</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org283148c">Superposición de Procesos de Poisson: competencia</a>
<ul>
<li>
<ul>
<li><a href="#org8a6f1cb">Teorema 1.11 (Superposición)</a></li>
<li><a href="#org5180e2a">Demostración</a></li>
<li><a href="#org12be1f8">Lema 1.12</a></li>
<li><a href="#org8bd855c">Demostración</a></li>
<li><a href="#orgd79ed87">Teorema 1.13 (Competencia)</a></li>
<li><a href="#org0c20d7a">Demostración</a></li>
<li><a href="#org51d2d69">Ejemplo 1.14 (Insectos en un asado)</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org071df0b">Procesos de Poisson compuestos</a>
<ul>
<li>
<ul>
<li><a href="#orge90cd1c">Lema 1.15</a></li>
<li><a href="#org63a0b67">Demostración</a></li>
<li><a href="#orga1d05f0">Ejemplo 1.16</a></li>
<li><a href="#org14c6454">Solución</a></li>
<li><a href="#orgda31c67">Ejercicios adicionales</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org143756b">Bibliografía consultada</a></li>
</ul>
</div>
</div>
<div id="outline-container-org13b8fa2" class="outline-2">
<h2 id="org13b8fa2">Proceso puntual de Poisson</h2>
<div class="outline-text-2" id="text-org13b8fa2">
</div>
<div id="outline-container-orgfe705a9" class="outline-3">
<h3 id="orgfe705a9">Procesos puntuales</h3>
<div class="outline-text-3" id="text-orgfe705a9">
<p>
Informalmente, un proceso puntual aleatorio es un conjunto enumerable
de puntos al eato rios ubicados sobre la recta real. En la mayoría de
las aplicaciones un punto de un proceso puntual es el instante en que
ocurre algún evento, motivo por el cual los puntos también se llaman
eventos o arribos. Por ejemplo, los tiempos de arribo de clientes a la
caja de un supermercado o de los trabajos al procesador central de una
computadora son procesos puntuales.  En teoría fiabilidad, un evento
podría ser el instante en que ocurre una falla. El ejemplo básico de
este tipo de procesos es el proceso de Poisson.
</p>
</div>
<div id="outline-container-orge773ddc" class="outline-5">
<h5 id="orge773ddc">Definición 1.1 (Proceso puntual aleatorio)</h5>
<div class="outline-text-5" id="text-orge773ddc">
<p>
Un proceso puntual aleatorio sobre la semirecta positiva es una sucesión \{S
n
</p>
<pre class="example">
n \geq 0{\} de variables aleato rias no negativas tales que, casi seguramente,

</pre>
<p>
(a) S}
0
≡ 0,}
(b) 0 &lt; S}
1
&lt; S
2
&lt; · · · , 
(c) lim
n{&rarr;&infin;}
S
n
= +{&infin;}.
</p>

<p>
La condición (b) significa que no hay arribos simultáneos. La condición (c) significa que
no hay explosiones, esto es, no hay una acumulación de arribos en tiempos finitos.
La sucesión de variables aleatorias \{T
n
</p>
<pre class="example">
n \geq 1{\} definida por

</pre>
<p>
T
n
:= S}
n
− S}
n{−{1
(1)
se llama la sucesión de tiempos de espera entre arribos.
Introducimos una familia de nuevas variables aleatorias N(t), t &ge; 0, de la siguiente manera:
para cada t &ge; 0 definimos N(t) como la cantidad de arribos ocurridos durante el intervalo de
tiempo (0, t],
N ( t) :=}
X
n &ge; 1
1\{S
n
&le; t\} (2)
= máx\{n &ge; 0 : S}
n
&le; t\. (3)}
2
1
2
3
4
5
t
N ( t ) 
S
5
S
4
S
3
S
2
S
1
T
2
T
3
T
1
T
4
T
5
Figura 1: Realización típica de un proceso puntual aleatorio sobre la semi-recta positiva.
</p>
</div>
</div>
<div id="outline-container-org077164e" class="outline-5">
<h5 id="org077164e">Observación 1.2.</h5>
<div class="outline-text-5" id="text-org077164e">
<p>
Notar que N ( t ) es una función de t y de las variables aleatori as T
1
, T
2
, &hellip;
a valores enteros no negativos. Indicaremos esa relación de la siguiente manera
N ( t) = Ψ(t{; T
1
, T
2
, &hellip;  ) , (4)
donde Ψ es la relación definida en (2).
La cantidad de arribos ocurridos durante el intervalo de tiempo (s, t] &sub; R
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
, N(s, t], es el
incremento N(t) − N}(s)
N ( s, t] := N ( t) − N ( s) =}
X
n &ge; 1
1\{s &lt; S
n
&le; t\. (5)}
De (3) se obtiene la relación básica que conecta a las variables N(t) con las S}
n
</p>
<pre class="example">


</pre>
<p>
N ( t) &ge; n \iff S
n
&le; t. (6)}
De allí se desprende que
N ( t) = n \iff S
n
&le; t &lt; S}
{n+1}
. (7)
Proceso de conteo. La familia de variables aleatorias \{N}(t) : t &ge; 0{\} es un proceso es
tocástico denominado el proceso de conteo de la sucesión de arribos \{S }
n
</p>
<pre class="example">
n \geq 0{\}. Debido a que

</pre>
<p>
la sucesión de arribos se puede reconstruir a partir de N , N también recibe la denominación
/"{proceso puntual ''.}
Propiedades. Por definición, el proceso de conteo satisface las siguientes propiedades:}
(i) Para cada t &ge; 0, la variable aleatoria N(t) tiene valores enteros no negativos.
(ii) N(0) = 0 y lim
t{&rarr;&infin;}
N ( t) = &infin;}.
3
(iii) Si s &lt; t, entonces N (s) &le; N}(t).
(iv) Como el intervalo (0, t] es cerrado a la derecha, la función (aleatoria) N : \Re
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
&rarr; N}
0
es continua a derecha. Además, en los puntos de discontinuidad tiene saltos de longitud 1.
En otras palabras, el gráfico de la función aleatoria N : \Re
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
&rarr; N}
0
es una escalera no
decreciente, continua a derecha y con saltos de longitud 1 en cada uno de los arribos del
proceso puntual.
Programa. En lo que sigue estudiaremos la distribución conjunta de las N(t) bajo ciertas}
condiciones sobre los tiempos de espera entre arribos T}
n
y vice versa.
</p>
</div>
</div>
</div>
<div id="outline-container-org3bb4c7e" class="outline-3">
<h3 id="org3bb4c7e">Procesos de Poisson</h3>
<div class="outline-text-3" id="text-org3bb4c7e">
<p>
Existen varias definiciones equivalentes de procesos de Poisson. Adoptamos la que nos
parece más senci lla y generalizable.
1
</p>
</div>
<div id="outline-container-orgca15f94" class="outline-5">
<h5 id="orgca15f94">Definición 1.3 (Proceso de Poisson)</h5>
<div class="outline-text-5" id="text-orgca15f94">
<p>
Un proceso puntual \{S
n
</p>
<pre class="example">
n \geq 0{\} sobre la semi-recta

</pre>
<p>
positiva es un proceso de Poisson de intensidad &lambda; &gt; 0 si satisface las siguientes condiciones
(i) El proceso tiene incrementos independientes{: para cada colección finita de tiempos 0 =
t
0
&lt; t
1
&lt; · · · &lt; t
n
, los incrementos N(t
i{−{1
, t
i
] = N(t
i
) − N}(t
i{−{1
), i = 1, &hellip; , n son
independientes.
(ii) Los incrementos individuales N ( s, t] = N ( t) − N ( s) tienen la distribución Poisson: 
\mathbb{P}(N(s, t] = n) = e}
−{&lambda; ( t}−{s ) 
( &lambda; (t − s))
n
n{!}
, n = 0, 1, &hellip; , 0 &le; s &lt; t. (8)
</p>
</div>
</div>
<div id="outline-container-orgb944bbf" class="outline-5">
<h5 id="orgb944bbf">Nota Bene</h5>
<div class="outline-text-5" id="text-orgb944bbf">
<p>
La condición (ii) de la Definición 1.3 se puede descomponer en dos partes.
(a) Los incrementos son temporalmente homogéneos (i.e., la distribución de los incrementos
depende solamente de la longitud del intervalo de tiempo pero no de su posición) y (b) la}
distribución de cada incremento individual es Poisson de media proporcional a la cantidad de
tiempo considerado.
Que un proceso puntual sea temporalmente homogéneo y que tenga incrementos independi
entes significa que si se lo reinicia desde cualquier instante de tiempo t, el proceso así obtenido}
es independiente de todo lo que ocurrió previamente (por tener incrementos independientes)
y que tiene la misma distribución que el proceso original (por ser temporalmente homogéneo).
En otras palabras, el proceso no tiene memoria.
Es de suponer que, bajo esas condiciones, los tiempos de espera entre arribos tienen
que ser variables aleatorias independientes, cada una con distribución exponencial del mismo
parámetro.
´
Esto último es consistente con la condición sobre la distribución que tienen los
incrementos individuales (8).
1
Elegimos la Definición 1.3 porque tiene la virtud de que se puede extender a R}
d
sin ninguna dificultad:
un subconjunto aleatorio (numerable) &Pi; de R}
d
se llama un proceso de Poisson de intensidad &lambda; si, para todo 
A &isin; B(R 
d
), las variables aleatorias N (A) = | &Pi; &cap; A| satisfacen (a) N (A) tiene la distribución Poisson de
parámetro &lambda; | A | , y (b) Si A
1
, A
2
, &hellip; , A
n
&isin; B(R
d
) son conjuntos disjuntos, entonces N(A
1
), N(A
2
), &hellip; N (A
n
)
son variables aleatorias independientes.
4
En efecto, de la relación básica (6) se deduce que si \{S
n
</p>
<pre class="example">
n \geq 0{\} es un proceso de Poisson

</pre>
<p>
de intensidad &lambda;, entonces las variables S}
n
tienen distribución &Gamma;(n, &lambda;):
\mathbb{P}(S
n
&gt; t) = \mathbb{P}(N  ( t ) &lt; n) =}
n{−{1
X
{k=0}
\mathbb{P}(N(t) = k) =}
n{−{1
X
{k=0}
e
−{\lambdat}
(\lambdat)
k
k{!}
.
</p>
</div>
</div>
</div>
<div id="outline-container-org85bc904" class="outline-3">
<h3 id="org85bc904">Construcción</h3>
<div class="outline-text-3" id="text-org85bc904">
<p>
En lo que sigue mostraremos una forma de construir un proceso puntual de Poisson \{S
n
</p>
<pre class="example">


</pre>
<p>
n &ge; 0\} de intensidad &lambda;}. Los arribos, S 
n
, se construyen utilizando una sucesión de variables
aleatorias a valores positivos \{T
n
</p>
<pre class="example">
n \geq 1{\}:

</pre>
<p>
S
0
:= 0, S}
n
:=
n
X
{i=1}
T
i
, n = 1, 2, &hellip; . (9)
</p>
</div>
<div id="outline-container-orgc81c7ea" class="outline-5">
<h5 id="orgc81c7ea">Teorema 1.4.</h5>
<div class="outline-text-5" id="text-orgc81c7ea">
<p>
Sea \{T
n
</p>
<pre class="example">
n \geq 1{\} una sucesión de variables aleatorias independientes, cada

</pre>
<p>
una con distribución exponencial de intensidad &lambda;}. El proceso de arribos \{S
n
</p>
<pre class="example">
n \geq 0{\} de finido

</pre>
<p>
en (9) es un proceso puntual de Poisson de intensidad &lambda;}. (Ver la Definición 1.3).
</p>
</div>
</div>
<div id="outline-container-orgc3be120" class="outline-5">
<h5 id="orgc3be120">Demostración</h5>
<div class="outline-text-5" id="text-orgc3be120">
<ol class="org-ol">
<li>Proceso Puntual. Para cada n &ge; 1, \mathbb{P}(T</li>
</ol>
<p>
n
&gt; 0) = 1 y por la l ey fuerte de los grandes}
números
1
n
P
n
{i=1}
T
i
&rarr;
1
&lambda;
casi seguramente. Por lo tanto, \{S
n
</p>
<pre class="example">
n \geq 0{\} es un proceso puntual.

</pre>
<ol class="org-ol">
<li>Distribuciones Poisson. Para cada n &ge; 1, S</li>
</ol>
<p>
n
= T}
1
</p>
<ul class="org-ul">
<li>· · · + T}</li>
</ul>
<p>
n
tiene distribución &Gamma;(n, &lambda;):
F
S
n
(t) = \mathbb{P}(S}
n
&le; t) =
1 − e}
−{\lambdat}
n{−{1
X
{k=0}
(\lambdat)
k
k{!}
!
1\{t &ge; 0\} =
e
−{\lambdat}
&infin;
X
{k=n}
(\lambdat)
k
k{!}
!
1\{t &ge; 0\}.
Observando que \{N}(t) = n{\} = \{N}(t) &lt; n + 1{\}  &setminus;  \{N (t) &lt; n{\} y usando la relación básica,
N ( t ) &lt; n \iff S
n
&gt; t, se deduce que}
\mathbb{P}(N(t) = n) = \mathbb{P}(N(t) &lt; n + 1) − \mathbb{P}(N(t) &lt; n) = \mathbb{P}(S
{n+1}
&gt; t) − \mathbb{P}(S
n
&gt; t ) 
= e
−{\lambdat}
n
X
{k=0}
(\lambdat)
k
k{!}
− e
−{\lambdat}
n{−{1
X
{k=0}
(\lambdat)
k
k{!}
= e
−{\lambdat}
(\lambdat)
n
n{!}
, n = 0, 1, &hellip; . (10)
Por lo tanto, para cada t &gt; 0 fijo, el incremento N(t) tiene una distribución Poisson de media
\lambdat{:}
N ( t) &sim; P oisson ( \lambdat ) .
</p>
<ol class="org-ol">
<li>Pérdida de memoria. Fijamos t &gt; 0 y consideramos los arribos posteriores al instante t.</li>
</ol>
<p>
Por (3) tenemos que S}
N ( t ) 
&le; t &lt; S}
N ( t)+1}
. El tiemp o de espera desde t hasta el primer arribo
posterior a t es S}
N ( t)+1}
−{t{; el tiempo de espera entre el primer y el segundo arribo posteriores
a t es T}
N ( t)+2}
; y así siguiendo. De este modo
T
(t)
1
:= S}
N ( t)+1}
− t, T}
(t)
2
:= T}
N ( t)+2}
, T
(t)
3
:= T}
N ( t)+3}
, &hellip; (11)
5
definen los tiempos de espera entre arribos posteriores a t.
Debido a la independencia de las T}
k
y la propiedad de pérdida de memoria de la distribu
ción exponencial, parece intuitivamente claro que condicionando al evento \{N}(t) = n{\} las
variables aleatorias (11) son independientes y con distribución exponencial.
En lo que sigue mostraremos que N(t), T}
(t)
1
, T
(t)
2
, &hellip; son variables aleatorias independi
entes y que
(T}
(t)
1
, T
(t)
2
, &hellip; ) &sim; (T
1
, T
2
, &hellip;  ) . (12)
Basta mostrar que para to do n &ge; 0 y para toda elección de números positivos t
1
, &hellip; , t
m
,
m &isin; N, vale que 
\mathbb{P}(N(t) = n, T
(t)
1
&gt; t
1
, &hellip; , T
(t)
m
&gt; t
m
) = \mathbb{P}(N(t) = n)e
−{\lambdat}
1
· · · e
−{\lambdat}
m
. (13)
Para probarlo condicionaremos sobre la variable S}
n
,
\mathbb{P}(N(t) = n, T
(t)
1
&gt; t
1
) = \mathbb{P}(S}
n
&le; t &lt; S}
{n+1}
, S
{n+1}
− t &gt; t}
1
)
= \mathbb{P}(S}
n
&le; t, T}
{n+1}
&gt; t
1
</p>
<ul class="org-ul">
<li>t − S}</li>
</ul>
<p>
n
)
=
Z
t
0
\mathbb{P}(T
{n+1}
&gt; t
1
</p>
<ul class="org-ul">
<li>t − s)f</li>
</ul>
<p>
S
n
(s)ds}
= e
−{\lambdat}
1
Z
t
0
\mathbb{P}(T
{n+1}
&gt; t − s ) f
S
n
(s)ds}
= e
−{\lambdat}
1
\mathbb{P}(S
n
&le; t, T}
{n+1}
&gt; t − S
n
)
= \mathbb{P}(N(t) = n)e
−{\lambdat}
1
.
Para obtener la segunda igualdad hay que observar que \{S
{n+1}
&gt; t{\} &cap; &sect;
{n+1}
− t &gt; t}
1
\} =}
\{S}
{n+1}
&gt; t
1
</p>
<ul class="org-ul">
<li>t{\} y escribir S}</li>
</ul>
<p>
{n+1}
= S}
n
</p>
<ul class="org-ul">
<li>T}</li>
</ul>
<p>
{n+1}
; la tercera se obtiene condicionando sobre S}
n
; la
cuarta se obtiene usando la propiedad de pérdida de memoria de la exponencial (\mathbb{P}(T}
{n+1}
&gt;
t
1
</p>
<ul class="org-ul">
<li>t − s) = \mathbb{P}(T}</li>
</ul>
<p>
{n+1}
&gt; t
1
)\mathbb{P}(T}
{n+1}
&gt; t − s) = e
−{\lambdat}
1
\mathbb{P}(T
{n+1}
&gt; t − s)).
Por la independencia de las variables T}
n
,
\mathbb{P}(N(t) = n, T
(t)
1
&gt; t
1
, &hellip; , T
(t)
m
&gt; t
m
)
= \mathbb{P}(S}
n
&le; t &lt; S}
{n+1}
, S
{n+1}
− t &gt; t}
1
, T
n{+2}
&gt; t
2
, T
n{+}m
&gt; t
m
)
= \mathbb{P}(S}
n
&le; t &lt; S}
{n+1}
, S
{n+1}
− t &gt; t}
1
)e
−{\lambdat}
2
· · · e
−{\lambdat}
m
= \mathbb{P}(N(t) = n)e
−{\lambdat}
1
· · · e
−{\lambdat}
m
.
</p>
<ol class="org-ol">
<li>Incrementos estacionarios e independientes. Por (6), N(t + s) − N}(t) &ge; m, o N (t +}</li>
</ol>
<p>
s) &ge; N ( t) + m, si y solo si S
N ( t)+}m
&le; t + s, que es la misma cosa que T}
(t)
1
</p>
<ul class="org-ul">
<li>· · · + T}</li>
</ul>
<p>
(t)
m
&le; s. Así
N ( t + s) − N ( t) = máx\{m : T
(t)
1
</p>
<ul class="org-ul">
<li>· · · + T}</li>
</ul>
<p>
(t)
m
&le; s\. (14)}
Comparando (14) y (3) se puede ver que para t fijo las variables aleatorias N(t + s) − N}(t)
para s &ge; 0 se definen en términos de la sucesión (11) exactamente de la misma manera en
que las N(s) se definen en términos de la sucesión original de tiempos de espera. En otras
palabras,
N ( t + s) − N ( t) = Ψ(s{; T
(t)
1
, T
(t)
2
, &hellip;  ) , (15)
6
donde Ψ es la función definida en la Observación 4. De acuerdo con (12)
\{N ( t + s ) − N ( t) : s &ge; 0\} &sim; \{N  ( s) : s &ge; 0}\. (16)}
De (15) y lo visto en 3. se deduce que N(t) y \{N (t+s)−{N}(t) : s &ge; 0{\} son independientes.
Sean n &ge; 2 y 0 &lt; t}
1
&lt; t
2
&lt; &hellip; &lt; t
n
. Como (N (t
2
) − N}(t
1
), &hellip; , N (t
n
) − N}(t
n{−{1
)) es una
función de \{N (t
1
</p>
<ul class="org-ul">
<li>s) − N}(t</li>
</ul>
<p>
1
) : s &ge; 0{\, tenemos que
N ( t
1
) y (N(t
2
) − N}(t
1
), &hellip; , N (t
n
) − N}(t
n{−{1
))
son independientes. Esto es,
\mathbb{P}(N(t}
1
) = m
1
, N  ( t
2
) − N}(t
1
) = m
2
, &hellip; , N  ( t
n
) − N}(t
n{−{1
) = m
n
)
= \mathbb{P}(N(t
1
) = m
1
)\mathbb{P}(N(t
2
) − N}(t
1
) = m
2
, &hellip; , N  ( t
n
) − N}(t
n{−{1
) = m
n
)
En particular, se obtiene la la independencia de los incrementos para el caso en que n = 2:
\mathbb{P}(N(t}
1
) = m
1
, N  ( t
2
) − N}(t
1
) = m
2
) = \mathbb{P}(N(t
1
) = m
1
)\mathbb{P}(N(t
2
) − N}(t
1
) = m
2
).
Usando (16) se concluye que
(N(t
2
) − N}(t
1
), N (t
3
) − N}(t
2
), &hellip; , N (t
n
) − N}(t
n{−{1
))
&sim; (N(t}
2
− t
1
), N (t
3
− t
1
) − N}(t
2
− t
1
), &hellip; , N (t
n
− t
1
) − N}(t
n{−{1
− t
1
)). (17)
El caso general se obtiene por iteración del mismo argumento, aplicado al lado derecho de
(17):
\mathbb{P}(N(t}
2
) − N}(t
1
) = m
2
, N  ( t
k
) − N}(t
k{−{1
) = m
k
, 3 &le; k &le; n ) 
= \mathbb{P}(N(t
2
− t
1
) = m
2
, N  ( t
k
− t
1
) − N}(t
k{−{1
− t
1
) = m
k
, 3 &le; k &le; n ) 
= \mathbb{P}(N(t
2
− t
1
) = m
2
)\mathbb{P}(N(t
k
− t
1
) − N}(t
k{−{1
− t
1
) = m
k
, 3 &le; k &le; n ) 
= \mathbb{P}(N(t
2
) − N}(t
1
) = m
2
)\mathbb{P}(N(t
k
) − N}(t
k{−{1
) = m
k
, 3 &le; k &le; n ) 
= · · ·}
=
n
Y
{k=2}
\mathbb{P}(N(t}
k
) − N}(t
k{−{1
) = m
k
).
Por lo tanto, si 0 = t
0
&lt; t
1
&lt; · · · &lt; t
n
, entonces
\mathbb{P}(N(t}
k
) − N}(t
k{−{1
) = m
k
, 1 &le; k &le; n) =}
n
Y
{k=1}
\mathbb{P}(N(t}
k
− t
k{−{1
) = m
k
). (18)
De (18) y (10) se obtienen las dos condiciones que definen a un proceso de Poisson.
En lo que sigue mostraremos que vale la recíproca. Esto es, los tiempos de espera entre
arribos de un proceso de Poisson de intensidad &lambda; son variables aleatorias independientes cada
una con distribución exponencial de intensidad &lambda;}.
</p>
</div>
</div>
<div id="outline-container-org736014f" class="outline-5">
<h5 id="org736014f">Teorema 1.5</h5>
<div class="outline-text-5" id="text-org736014f">
<p>
Sea \{S
n
</p>
<pre class="example">
n \geq 0{\} un proceso puntual de Poisson de intensid ad \lambda sobre la semi}

</pre>
<p>
recta positiva. Los tiempos de espera entre arribos T}
n
, n &ge; 1 , definidos en (1), constituyen
una sucesión de variables aleatorias independientes cada una con distribución exponencial de
intensidad &lambda;}.
7
</p>
</div>
</div>
<div id="outline-container-orga97b68d" class="outline-5">
<h5 id="orga97b68d">Demostración</h5>
<div class="outline-text-5" id="text-orga97b68d">
<p>
La densidad conjunta de T = (T
1
, T
2
dots , T
n
) se obtendrá a partir de la
densidad conjunta de las variables S = (S}
1
, S
2
, &hellip; , S
n
) usando el método del Jacobiano. Por
definición,
(T}
1
, T
2
, &hellip; , T
n
) = g(S}
1
, S
2
, &hellip; , S
n
), 
donde g : G}
0
&rarr; G es la transformación lineal biyectiva entre los conjuntos abiertos G 
0
=
\(s}
1
, &hellip; , s
n
) &isin; \Re}
n
</p>
<pre class="example">
0 &lt; s}

</pre>
<p>
1
&lt; s
2
&lt; · · · &lt; s
n
\} y G = \(t}
1
, &hellip; , t
n
) : t
1
&gt; 0, &hellip; , t
n
&gt; 0{\} definida}
por
g ( s
1
, s
2
, &hellip; , s
n
) = (s
1
, s
2
− s
1
, &hellip; , s
n
− s
n{−{1
).
La función i nversa h = g
−{1}
es de la forma
h ( t
1
, &hellip; , t
n
) = (t
1
, t
1
</p>
<ul class="org-ul">
<li>t</li>
</ul>
<p>
2
, &hellip; , t
1
</p>
<ul class="org-ul">
<li>· · · + t</li>
</ul>
<p>
n
)
y sus derivadas parciales
\partials
i
\partialt
j
=
&part;
P
i
{k=1}
t
k
\partialt
j
= 1\{j &le; i\, 1 &le; i, j &le; n}
son continuas en G}. El jacobiano es
J(s, t) =





\partials
i
\partialt
j





= 1
debido a que se trata de una matriz triangular inferior con 1's en la diagonal. Bajo esas
condiciones tenemos que
f
T
(t) = f
S
(h(t))1{\t &isin; G\}.}
La densidad conjunta de las variables (S}
1
, &hellip; , S
2
) queda unívocamente determinada por la
relación
\mathbb{P}(S &isin; A}) =}
Z
A
f
S
(s)ds, A = (a
1
, b
1
] &times; · · · (a
n
, b
n
] &sub; G
0
.
Supongamos que 0 = b
0
&le; a
1
&lt; b
1
&lt; a
2
&lt; b
2
&lt; · · · &lt; a
n
&lt; b
n
y calculemos la probabilidad
del evento
T
n
{i=1}
\{a
i
&lt; S
i
&le; b
i
\. Para ello observamos que}
T
n
{i=1}
\{a
i
&lt; S
i
&le; b
i
\} =}
T
n{−{1
{i=1}
\{N ( a}
i
)−}
N ( b
i{−{1
) = 0, N(b
i
) − N}(a
i
) = 1{\} &cap; \{N}(a
n
) − N}(b
n{−{1
) = 0, N(b
n
) − N}(a
n
) &ge; 1{\} y usamos las
propiedades de independencia y homogeneidad temporal que caracterizan a los incrementos
de un proceso de Poisson de intensidad &lambda;}:
P
n
 &setminus; 
{i=1}
\{a
i
&lt; S
i
&le; b
i
\}
!
=
n{−{1
Y
{i=1}
e
−{&lambda; ( a}
i
−b
i{−{1
)
&lambda; ( b
i
− a
i
)e
−{&lambda; ( b}
i
−a
i
)
!
e
−{&lambda; ( a}
n
−b
n{−{1
)
(1 − e}
−{&lambda; ( b}
n
−a
n
)
)
=
n{−{1
Y
{i=1}
&lambda; ( b
i
− a
i
)
!
e
−{\lambdaa}
n
(1 − e}
−{&lambda; ( b}
n
−a
n
)
)
=
n{−{1
Y
{i=1}
&lambda; ( b
i
− a
i
)
!
(e
−{\lambdaa}
n
− e
−{\lambdab}
n
)
=
Z
b
1
a
1
\lambdads
1
· · ·
Z
b
n{−{1
a
n{−{1
\lambdads
n{−{1
Z
b
n
a
n
\lambdae
−{\lambdas}
n
ds
n
=
Z
b
1
a
1
· · ·
Z
b
n{−{1
a
n{−{1
Z
b
n
a
n
&lambda;
n
e
−{\lambdas}
n
ds
1
· · · ds}
n{−{1
ds
n
(19)
8
De (19) se deduce que la densidad conjunta de (S}
1
, &hellip; , S
n
) es
f
(S}
1
,&#x2026;,S
n
)
(s
1
, &hellip; , s
n
) = &lambda;}
n
e
−{\lambdas}
n
1\{0 &lt; s 
1
&lt; · · · &lt; s
n
\}.
Por lo tanto,
f
(T}
1
,&#x2026;,T
n
)
(t
1
, &hellip; , t
n
) = &lambda;}
n
e
− &lambda; 
P
n
{i=1}
t
i
1\{t}
1
&gt; 0, &hellip; , t
n
&gt; 0{\
=
n
Y
{i=1}
\lambdae
−{\lambdat}
i
1\{t}
i
&gt; 0{\} . (20)
La identidad (20) significa que los tiempos de espera entre arribos son independientes cada
uno con distribución exponencial de intensidad &lambda;}.
</p>
</div>
</div>
<div id="outline-container-org30256b7" class="outline-5">
<h5 id="org30256b7">Ejemplo 1.6</h5>
<div class="outline-text-5" id="text-org30256b7">
<p>
Suponga que el ﬂujo de inmigración de personas hacia un territorio es un}
proceso de Poisson de tasa &lambda; = 1 por día.
(a) ¿Cuál e s el tiempo esperado hasta que se produce el arribo del décimo inmigrante?
(b) ¿Cuál es la probabilidad de que el tiempo de espera entre el décimo y el undécimo arribo
supere los dos días?
Solución:
(a) E[S}
10
] =
10
&lambda;
= 10 días.
(b) \mathbb{P}(T}
11
&gt; 2) = e
−{2 &lambda; }
= e
−{2}
&asymp; 0.133.
</p>
</div>
</div>
<div id="outline-container-orgb73a8fc" class="outline-5">
<h5 id="orgb73a8fc">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-orgb73a8fc">
<ol class="org-ol">
<li>En un sistema electrónico se producen fallas de acuerdo con un proceso de Poisson de tasa}</li>
</ol>
<p>
2.5 por mes. Por motivos de seguridad se ha decidido cambiarlo cuando ocurran 196 fallas.
Hallar la media y la varianza del tiempo de uso del sistema.
</p>
<ol class="org-ol">
<li>Sean T una variable aleatoria con distribución exponencial de media 2 y \{N}(t), t &ge; 0{\} un}</li>
</ol>
<p>
proceso de Poisson de tasa 10 (independiente de T ). Hallar Cov(T, N(T )).
</p>
<ol class="org-ol">
<li></li>
</ol>
<p>

h Sea A(t) = t − S
N ( t ) 
el tiempo reverso al evento más reciente en un proceso de
Poisson y sea B(t) = S}
N ( t)+1}
− t el tiempo directo hasta el próximo evento. Mostrar que
(a) A(t) y B(t) son independientes,
(b) B(t) se distribuye como T}
1
(exponencial de i ntensidad &lambda;) ,
(c) A(t) se distribuye como mín(T}
1
, t):}
\mathbb{P}(A(t) &le; x) = (1 − e
−{\lambdax}
)1{\}0 &le; x &lt; t\} + 1\{x &ge; t\}.}
9
\hypertarget{pfa}
</p>
<ol class="org-ol">
<li></li>
</ol>
<p>

h Sea L(t) = A(t) + B(t) = S
N ( t)+1}
− S}
N ( t ) 
la longitud del intervalo de tiempo entre
arribos que contiene a t.
(a) Mostrar que L(t) tiene densidad
d
t
(x) = &lambda;}
2
xe
−{\lambdax}
1\{0 &lt; x &lt; t}\} + &lambda;(1 + \lambdat)e
−{\lambdax}
1\{x &ge; t\}.
(b) Mostrar que E[L(t)] converge a 2{E[T}
1
] cuando t &rarr; &infin;} . Esto parece una paradoja debido
a que L(t) es uno de los T}
n
. Dar una resolución intuitiva de esta paradoja.
</p>
</div>
</div>
</div>
<div id="outline-container-orgc401cee" class="outline-3">
<h3 id="orgc401cee">Distribución condicional de los tiempos de llegada</h3>
<div class="outline-text-3" id="text-orgc401cee">
<p>
Supongamos que sabemos que ocurrió exactamente un arribo de un proceso de Poisson
en el intervalo [0, t]. Queremos determinar la distribución del tiempo en que el arribo ocurrió.
Como el proceso de Poisson es temporalmente homogéneo y tiene incrementos independientes
es razonable pensar que los intervalos de igual longitud contenidos en el intervalo [0, t] deb
en
tener la misma probabilidad de contener al arribo. En otras palabras, el tiempo en que ocur
rió el arribo debe estar distribuido uniformemente sobre el intervalo [0, t]. Esto es fácil de
verificar puesto que, para s &le; t,
\mathbb{P}(T
1
&lt; s | N ( t) = 1) =}
\mathbb{P}(T
1
&lt; s, N ( t) = 1)
\mathbb{P}(N(t) = 1)
=
\mathbb{P}(1 arribo en (0, s], 0 arribos en (s, t])
\mathbb{P}(N(t) = 1)
=
\mathbb{P}(1 arribo en (0, s])\mathbb{P}(0 arribos en (s, t])
\mathbb{P}(N(t) = 1)
=
\lambdase
−{\lambdas}
e
−{&lambda; ( t}−{s ) 
\lambdate
−{\lambdat}
=
s
t
Este resultado puede generalizarse
</p>
</div>
<div id="outline-container-orgccfbd64" class="outline-5">
<h5 id="orgccfbd64">Teorema 1.7 (Propiedad condicional)</h5>
<div class="outline-text-5" id="text-orgccfbd64">
<p>
Sea &Pi; un proceso de Poisson de intensidad &lambda; sobre}
R
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
. Condicional al evento N  ( t) = n, los n arribos ocurridos en el intervalo [0, t] tienen la mis
ma distribución conjunta que l a de n puntos independientes elegidos al azar sobre el intervalo
[0, t]. En otras palabras, condicional a N  ( t) = n los puntos en c ue stión se distribuyen como}
n variables aleatorias independientes, cada una con distribución uniforme sobre el intervalo}
[0, t].
</p>
</div>
</div>
<div id="outline-container-orgdf1a9c1" class="outline-5">
<h5 id="orgdf1a9c1">Demostración</h5>
<div class="outline-text-5" id="text-orgdf1a9c1">
<p>
Sea A}
1
, A
2
, &hellip; , A
k
una partición del intervalo [0, t]. Si n
1
+n
2
+{· · ·}+n
k
= n,
entonces
\mathbb{P}(N(A}
i
) = n
i
, 1 &le; i &le; k | N ( t) = n) =}
Q
i
\mathbb{P}(N(A}
i
) = n
i
)
\mathbb{P}(N(t) = n)
=
Q
i
e
− &lambda; |A
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<tbody>
<tr>
</tr>
</tbody>
</table>
<p>
(&lambda; | A}
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">)</td>
</tr>
</tbody>
</table>
<p>
n
i
/n
i
!
e
−{\lambdat}
(\lambdat)
n
/n{!}
=
n{!}
n
1
!n
2
! · · · n}
k
!
Y
i

</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">A</td>
</tr>
</tbody>
</table>
<p>
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<tbody>
<tr>
</tr>
</tbody>
</table>
<p>
t

n
i
. (21)
10
\hypertarget{pfb}
Por una parte la distribución condicional de las posiciones de los n arribos queda completa
mente caracterizada por esta función de A
1
, &hellip; , A
k
.
Por otra parte la distribución multinomial (21) es la distribución conjunta de n puntos
independientes elegidos al azar de acuerdo con la distribución uniforme sobre el intervalo [0, t].
En efecto, basta observar que si U}
1
, &hellip; , U
n
son variables aleatorias independientes con
distribución uniforme sobre un conjunto A, y M(B) =
P
i
1\{U
i
&isin; B\, entonces}
\mathbb{P}(M(B}
i
) = n
i
, i = 1, &hellip; , k) =}
n{!}
n
1
! · · · n}
k
!
k
Y
{i=1}

</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">B</td>
</tr>
</tbody>
</table>
<p>
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<tbody>
<tr>
</tr>
</tbody>

<tr>
<td class="org-left">A</td>
</tr>
</tbody>
</table>
<p>
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<tbody>
<tr>
</tr>
</tbody>
</table>
<p>

n
i
.
Se infiere que la distribución conjunta de los puntos en &Pi; &cap; [0, t] condicional a que hay
exactamente n de ellos, es la misma que la de n puntos independientes elegidos al azar con
la distribución uniforme sobre el intervalo [0, t].
</p>
</div>
</div>
<div id="outline-container-orgb90a757" class="outline-5">
<h5 id="orgb90a757">Nota Bene</h5>
<div class="outline-text-5" id="text-orgb90a757">
<p>
La propiedad condicional permite probar la existencia de procesos de Poisson}
mediante simulación. Sea &lambda; &gt; 0 y sea A
1
, A
2
, &hellip; una partición de R
d
en conjuntos borelianos
de medida de Lebesgue finita. Para cada i, simulamos una variable aleatoria N}
i
con distribu
ción Poisson de parámetro &lambda; | A}
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">. Luego muestreamos n puntos elegidos independientemente}</td>
</tr>
</tbody>
</table>
<p>
sobre A
i
, cada uno con distribución uniforme sobre A
i
. La unión sobre i de tales conjuntos de
puntos es un proceso de Poisson de intensidad &lambda;}. (Para más detalles ver el Chap 7 de Ferrari,
Galves (2001))
</p>
</div>
</div>
<div id="outline-container-orgd54c38b" class="outline-5">
<h5 id="orgd54c38b">Ejemplo 1.8 (Insectos en un asado)</h5>
<div class="outline-text-5" id="text-orgd54c38b">
<p>
Todo tipo de insectos aterrizan en la mesa de un asado}
a la manera de un proceso de Poisson de tasa 3 por minuto. Si entre las 13:30 y las 13:35
aterrizaron 8 insectos, cuál es la probabilidad de que exactamente 3 de ellos hayan aterrizado
durante el primer minuto?
</p>
</div>
</div>

<div id="outline-container-org30d4d6d" class="outline-5">
<h5 id="org30d4d6d">Solución</h5>
<div class="outline-text-5" id="text-org30d4d6d">
<p>
Dado que aterrizaron 8 insectos durante 5 minutos, la distribución de
cada aterrizaje se distribuye, independientemente de los demás, como
una var iable uniforme sobre el intervalo [0, 5]. En consecuencia, la
probabilidad de que cada insecto hubiese aterrizado du rante el primer
minuto es 1 / 5. Por lo tanto, la probabilidad de que exactamente 3
insectos hayan aterrizado durante el primer minuto es
</p>

<p>

8
3

1
5

3

4
5

5
= 56
4
5
5
8
= 0.1468 &hellip;}
</p>
</div>
</div>
</div>
<div id="outline-container-org1d02c7c" class="outline-3">
<h3 id="org1d02c7c">Coloración y adelgazamiento de procesos de Poisson</h3>
<div class="outline-text-3" id="text-org1d02c7c">
</div>
<div id="outline-container-orgaedc17e" class="outline-5">
<h5 id="orgaedc17e">Teorema 1.9 (Coloración).</h5>
<div class="outline-text-5" id="text-orgaedc17e">
<p>
Sea &Pi; un proceso de Poisson de i ntensida d &lambda; sobre R }
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
. Col
oreamos los puntos de &Pi; de la siguiente manera. Cada punto de &Pi; se pinta de rojo con
probabilidad p o de negro con proba bili
dad 1 − p} . Los puntos se pintan independientemente
unos de otros. Sean &Pi;}
1
y &Pi;}
2
los conjuntos de puntos pintado de rojo y de negro, respec
tivamente. Entonces &Pi;}
1
y &Pi;}
2
son procesos de Poisson independie ntes de intensidades p&lambda; y
(1 − p)&lambda;, respectivamente.}
11
\hypertarget{pfc}
</p>
</div>
</div>
<div id="outline-container-orgbc2cf71" class="outline-5">
<h5 id="orgbc2cf71">Demostración</h5>
<div class="outline-text-5" id="text-orgbc2cf71">
<p>
Sea t &gt; 0 fijo. Por la propiedad condicional, si N(t) = n, esos puntos tienen}
la misma distribución que n puntos independientes elegidos al azar sobre el intervalo [0, t] de
acuerdo con la distribución uniforme. Por tanto, podemos considerar n puntos elegidos al azar
de esa manera. Por la independencia de los puntos, sus colores son independientes unos de los
otros. Como la probabilidad de que un punto dado sea pintado de rojo es p y la probabilidad
de sea pintado de negro es 1 − p se deduce que, condicional a N(t) = n, las cantidades N}
1
(t)
y N}
2
(t) de puntos rojos y negros en [0, t] tienen, conjuntamente, la distribución binomial
\mathbb{P}(N
1
(t) = n
1
, N
2
(t) = n
2
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{N ( t) = n) =</td>
</tr>
</tbody>
</table>
<p>
n{!}
n
1
!n
2
!
p
n
1
(1 − p)
n
2
, donde n
1
</p>
<ul class="org-ul">
<li>n</li>
</ul>
<p>
2
= n.
Por lo tanto, la probabilidad incondicional es
\mathbb{P}(N
1
(t) = n
1
, N
2
(t) = n
2
) =

(n
1
</p>
<ul class="org-ul">
<li>n</li>
</ul>
<p>
2
)!
n
1
!n
2
!
p
n
1
(1 − p)
n
2

e
−{\lambdat}
(\lambdat)
n
1
+n
2
(n
1
</p>
<ul class="org-ul">
<li>n</li>
</ul>
<p>
2
)!

=

e
−{p\lambdat}
(p\lambdat)
n
1
n
1
!

e
−(1}−{p ) \lambdat}
((1 − p)\lambdat)
n
2
n
2
!
!
.
Vale decir, las cantidades N}
1
(t) y N}
2
(t) de puntos rojos y negros en el interval o [0, t] son inde
pendientes y tienen distribuciones Poisson de intensidades p\lambdat y (1 − p)\lambdat, respectivamente.
La independencia de las contadoras de puntos en intervalos disjuntas sigue trivialmente
del hecho de que &Pi; tiene esa propiedad.
Otra prueba. Sean N
1
(t) y N}
2
(t) la cantidad de arribos de tip o I y de tipo II que ocurren
en [0, t], respectivamente. Es claro que N(t) = N}
1
(t) + N}
2
(t).
Los arribos de tipo I (II) son un proceso puntual aleatorio debido a que son una subsucesión
(aleatoria) infinita de los arribos del proceso original y heredan su propiedad de independencia
para intervalos disjuntos.
La prueba de que \{N
1
(t), t &ge; 0{\} y que \{N
2
(t), t &ge; 0{\} son procesos de Poisson independi
entes de intensidades p&lambda; y (1 − p) &lambda; , respectivamente, se completa observando que
\mathbb{P}(N
1
(t) = n, N}
2
(t) = m) = \mathbb{P}(N}
1
(t) = n)\mathbb{P}(N}
2
(t) = m).
Condicionando a los valores de N(t) y usando probabilidades totales se obtiene
\mathbb{P}(N
1
(t) = n, N}
2
(t) = m) =
&infin;
X
{i=0}
\mathbb{P}(N
1
(t) = n, N}
2
(t) = m | N(t) = i)\mathbb{P}(N(t) = i)
Puesto que \mathbb{P}(N}
1
(t) = n, N}
2
(t) = m | N(t) = i) = 0 cuando i &ne; n + m, l a ecuación anterior
se reduce a
\mathbb{P}(N
1
(t) = n, N}
2
(t) = m) = \mathbb{P}(N}
1
(t) = n, N}
2
(t) = m | N(t) = n + m)\mathbb{P}(N(t) = n + m)
= \mathbb{P}(N}
1
(t) = n, N}
2
(t) = m | N(t) = n + m)e
−{\lambdat}
(\lambdat)
n{+}m
(n + m)!
.
Dado que ocurrieron n + m arribos, la probabilidad de que n sean de tipo I (y m sean de tipo
12
\hypertarget{pfd}
II) es la probabilidad binomial de que ocurran n éxitos en n + m ensayos. Por lo tanto,
\mathbb{P}(N
1
(t) = n, N}
2
(t) = m) =

n + m
n

p
n
(1 − p)
m
e
−{\lambdat}
(\lambdat)
n{+}m
(n + m)!
=
(n + m)!
n{! m{!}
p
n
(1 − p)
m
e
−{\lambdapt}
e
−{&lambda;(1} −{p ) t}
(\lambdat)
n
(\lambdat)
m
(n + m)!
=

e
−{\lambdapt}
(\lambdapt)
n
n{!}

e
−{&lambda;(1} −{p ) t}
( &lambda; (1 − p)t)
m
m{!}

.
Lo que completa la demostración.
</p>
</div>
</div>
<div id="outline-container-org131e6c6" class="outline-5">
<h5 id="org131e6c6">Ejemplo 1.10 (Insectos en un asado)</h5>
<div class="outline-text-5" id="text-org131e6c6">
<p>
Todo tipo de insectos aterrizan en la mesa de un}
asado a la manera de un proceso de Poisson de tasa 3 por minuto y cada insecto puede ser
una mosca con probabilidad 2 / 3, independientemente de la naturaleza de los demás insectos.
Si a las 13:30 se sirven los chorizos, cuál es la probabilidad de que la tercer mosca tarde más
de 2 minutos en aterrizar en la mesa?
Solución: Las moscas aterrizan en la mesa a la manera de un pro ceso de Poisson de tasa}
2
3
3 = 2 por minuto. En consecuencia, los aterrizajes de moscas ocurren cada tiempos exponen
ciales independientes de intensidad 2. De aquí se deduce que el tiempo que tarda en aterrizar
la tercer mosca, S}
3
tiene distribución &Gamma;(3, 2). Por lo tanto, la probabilidad de que la tercer
mosca tarde más de 2 minutos en aterrizar en la mesa es
\mathbb{P}(S
3
&gt; 2) = e
−{2}·{2}
3{−}1
X
{i=0}
(2 · 2)
i
i{!}
= e
−{4}
(1 + 4 + 8) = 0.2381 &hellip;}
</p>
</div>
</div>
<div id="outline-container-orgb70d3d8" class="outline-5">
<h5 id="orgb70d3d8">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-orgb70d3d8">
<ol class="org-ol">
<li>A un banco llegan clientes de acuerdo con un proceso de Poisson de intensidad 20 por}</li>
</ol>
<p>
hora. En forma independiente de los demás, cada cliente realiza un depósito con probabilidad
1 / 4 o una extracción con probabilidad 3 / 4.
(a) Si el banco abre sus puertas a las 10:00, cuál es la probabilidad de que el segundo depósito
se efectué pasadas las 10:30?
(b) Cada depósito (en pesos) se distribuye como una variable U[100, 900] y cada extracción
como una variable U[100, 500]. Si un cliente realiza una operación bancaria de 200 pesos, cuál
es la probabilidad de que se trate de un depósito?
</p>
</div>
</div>
</div>
<div id="outline-container-org283148c" class="outline-3">
<h3 id="org283148c">Superposición de Procesos de Poisson: competencia</h3>
<div class="outline-text-3" id="text-org283148c">
<p>
El siguiente teorema de superposición puede verse como complementario del teorema de
coloración.
</p>
</div>
<div id="outline-container-org8a6f1cb" class="outline-5">
<h5 id="org8a6f1cb">Teorema 1.11 (Superposición)</h5>
<div class="outline-text-5" id="text-org8a6f1cb">
<p>
Sean &Pi; }
1
y &Pi;}
2
dos procesos de Poisson independientes de
intensidades &lambda;}
1
y &lambda;}
2
, respectivamente, so bre R}
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
. El conjunto &Pi; = &Pi;}
1
&cup; &Pi;}
2
es un proceso de
Poisson de intensidad &lambda;}
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
.
13
\hypertarget{pfe}
</p>
</div>
</div>
<div id="outline-container-org5180e2a" class="outline-5">
<h5 id="org5180e2a">Demostración</h5>
<div class="outline-text-5" id="text-org5180e2a">
<p>
Sean N
1
(t) = | &Pi;
1
&cap; [0, t]| y N
2
(t) = | &Pi;
2
&cap; [0, t]|. Entonces N
1
(t) y N}
2
(t)
son variables aleatorias independientes con distribución Poisson de parámetros &lambda;}
1
t y &lambda;
2
t.
Se infiere que la suma N (t) = N}
1
(t) + N}
2
(t) tiene la distribución de Poisson de parámetro
&lambda;
1
t + &lambda;
2
t = (&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
)t. Más aún, si A
1
, A
2
, &hellip; , son intervalos disjuntos las variables aleatorias}
N ( A
1
), N (A
2
), &hellip; son independientes. Falta mostrar que, casi seguramente, N(t) = | &Pi;{&cap;[0, t] | 
para todo t &gt; 0, que es lo mismo que decir que &Pi;
1
y P1
2
no tienen puntos en común. Este es
un paso técnico (ver el Lema 1.12) y la prueba puede omitirse en una primera lectura.
</p>
</div>
</div>
<div id="outline-container-org12be1f8" class="outline-5">
<h5 id="org12be1f8">Lema 1.12</h5>
<div class="outline-text-5" id="text-org12be1f8">
<p>
Dos procesos de Poisson &Pi;
1
= \{S
1
n
</p>
<pre class="example">
n \geq 0{\} y \Pi

</pre>
<p>
2
= \{S
2
n
</p>
<pre class="example">
n \geq 0{\} independientes

</pre>
<p>
y de tasas &lambda;}
1
y &lambda;}
2
, respectivamente, no tienen puntos en común.
</p>
</div>
</div>
<div id="outline-container-org8bd855c" class="outline-5">
<h5 id="org8bd855c">Demostración</h5>
<div class="outline-text-5" id="text-org8bd855c">
<p>
Basta probar que \mathbb{P}(D(t)) = 0 para todo t, donde D(t) es el evento definido}
por
D ( t) := \ex isten puntos en común en el intervalo (0, t]\
Para simplificar la notación lo demostraremos para D = D(1).
Sean \{N
1
(t), t &ge; 0{\} y \{N
2
(t), t &ge; 0{\} los procesos de conteo de los procesos de Poisson
\{S}
1
n
</p>
<pre class="example">
n \geq 0{\} y \{S

</pre>
<p>
2
n
</p>
<pre class="example">
n \geq 0{\}. El evento

</pre>
<p>
D
n
:=

N
1

i
2
n
,
i + 1}
2
n

</p>
<ul class="org-ul">
<li>N}</li>
</ul>
<p>
2

i
2
n
,
i + 1}
2
n

&ge; 2 para algún i &isin; [0, 2}
n
− 1]

decrece a D cuando n tiende a infinito, y por lo tanto, por la continuidad de la probabilidad
para sucesiones monótonas de eventos,
\mathbb{P}(D) = lím}
n{&rarr;&infin;}
\mathbb{P}(D
n
) = 1 − lim
n{&rarr;&infin;}
\mathbb{P}(D
c
n
).
Pero
\mathbb{P}(D
c
n
) = P
2
n
−{1}
 &setminus; 
{i=1}

N
1

i
2
n
,
i + 1}
2
n

</p>
<ul class="org-ul">
<li>N}</li>
</ul>
<p>
2

i
2
n
,
i + 1}
2
n

&le; 1}

!
=
2
n
−{1}
Y
{i=1}
P

N
1

i
2
n
,
i + 1}
2
n

</p>
<ul class="org-ul">
<li>N}</li>
</ul>
<p>
2

i
2
n
,
i + 1}
2
n

&le; 1}

.
Debido a que los procesos son temporalmente homogéneos, para cada i vale que
P

N
1

i
2
n
,
i + 1}
2
n

</p>
<ul class="org-ul">
<li>N}</li>
</ul>
<p>
2

i
2
n
,
i + 1}
2
n

&le; 1}

= P

N
1

2
−n

</p>
<ul class="org-ul">
<li>N}</li>
</ul>
<p>
2

2
−n

&le; 1}

Y el problema se reduce a calcular \mathbb{P}(N}
1
(2
−n
) + N}
2
(2
−n
) &le; 1). La última probabilidad puede
expresarse como la suma de los siguientes términos
P

N
1

2
−n

= 0, N}
2

2
−n

= 0

= e
− &lambda; 
1
2
−n
e
− &lambda; 
2
2
−n
,
P

N
1

2
−n

= 0, N}
2

2
−n

= 1

= e
− &lambda; 
1
2
−n
e
− &lambda; 
2
2
−n
&lambda;
2
2
−n
,
P

N
1

2
−n

= 1, N}
2

2
−n

= 0

= e
− &lambda; 
1
2
−n
&lambda;
1
2
−n
e
− &lambda; 
2
2
−n
.
En consecuencia,
P

N
1

2
−n

</p>
<ul class="org-ul">
<li>N}</li>
</ul>
<p>
2

2
−n

&le; 1}

= e
−( &lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)2
−n

1 + ( &lambda; 
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
)2
−n

. (22)
14
\hypertarget{pff}
Por lo tanto,
\mathbb{P}(D
c
n
) = e
−( &lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)

1 + ( &lambda; 
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
)2
−n

2
n
. (23)
La última cantidad tiende a 1 cuando n &rarr; &infin;}, y se concluye que \mathbb{P}(D) = 0.
</p>
</div>
</div>
<div id="outline-container-orgd79ed87" class="outline-5">
<h5 id="orgd79ed87">Teorema 1.13 (Competencia)</h5>
<div class="outline-text-5" id="text-orgd79ed87">
<p>
En la sit uac ión del Teorema 1.11, sea T el primer arribo del}
proceso N = N}
1
</p>
<ul class="org-ul">
<li>N}</li>
</ul>
<p>
2
y J el índice del proceso de Poisson responsable por dicho arribo; en
particular T es el primer arribo de N}
J
. Entonces
\mathbb{P}(J = j , T &ge; t) = \mathbb{P}(J = j)\mathbb{P}(T &ge; t) =}
&lambda;
j
&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
e
−( &lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)t
.
En particular, J y T son independientes, \mathbb{P}(J = j) =
&lambda;
j
&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
y T tiene distribución exponencial
de intensidad &lambda;}
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
.
</p>
</div>
</div>
<div id="outline-container-org0c20d7a" class="outline-5">
<h5 id="org0c20d7a">Demostración</h5>
<div class="outline-text-5" id="text-org0c20d7a">
<p>
Ver la demostración del Teorema que caracteriza la distribución del mínimo}
de dos exponenciales independientes.
</p>
</div>
</div>
<div id="outline-container-org51d2d69" class="outline-5">
<h5 id="org51d2d69">Ejemplo 1.14 (Insectos en un asado)</h5>
<div class="outline-text-5" id="text-org51d2d69">
<p>
Moscas y abejas aterrizan en la mesa de un asado a la}
manera de dos procesos de Poisson independientes de tasas 2 y 1 por minuto, respectivamente.
Cuál es la probabilidad de que el primer insecto en aterrizar en la mesa sea una mosca? Rta.
2 / 3.
</p>
</div>
</div>
</div>
<div id="outline-container-org071df0b" class="outline-3">
<h3 id="org071df0b">Procesos de Poisson compuestos</h3>
<div class="outline-text-3" id="text-org071df0b">
<p>
Un proceso estocástico se dice un proceso de Poisson compuesto si puede representarse
como
X ( t) =}
N ( t ) 
X
{i=1}
Y
i
donde \{N}(t), t &ge; 0{\} es un proceso de Poisson, y las variables \{Y}
i
, i &ge; 1\} son iid e independi}
entes de N}.
</p>
</div>
<div id="outline-container-orge90cd1c" class="outline-5">
<h5 id="orge90cd1c">Lema 1.15</h5>
<div class="outline-text-5" id="text-orge90cd1c">
<p>
Sea X(t) un proceso de Poisson compuesto. Si \{N (t), t &ge; 0{\} tiene intensidad &lambda;
y las variables Y tienen esperanza finita, entonces
E[X(t)] = \lambdat} E[Y}
1
].
Más aún, si las variables Y tienen varianza finita, entonces,
V(X(t)) = \lambdat} E[Y}
2
1
].
</p>
</div>
</div>
<div id="outline-container-org63a0b67" class="outline-5">
<h5 id="org63a0b67">Demostración</h5>
<div class="outline-text-5" id="text-org63a0b67">
<p>
Para calcular la esperanza de X(t) c ondicionamos sobre N (t):}
E [X(t)] = E [}E [X(t) |{N}(t)]]
15
Ahora bien,
E [X(t) | N}(t) = n] = E


N ( t ) 
X
{i=1}
Y
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">N  ( t) = n}</td>
</tr>
</tbody>
</table>
<p>


= E}
"
n
X
{i=1}
Y
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">N  ( t) = n}</td>
</tr>
</tbody>
</table>
<p>
\#
= E}
"
n
X
{i=1}
Y
i
\#
por la i ndependencia de Y
i
y N(t)
= n{E[Y<sub>1</sub>
].
Esto implica que
E [X(t) | N}(t)] = N (t)E[Y}
1
]
y por l o tanto,
E [X(t)] = E [N (t)E[Y}
1
]] = E[N(t)]E[Y<sub>1</sub>
] = \lambdat{E[Y<sub>1</sub>
].
Aunque podemos obtener E[X(t)
2
] condicionando sobre N(t), usaremos la fórmula de la
varianza condicional
V(X(t)) = E[V(X(t)|{N}(t))] + V ( E[X(t)|{N (t)]).
Ahora bien,
V [X(t) | N}(t) = n] = V


N ( t ) 
X
{i=1}
Y
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">N  ( t) = n}</td>
</tr>
</tbody>
</table>
<p>


= V}
n
X
{i=1}
Y
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">N  ( t) = n}</td>
</tr>
</tbody>
</table>
<p>
!
= V}
n
X
{i=1}
Y
i
!
por la i ndependencia de Y
i
y N(t)
= n{V[Y<sub>1</sub>
].
Esto implica que
V (X(t) | N}(t)) = N (t)V(Y}
1
)
y por l o tanto,
V (X(t)) = E [N(t)V(Y}
1
)] + V(N(t)E[Y<sub>1</sub>
])
= V(Y<sub>1</sub>
)E[N(t)] + E[Y<sub>1</sub>
]
2
V(N(t))
= V(Y<sub>1</sub>
)\lambdat + E[Y<sub>1</sub>
]
2
\lambdat
= \lambdat{E[Y
2
1
].
16
</p>
</div>
</div>
<div id="outline-container-orga1d05f0" class="outline-5">
<h5 id="orga1d05f0">Ejemplo 1.16</h5>
<div class="outline-text-5" id="text-orga1d05f0">
<p>
Supongamos que la cantidad de accidentes en una fábrica industrial se
rige por} un proceso de Poisson de intensidad 4 por mes y que la
cantidad de trabajadores damnificados en cada accidente son variables
aleatorias independientes con distribución uniforme sobre \(\{1, 2,
3\}\). Supongamos también que la cantidad de trabajadores damnificados
en cada accidente es independiente de la cantidad de accidentes
ocurridos. Se quiere hallar la media y la varianza de la cantidad
anual de trabajadores damnificados en dicha fábrica.
</p>
</div>
</div>

<div id="outline-container-org14c6454" class="outline-5">
<h5 id="org14c6454">Solución</h5>
<div class="outline-text-5" id="text-org14c6454">
<p>
Sean N(t) la cantidad de accidentes en t meses e Y
</p>

<p>
i el número de trabajadores damnificados en el i-ésimo accidente, i =
1, 2, &hellip; . El número total de trabajadores damnificados en un año
puede expresarse en la forma X(12) =
</p>

<p>
P
N(12)
{i=1}
Y
i
.
Utilizando los resultados del Lema 1.15 tenemos que
E[X(12)] = (4 · 12)E[Y}
1
] = 48{E[Y<sub>1</sub>
] = 48 · 2 = 96
V(X(12)) = (4 · 12)E[Y}
2
1
] = 48 ·}
14
3
= 224.
</p>
</div>
</div>
<div id="outline-container-orgda31c67" class="outline-5">
<h5 id="orgda31c67">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-orgda31c67">
<ol class="org-ol">
<li>Una partícula suspendida en agua es bombardeada por moléculas en
movimiento térmico de acuerdo con un proceso de Poisson de
intensidad 10 impactos por segundo. Cuando recibe un impacto la
partícula se mueve un milímetro hacia la derecha con probabilidad 3
/ 4 o un milímetro hacia la izquierda con probabilidad 1
/ 4. Transcurrido un minuto, cuál es la posición media de la
partícula?</li>
<li>Un servidor recibe clientes de acuerdo con un proceso de Poisson de
intensidad 4 clientes por hora. El tiempo de trabajo (en minutos)
consumido en cada servicio es una variable aleatoria U[1, 9]. Al
cabo de 8 horas, cuál es el tiempo medio de trabajo consumido por
todos los servicios?</li>
</ol>
</div>
</div>
</div>
</div>
<div id="outline-container-org143756b" class="outline-2">
<h2 id="org143756b">Bibliografía consultada</h2>
<div class="outline-text-2" id="text-org143756b">
<p>
Para redactar estas notas se consultaron los siguientes libros:
</p>
<ol class="org-ol">
<li>Brémaud, P.: Markov Chains: Gibbs Fields, Monte Carlo Simulation,
and Queues. Springer, New York. (1999)</li>
<li>Feller, W.: An introduction to Probability Theory and Its
Applications. Vol. 2. John Wiley &amp; Sons, New York. (1971)</li>
<li>Ferrari, P. A., Galves, A.: Construction of Stochastic Procecesses,
Coupling and Regen eration. (2001)</li>
<li>Grimmett, G. R., Stirzaker, D. R.: Probability and Random
Processes. Oxford University Press, New York. (2001)</li>
<li>Kingman, J. F. K.: Poisson Processes. Oxford University Press. New
York. (2002)</li>
<li>Meester, R.: A Natural Introduction to Probability
Theory. Birkhauser, Berlin. (2008)</li>
<li>Ross, S.: Introduction to Probability Models. Academic Press, San
Diego. (2007)</li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
Last update: 2019-03-06
</div>
</body>
</html>
