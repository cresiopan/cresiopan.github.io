<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-04-24 Wed 01:09 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Probabilidad Condicional, Independencia Estoc√°stica</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/res/org.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "true" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "true" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "%AUTONUMBER"},
               MultLineWidth: "%MULTLINEWIDTH",
               TagSide: "right",
               TagIndent: "%TAGINDENT"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<div id="outline-container-org66575c7" class="outline-2">
<h2 id="org66575c7">Probabilidad Condicional</h2>
<div class="outline-text-2" id="text-org66575c7">
</div>
<div id="outline-container-org0fe6f8a" class="outline-3">
<h3 id="org0fe6f8a">Probabilidad Condicional</h3>
<div class="outline-text-3" id="text-org0fe6f8a">
<p>
Sea \((\Omega, \mathcal{A}, \mathbb{P})\) un espacio de probabilidad.
</p>
</div>
<div id="outline-container-orgfcaab6c" class="outline-5">
<h5 id="orgfcaab6c">Definici√≥n 1.1 (Probabilidad condicional)</h5>
<div class="outline-text-5" id="text-orgfcaab6c">
<p>
Sea \(A \subset \Omega\) un evento de probabilidad positiva.  Para cada
evento \(B\) definimos
</p>

\begin{equation}\mathbb{P}(B|A) := \frac{\mathbb{P}(B \cap A)}{\mathbb{P}(A)}\end{equation}

<p>
La cantidad definida en <a href="#orge6e1ca5">1</a> se llama la probabilidad condicional de
\(B\) dado que ocurri√≥ \(A\).
</p>
</div>
</div>
<div id="outline-container-org93cdb6f" class="outline-5">
<h5 id="org93cdb6f">Nota Bene</h5>
<div class="outline-text-5" id="text-org93cdb6f">
<p>
La probabilidad condicional induce una medida de probabilidad sobre
los eventos aleatorios.  Valen las siguientes propiedades:
</p>
<ol class="org-ol">
<li>Para cada \(B \in \mathcal{A}, \mathbb{P}(B|A) \geq 0\)</li>
<li>\(\mathbb{P}(\Omega | A) = 1\)</li>
<li>Si los eventos \(B\) y \(C\) no tienen elementos en com√∫n, entonces
\[\mathbb{P}(B \cup C | A) = \mathbb{P}(B | A) + \mathbb{P}(C | A)\]</li>
<li>Para cada sucesi√≥n decreciente de eventos \(B_1 \supset B_2 \supset
   \cdots\) tal que \(\bigcap_{n=1}^{\infty} B_n = \emptyset\) vale que
\(\displaystyle\lim_{n \rightarrow \infty} \mathbb{P}(B_n | A) = 0\).</li>
</ol>
<p>
Comparando las propiedades 1-4 con los axiomas I-IV, se concluye que
la funci√≥n \(\mathbb{P}(¬∑|A) :\mathcal{A} \rightarrow \Re\) es una medida de
probabilidad sobre los eventos aleatorios. Por lo tanto, todos los
resultados generales referidos a la propiedades de \(\mathbb{P}(¬∑)\) tambi√©n
valen para la probabilidad condicional \(\mathbb{P}(¬∑|A)\).
</p>
</div>
</div>
<div id="outline-container-orgadee576" class="outline-5">
<h5 id="orgadee576">Ejemplo 1.2</h5>
<div class="outline-text-5" id="text-orgadee576">
<p>
Se lanza un dado equilibrado. Sabiendo que el resultado del dado no
super√≥ al 4, cu√°l es la probabilidad condicional de haber obtenido un
3? Denotando mediante \(A\) al evento <i>el resultado no supera al 4</i>  y
mediante \(B\) el evento <i>el resultado es 3</i> . Tenemos que \(\mathbb{P}(A) = 4 /
6, \mathbb{P}(B) = 1 / 6\) y \(\mathbb{P}(A \cap B) = \mathbb{P}(A) = 1 / 6\). As√≠
</p>

<p>
\[\mathbb{P}(B | A) = \frac{\mathbb{P}(B \cap A)}{\mathbb{P}(A)} =
\frac{1 / 6}{4 / 6} = \frac{1}{4}\]
</p>

<p>
lo que intuitivamente tiene sentido (¬øpor qu√©?).
</p>
</div>
</div>
<div id="outline-container-org9aa9b81" class="outline-5">
<h5 id="org9aa9b81">Probabilidad compuesta</h5>
<div class="outline-text-5" id="text-org9aa9b81">
<p>
De la definici√≥n de la probabilidad condicional del evento \(B\) dado
que ocurri√≥ el evento \(A\) resulta inmediatamente la siguiente f√≥rmula
</p>

\begin{equation}\mathbb{P}(A \cap B) = \mathbb{P}(B | A)\mathbb{P}(A)\end{equation}

<p>
denominada <i>regla del producto</i>.
</p>

<p>
El siguiente Teorema generaliza la regla del producto <a href="#orgac8ec23">1</a> y se obtiene
por inducci√≥n.
</p>

<p>
Figura 1: Ilustraci√≥n de la regla del producto. El evento
\(\cap_{i=1}^n A_i\) tiene asociada una √∫nica trayectoria sobre un √°rbol
que describe la historia de un experimento aleatorio realizado por
etapas sucesivas. Las aristas de esta trayectoria corresponden a la
ocurrencia sucesiva de los eventos \(A_1, A_2, \dots , A_n\) y sobre
ellas registramos la correspondiente probabilidad condicional.  El
nodo final de la trayectoria corresponde al evento \(\bigcap_{i=1}^n
A_i\) y su probabilidad se obtiene multiplicando las probabilidades
condicionales registradas a lo largo de las aristas de la trayectoria:
\(\mathbb{P}(\bigcap_{i=1}^n A_i) = \mathbb{P}(A_1) \mathbb{P}(A_2|A_1)
\mathbb{P}(A_3|A_2 \cap A_1) \dots \mathbb{P}(A_n|\bigcap_{i=1}^{n-1}
A_i)\). Notar que cada nodo intermedio a lo largo de la trayectoria
tambi√©n corresponde a un evento intersecci√≥n y su probabilidad se
obtiene multiplicando las probabilidades condicionales registradas
desde el inicio de la trayectoria hasta llegar al nodo. Por ejemplo,
el evento \(A_1 \cap A_2 \cap A_3\) corresponde al nodo indicado en la
figura y su probabilidad es \(\mathbb{P}(A_1 \cap A_2 \cap A_3) =
\mathbb{P}(A_1) \mathbb{P}(A_2|A_1) \mathbb{P}(A_3|A_1 \cap A_2)\).
</p>
</div>
</div>
<div id="outline-container-orgd0f5790" class="outline-5">
<h5 id="orgd0f5790">Teorema 1.3 (Regla del producto)</h5>
<div class="outline-text-5" id="text-orgd0f5790">
<p>
Suponiendo que todos los eventos condicionantes tienen
probabilidad positiva, tenemos que
</p>

\begin{equation}
\mathbb{P}(\bigcap_{i=1}^n A_i) = \mathbb{P}(A_n|\bigcap_{i=1}^{n-1} A_i) \dots \mathbb{P}(A_3|A_1 \cap A_2) \mathbb{P}(A_2|A_1) \mathbb{P}(A_1)
\end{equation}
</div>
</div>

<div id="outline-container-orgdfaffbb" class="outline-5">
<h5 id="orgdfaffbb">Ejemplo 1.4</h5>
<div class="outline-text-5" id="text-orgdfaffbb">
<p>
Una urna contiene 5 bolas rojas y 10 bolas negras. Se extraen dos
bolas al azar sin reposici√≥n. ¬øCu√°l es la probabilidad que ambas bolas
sean negras?  Sean \(N_1\) y \(N_2\) los eventos definidos por <i>la primer
bola extra√≠da es negra</i> y <i>la segunda bola extra√≠da es negra</i> ,
respectivamente. Claramente \(\mathbb{P}(N_1) = 10 / 15\). Para calcular
\(\mathbb{P}(N_2 | N_1)\) observamos que si ocurri√≥ \(N_1\), entonces solo
9 de las 14 bolas restantes en la urna son negras.
</p>

<p>
As√≠ \(\mathbb{P}(N_2|N_1) = 9 / 14\) y
</p>

<p>
\[\mathbb{P}(N_2 \cap N_1) = \mathbb{P}(N_2 | N_1) \mathbb{P}(N_1) =
\frac{10}{15}\frac{9}{14} = \frac{3}{7}\]
</p>
</div>
</div>
</div>

<div id="outline-container-org785542d" class="outline-3">
<h3 id="org785542d">F√≥rmula de probabilidad total</h3>
<div class="outline-text-3" id="text-org785542d">
</div>
<div id="outline-container-orge51c074" class="outline-5">
<h5 id="orge51c074">Teorema 1.5 (F√≥rmula de probabilidad total)</h5>
<div class="outline-text-5" id="text-orge51c074">
<p>
Sea \(A_1, A_2, \dots\) una sucesi√≥n de eventos disjuntos dos a dos tal
que \(\bigcup_{n \geq 1} A_n = \Omega\). Para cada \(B \in \mathcal{A}\)
vale la siguiente f√≥rmula
</p>

\begin{equation}\mathbb{P}(B) = \displaystyle\sum_{n \geq 1} \mathbb{P}(B | A_n) \mathbb{P}(A_n)\end{equation}

<p>
denominada f√≥rmula de probabilidad total<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
</p>

<p>
Figura 2: Ilustraci√≥n de la f√≥rmula de probabilidad total. Un
experimento de dos etapas binarias y su correspondiente diagrama de
√°rbol. La primera ramificaci√≥n (de izquierda a derecha) se basa en el
resultado de la primer etapa del experimento (\(A\) o \(A^c\)) y la
segunda en su resultado final (\(B\) o \(B^c\)). Multiplicando las
probabilidades registradas a lo largo de cada trayectoria se obtiene
la probabilidad del evento intersecci√≥n representado por el nodo
final. Sumando las probabilidades de las trayectorias que corresponden
al evento \(B\) se obtiene: \(\mathbb{P}(B) = \mathbb{P}(A \cap B) + \mathbb{P}(A^c \cap B) =
\mathbb{P}(B|A)\mathbb{P}(A) + \mathbb{P}(B|A^c)\mathbb{P}(A^c)\).
</p>
</div>
</div>
<div id="outline-container-orgb609052" class="outline-5">
<h5 id="orgb609052">Demostraci√≥n de la f√≥rmula de probabilidad total</h5>
<div class="outline-text-5" id="text-orgb609052">
<p>
De la identidad de conjuntos
</p>

<p>
\[B = B \cap \Omega = B \cap \left(\bigcup_{n \geq 1} A_n \right) =
\bigcup_{n \geq 1} (B \cap A_n)\]
</p>

<p>
y la \(\sigma\) - aditividad de la medida de probabilidad \(P\) se deduce
que
</p>

<p>
\[\mathbb{P}(B) = \displaystyle\sum_{n=1}^{\infty} \mathbb{P}(B \cap
A_n)\]
</p>

<p>
Si \(\mathbb{P}(A_n) = 0, \mathbb{P}(B \cap A_n) = 0\) porque \(B \cap
A_n \subset A_n\). Si \(\mathbb{P}(A_n) > 0\), entonces \(\mathbb{P}(B
\cap A_n) = \mathbb{P}(B | A_n)\mathbb{P}(A_n)\).
</p>
</div>
</div>
<div id="outline-container-org09b5b0b" class="outline-5">
<h5 id="org09b5b0b">Nota Bene: C√°lculo mediante condicionales</h5>
<div class="outline-text-5" id="text-org09b5b0b">
<p>
Si se dispone de una colecci√≥n de eventos \(A_1, A_2, \dots\) de los
cuales uno y solamente uno debe ocurrir, la f√≥rmula de probabilidad
total (4) permite calcular la probabilidad de cualquier evento \(B\)
condicionando a saber cu√°l de los eventos \(A_i\) ocurri√≥. M√°s
precisamente, la f√≥rmula (4) establece que la probabilidad \(\mathbb{P}(B)\) es
igual al promedio ponderado de las probabilidades condicionales \(\mathbb{P}(B |
A_i)\) donde cada t√©rmino se pondera por la probabilidad del evento
sobre el que se condicion√≥. Esta f√≥rmula es √∫til debido a que a veces
es m√°s f√°cil evaluar las probabilidades condicionales \(\mathbb{P}(B | A_i)\) que
calcular directamente la probabilidad \(\mathbb{P}(B)\).
</p>
</div>
</div>
<div id="outline-container-org12926d7" class="outline-5">
<h5 id="org12926d7">Ejemplo 1.6 (Experimentos de dos etapas)</h5>
<div class="outline-text-5" id="text-org12926d7">
<p>
La primera etapa del experimento produce una partici√≥n \(A_1, A_2,
\dots\) del espacio muestral \(\Omega\). La segunda etapa produce el
evento \(B\). La f√≥rmula (4) se utiliza para calcular la probabilidad de
\(B\).
</p>
</div>
</div>
<div id="outline-container-org232f8cc" class="outline-5">
<h5 id="org232f8cc">Ejemplo 1.7</h5>
<div class="outline-text-5" id="text-org232f8cc">
<p>
Una urna contiene 5 bolas rojas y 10 bolas negras. Se extraen dos
bolas sin reposici√≥n. ¬øCu√°l es la probabilidad de que la segunda bola
sea negra?
</p>

<p>
El espacio muestral de este experimento aleatorio se puede representar
como las trayectorias a lo largo de un √°rbol como se muestra en la
Figura 3.
</p>

<p>
Figura 3: Observando el √°rbol se deduce que la probabilidad de que la
segunda bola sea negra es: \(\frac{1}{3} \frac{10}{14} + \frac{2}{3}
\frac{9}{14} = \frac{2}{3}\).
</p>

<p>
Formalmente, el problema se resuelve mediante la f√≥rmula de
probabilidad total. Sean \(N_i\) y \(R_i\) los eventos definidos por <i>la
i-√©sima bola extra√≠da es negra</i> y <i>la i-√©sima bola extra√≠da es
roja</i>, respectivamente \((i = 1, 2)\). Vale que
</p>

<p>
\[\mathbb{P}(N_1) = \frac{10}{15} , \mathbb{P}(R_1) = \frac{5}{15} ,
\mathbb{P}(N_2 | R_1) = \frac{10}{14} , \mathbb{P}(N_2 | N_1) =
\frac{9}{14}\]
</p>

<p>
Usando la f√≥rmula de probabilidad total obtenemos
</p>

\begin{align*}
\mathbb{P}(N_2) &= \mathbb{P}(N_2 \cap R_1) + \mathbb{P}(N_2 \cap N_1)\\
       &= \mathbb{P}(N_2 |R_1) \mathbb{P}(R_1) + \mathbb{P}(N_2 | N_1) \mathbb{P}(N_1)\\
       &= \frac{10}{14} \frac{1}{3} + \frac{9}{14} \frac{2}{3} = \frac{2}{3}
\end{align*}
</div>
</div>
</div>
<div id="outline-container-orga50c935" class="outline-3">
<h3 id="orga50c935">Regla de Bayes</h3>
<div class="outline-text-3" id="text-orga50c935">
<p>
Primera versi√≥n de la regla de Bayes. Sean \(A\) y \(B\) dos eventos de
probabilidad positiva. De la regla del producto (2) y su an√°loga
\(\mathbb{P}(A \cap B) = \mathbb{P}(A | B) \mathbb{P}(B)\) se obtiene la
siguiente f√≥rmula importante
</p>

\begin{equation}\mathbb{P}(A | B) = \frac{\mathbb{P}(B | A) \mathbb{P}(A)}{\mathbb{P}(B)}\end{equation}

<p>
que contiene lo esencial del Teorema de Bayes.
</p>
</div>
<div id="outline-container-org44f516d" class="outline-5">
<h5 id="org44f516d">Ejemplo 1.8</h5>
<div class="outline-text-5" id="text-org44f516d">
<p>
Un test de sangre es 95% efectivo para detectar una enfermedad cuando
una persona realmente la padece. Sin embargo, el test tambi√©n produce
un <i>falso positivo</i>  en el 1 % de las personas saludables
testeadas. Si el 0.5% de la poblaci√≥n padece la enfermedad, cu√°l es la
probabilidad de que una persona tenga la enfermedad si su test result√≥
positivo?  Sea \(A\) el evento definido por <i>la persona testeada tiene
la enfermedad</i> y sea \(B\) el evento definido por <i>el resultado de su
test es positivo</i>. La probabilidad que nos interesa es \(\mathbb{P}(A | B)\) y
se puede calcular de la siguiente manera. Sabemos que
</p>

<p>
\[\mathbb{P}(A) = 0.005, \mathbb{P}(A^c) = 0.995, \]
</p>

<p>
\[\mathbb{P}(B | A) = 0.95, \mathbb{P}(B | A^c) = 0.01, \]
</p>

<p>
y usando esa informaci√≥n queremos calcular
</p>

<p>
\[\mathbb{P}(A | B) = \mathbb{P}(A \cap B) \mathbb{P}(B)\]
</p>

<p>
El numerador, \(\mathbb{P}(A \cap B)\), se puede calcular mediante la
regla del producto
</p>

<p>
\[\mathbb{P}(A \cap B) = \mathbb{P}(B | A)\mathbb{P}(A) =
(0.95)(0.005)\]
</p>

<p>
y el denominador, \(\mathbb{P}(B)\), se puede calcular usando la f√≥rmula
de probabilidad total
</p>

<p>
\[\mathbb{P}(B) = \mathbb{P}(B | A)\mathbb{P}(A) + \mathbb{P}(B | A^c)
\mathbb{P}(A^c) = (0.95)(0.005) + (0.01)(0.995)\]
</p>

<p>
Por lo tanto,
</p>

<p>
\[\mathbb{P}(A | B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} =
\frac{\mathbb{P}(B | A)\mathbb{P}(A)}{\mathbb{P}(B | A)\mathbb{P}(A) +
\mathbb{P}(B | A^c) \mathbb{P}(A^c)} = \frac{95}{294} \approx 0.323\]
</p>

<p>
En otras palabras, s√≥lo el 32% de aquellas personas cuyo test result√≥
positivo realmente tienen la enfermedad.
</p>
</div>
</div>
<div id="outline-container-orgd644209" class="outline-5">
<h5 id="orgd644209">Teorema 1.9 (Bayes)</h5>
<div class="outline-text-5" id="text-orgd644209">
<p>
Sean \(A_1, A_2, \dots\) , eventos disjuntos dos a dos y tales que
\(\bigcup_{n \geq 1} A_n = \Omega\).  Sea \(B\) un evento de probabilidad
positiva. Entonces,
</p>

\begin{equation}\mathbb{P}(A_n | B) = \frac{\mathbb{P}(B | A_n) \mathbb{P}(A_n)}{\displaystyle\sum_{k \geq 1} \mathbb{P}(B | A_k) \mathbb{P}(A_k)} , n \geq 1\end{equation}

<p>
Si los eventos \(A_1, A_2, \dots\) se llaman <i>hip√≥tesis</i> , la f√≥rmula
(6) se considera como la probabilidad de ocurrencia de la hip√≥tesis
\(A_n\) sabiendo que ocurri√≥ el evento \(B\). En tal caso,
\(\mathbb{P}(A_n)\) es la probabilidad a priori de la hip√≥tesis \(A_n\) y
la f√≥rmula (6) para \(\mathbb{P}(A_n | B)\) se llama la regla de Bayes
para la probabilidad a posteriori de la hip√≥tesis \(A_n\).
</p>
</div>
</div>
<div id="outline-container-org19d6ae8" class="outline-5">
<h5 id="org19d6ae8">Nota Bene</h5>
<div class="outline-text-5" id="text-org19d6ae8">
<p>
Advertimos al lector que no trate de memorizar la f√≥rmula
(6). Matem√°ticamente, solo se trata de una forma especial de escribir
la f√≥rmula (5) y de nada m√°s.
</p>
</div>
</div>
<div id="outline-container-org67c1f28" class="outline-5">
<h5 id="org67c1f28">Ejemplo 1.10 (Canal de comunicaci√≥n binario)</h5>
<div class="outline-text-5" id="text-org67c1f28">
<p>
Un canal de comunicaci√≥n binario simple transporta mensajes usando
solo dos se√±ales: 0 y 1. Supongamos que en un canal de comu nicaci√≥n
binario dado el 40% de las veces se transmite un 1; que si se
transmiti√≥ un 0 la probabilidad de recibirlo correctamente es 0.90; y
que si se transmiti√≥ un 1 la probabilidad de recibirlo correctamente
es 0.95. Queremos determinar
</p>
<ol class="org-ol">
<li>la probabilidad de recibir un 1;</li>
<li>dado que se recibi√≥ un 1, la probabilidad de que haya sido
transmitido un 1;</li>
</ol>
</div>
</div>
<div id="outline-container-org0105fdf" class="outline-5">
<h5 id="org0105fdf">Soluci√≥n</h5>
<div class="outline-text-5" id="text-org0105fdf">
<p>
Consideramos los eventos \(A\) = <i>se transmiti√≥ un 1</i>  y \(B\) = <i>se
recibi√≥ un 1</i>. La informaci√≥n dada en el enunciado del problema
significa que \(\mathbb{P}(A) = 0.4, \mathbb{P}(A^c) = 0.6,
\mathbb{P}(B | A) = 0.95, \mathbb{P}(B | A^c) = 0.1, \mathbb{P}(B^c|A)
= 0.05, \mathbb{P}(B^c|A^c) = 0.90\) y se puede representar en la forma
de un diagrama de √°rbol tal como se indic√≥ en la secci√≥n 1.2.
</p>

<p>
Figura 4: Observando el √°rbol se deduce que la probabilidad de recibir
un 1 es \(\mathbb{P}(B) = (0.4)(0.95) + (0.6)(0.1) = 0.44\). Tambi√©n se
deduce que la probabilidad de que haya sido transmitido un 1 dado que
se recibi√≥ un 1 es \(\mathbb{P}(A | B) = \frac{\mathbb{P}(B | A)
\mathbb{P}(A)}{\mathbb{P}(B)} = \frac{(0.4)(0.95)}{0.44} = 0.863\dots\)
</p>
</div>
</div>
<div id="outline-container-orgbd66d84" class="outline-5">
<h5 id="orgbd66d84">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-orgbd66d84">
<ol class="org-ol">
<li>Los dados de Efron. Se trata de cuatro dados \(A, B, C, D\) como los
que se muestran en la Figura 5.</li>
</ol>

<p>
Figura 5: Dados de Efron
</p>

<p>
Las reglas del juego son las siguientes: juegan dos jugadores, cada
jugador elige un dado, se tiran los dados y gana el que obtiene el
n√∫mero m√°s grande.
</p>
<ol class="org-ol">
<li>Calcular las siguientes probabilidades: que \(A\) le gane a \(B\); que
\(B\) le gane a \(C\); que \(C\) le gane a \(D\); que \(D\) le gane a \(A\).</li>
<li>¬øCu√°l es la mejor estrategia para jugar con los dados de Efron?.</li>
<li>Lucas y Monk jugaran con los dados de Efron eligiendo los dados al
azar. Calcular las siguientes probabilidades:
<ol class="org-ol">
<li>que Lucas pierda la partida si Monk obtiene un 3,</li>
<li>que Lucas gane la partida si le toca el dado \(A\).</li>
</ol></li>
<li>¬øQu√© ocurre con el juego cuando los dados se eligen al azar?</li>
<li>¬øQu√© ocurre con el juego si a un jugador se le permite elegir un
dado y el otro debe elegir al azar uno entre los restantes tres?</li>
<li>Lucas y Monk jugaron c on los dados de Efron, eligiendo los dados
al azar. Lucas gan√≥, ¬øcu√°l es la probabilidad de que le haya tocado
el dado \(C\)?</li>
</ol>
</div>
</div>
</div>
</div>
<div id="outline-container-org651d7d6" class="outline-2">
<h2 id="org651d7d6">Independencia estoc√°stica</h2>
<div class="outline-text-2" id="text-org651d7d6">
</div>
<div id="outline-container-org3870d7c" class="outline-5">
<h5 id="org3870d7c">Definici√≥n 2.1 (Independencia estoc√°stica)</h5>
<div class="outline-text-5" id="text-org3870d7c">
<p>
Los eventos \(A_1, A_2, \dots , A_n\) son mutuamente independientes si satisfacen
las siguientes \(2^n ‚àí n ‚àí 1\) ecuaciones:
</p>

\begin{equation}\mathbb{P}(A_{i_1} \cap A_{i_2} \cap \cdots \cap A_{i_m}) =
\mathbb{P}(A_{i_1}) \mathbb{P}(A_{i_2}) \cdots \mathbb{P}(A_{i_m})\end{equation}

<p>
donde \(m = 1, 2, \dots , n\), y \(1 \leq i_1 < i_2 < \dots < i_m \leq n\)
</p>
</div>
</div>
<div id="outline-container-org3a9006a" class="outline-5">
<h5 id="org3a9006a">Nota Bene 1</h5>
<div class="outline-text-5" id="text-org3a9006a">
<p>
Para \(n = 2\) el sistema de ecuaciones (7) se reduce a una condici√≥n:
dos eventos \(A_1\) y \(A_2\) son independientes si satisfacen la ecuaci√≥n
</p>

\begin{equation}\mathbb{P}(A_1 \cap A_2) = \mathbb{P}(A_1) \mathbb{P}(A_2)\end{equation}
</div>
</div>

<div id="outline-container-org7e8f099" class="outline-5">
<h5 id="org7e8f099">Ejemplo 2.2</h5>
<div class="outline-text-5" id="text-org7e8f099">
<ol class="org-ol">
<li>Se extrae un naipe al azar de un mazo de naipes de poker. Por razones de
simetr√≠a esperamos que los eventos <i>coraz√≥n y As</i>  sean independientes. En
todo caso, sus probabilidades son \(1 / 4\) y \(1 / 13\), respectivamente y la
probabilidad de su realizaci√≥n simult√°nea es \(1 / 52\).</li>
<li>Se arrojan dos dados. Los eventos <i>as en el primer dado</i>  y <i>par en el
segundo</i> son independientes pues la probabilidad de su realizaci√≥n
simult√°nea, \(3 / 36 = 1 / 12\), es el producto de sus probabilidades
respectivas: \(1 / 6\) y \(1 / 2\).</li>
<li>En una permutaci√≥n aleatoria de las cuatro letras a, b, c, d los eventos <i>a
precede a b</i> y <i>c precede a d</i>  son independientes. Esto es intuitivamente
claro y f√°cil de verificar.</li>
</ol>
</div>
</div>
<div id="outline-container-orge71ae2b" class="outline-5">
<h5 id="orge71ae2b">Nota Bene 2</h5>
<div class="outline-text-5" id="text-orge71ae2b">
<p>
Para \(n > 2\), los eventos \(A_1, A_2, \dots , A_n\) pueden ser
independientes de a pares: \(\mathbb{P}(A_i \cap A_j) = \mathbb{P}(A_i)
\mathbb{P}(A_j), 1 \leq i < j \leq n\), pero no ser mutuamente
independientes.
</p>
</div>
</div>
<div id="outline-container-orge61bfe6" class="outline-5">
<h5 id="orge61bfe6">Ejemplo 2.3</h5>
<div class="outline-text-5" id="text-orge61bfe6">
<p>
Sea \(\Omega\) un conjunto formado por cuatro elementos: \(\omega_1,
\omega_2, \omega_3, \omega_4\) ; las correspondientes probabilidades
elementales son todas iguales a \(1 / 4\). Consideramos tres eventos:
</p>

<p>
\[A_1 = \{\omega_1, \omega_2\}, A_2 = \{\omega_1, \omega_3\}, A_3 =
\{\omega_1, \omega_4\}\]
</p>

<p>
Es f√°cil ver que los eventos \(A_1, A_2, A_3\) son independientes de a
pares, pero no son mutuamente independientes:
</p>

<p>
\[\mathbb{P}(A_1) = \mathbb{P}(A_2) = \mathbb{P}(A_3) = 1 / 2, \]
</p>

<p>
\[\mathbb{P}(A_1\cap A_2) = \mathbb{P}(A_1\cap A_3) =
\mathbb{P}(A_2\cap A_3) = 1 / 4 = (1 / 2)^2,\]
</p>

<p>
\[\mathbb{P}(A_1\cap A_2\cap A_3) = 1 / 4 \neq (1 / 2)^3\]
</p>
</div>
</div>
<div id="outline-container-org79720df" class="outline-5">
<h5 id="org79720df">Independencia y probabilidades condicionales</h5>
<div class="outline-text-5" id="text-org79720df">
<p>
Para introducir el concepto de independencia no utilizamos
probabilidades condicionales. Sin embargo, sus aplicaciones dependen
generalmente de las propiedades de ciertas probabilidades
condicionales.
</p>

<p>
Para fijar ideas, supongamos que \(n = 2\) y que las probabilidades de
los eventos \(A_1\) y \(A_2\) son positivas. En tal caso, los eventos
\(A_1\) y \(A_2\) son independientes si y solamente si
</p>

<p>
\[\mathbb{P}(A_2|A_1) = \mathbb{P}(A_2) \text{ y } \mathbb{P}(A_1|A_2)
= \mathbb{P}(A_1)\]
</p>

<p>
El siguiente Teorema expresa la relaci√≥n general entre el concepto de
independencia y las probabilidades condicionales.
</p>
</div>
</div>
<div id="outline-container-org3f864e3" class="outline-5">
<h5 id="org3f864e3">Teorema 2.4</h5>
<div class="outline-text-5" id="text-org3f864e3">
<p>
Sean \(A_1, A_2, \dots A_n\) eventos tales que todas las probabilidades
\(\mathbb{P}(A_i)\) son positivas. Una condici√≥n necesaria y suficiente
para la mutua independencia de los eventos \(A_1, A_2, \dots , A_n\) es
la satisfacci√≥n de las ecuaciones
</p>

\begin{equation}\mathbb{P}(A_i|A_{i_1} \cap A_{i_2} \cap \cdots \cap A_{i_k}) = \mathbb{P}(A_i)\end{equation}

<p>
cualesquiera sean \(i_1, i_2, \dots , i_k , i\) distintos dos a dos.
</p>
</div>
</div>
<div id="outline-container-org79e8262" class="outline-5">
<h5 id="org79e8262">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-org79e8262">
<ol class="org-ol">
<li>Se tira una moneda honesta n veces. Sea \(A\) el evento que se
obtenga al menos una cara y sea \(B\) el evento que se obtengan al
menos una cara y al menos una ceca. Analizar la independencia de
los eventos \(A\) y \(B\).</li>
<li>Andr√©s, Francisco, Jemina e Ignacio fueron amigos en la escuela
primaria. Se reencontraron en el curso 23 (PyE 61.09) de la FIUBA y
se reunieron de a parejas a charlar. Como resultado de esas
charlas, cada pareja renov√≥ su amistad con probabilidad \(1 / 2\) y
no lo hizo con probabilidad \(1 / 2\), independientemente de las
dem√°s. Posteriormente, Andr√©s recibi√≥ un rumor y lo transmiti√≥ a
todas sus amistades. Suponiendo que cada uno de los que reciba un
rumor lo transmitir√° a todas sus amistades, cu√°l es la probabilidad
de que Ignacio haya recibido el rumor transmitido por Andr√©s?.</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org908daf4" class="outline-2">
<h2 id="org908daf4">Modelos discretos</h2>
<div class="outline-text-2" id="text-org908daf4">
<p>
Los espacios muestrales m√°s simples son aquellos que contienen un
n√∫mero finito, \(n\), de puntos. Si \(n\) es peque√±o (como en el caso de
tirar algunas monedas), es f√°cil visualizar el espacio. El espacio de
distribuciones de cartas de poker es m√°s complicado. Sin embargo,
podemos imaginar cada punto muestral como una ficha y considerar la
colecci√≥n de esas fichas como representantes del espacio muestral. Un
evento \(A\) se representa por un determinado conjunto de fichas, su
complemento \(A^c\) por las restantes. De aqu√≠ falta s√≥lo un paso para
imaginar una bol con infinitas fichas o un espacio muestral con una
sucesi√≥n infinita de puntos \(\Omega = \{\omega_1, \omega_2, \omega_3,
\dots \}\).
</p>
</div>
<div id="outline-container-orga0c9b8d" class="outline-5">
<h5 id="orga0c9b8d">Definici√≥n 3.1</h5>
<div class="outline-text-5" id="text-orga0c9b8d">
<p>
Un espacio muestral se llama discreto si contiene finitos o infinitos
puntos que \(p\) ueden ordenarse en una sucesi√≥n \(\omega_1, \omega_2,
\dots\).
Sean \(\Omega\) un conjunto infinito numerable y \(\mathcal{A}\) la
\(\sigma\) - √°lgebra de todos los subconjuntos con tenidos en
\(\Omega\). Todos los espacios de probabilidad que se pueden construir
sobre \((\Omega, \mathcal{A})\) se obtienen de la siguiente manera:
</p>
<ol class="org-ol">
<li>Tomamos una sucesi√≥n de n√∫meros no negativos \(\{p(\omega) : \omega
   \in \Omega\}\) tal que \[\displaystyle\sum_{\omega \in \Omega}
   p(\omega) = 1\]</li>
<li>Para cada evento \(A \in \mathcal{A}\) definimos \(\mathbb{P}(A)\) como la suma
de las probabilidades de los eventos elementales contenidos en \(A\):</li>
</ol>

\begin{equation}
\mathbb{P}(A) := \displaystyle\sum_{\omega \in A} p (\omega)
\end{equation}
</div>
</div>

<div id="outline-container-org17aaab4" class="outline-5">
<h5 id="org17aaab4">Nombres.</h5>
<div class="outline-text-5" id="text-org17aaab4">
<p>
La funci√≥n \(p : \Omega \rightarrow [0, 1]\) que asigna probabilidades a
los eventos elementales \(\omega \in \Omega\) se llama funci√≥n de
probabilidad. La funci√≥n \(\mathbb{P} : A \rightarrow [0, 1]\) definida en (10)
se llama la medida de probabilidad inducida por p.
</p>
</div>
</div>
<div id="outline-container-orgfe78004" class="outline-5">
<h5 id="orgfe78004">Nota Bene 1</h5>
<div class="outline-text-5" id="text-orgfe78004">
<p>
De la definici√≥n (10) resultan inmediatamente las siguientes propiedades
</p>
<ol class="org-ol">
<li>Para cada \(A \in \mathcal{A}\) vale que \(\mathbb{P}(A) \geq 0\)</li>
<li>\(\mathbb{P}(\Omega) = 1\).</li>
<li>\(\sigma\) - aditividad. Si \(A_1, A_2, \dots\) es una sucesi√≥n de eventos
disjuntos dos a dos, entonces \[\mathbb{P} \left(\bigcup_{n=1}^{\infty} A_n
   \right) = \displaystyle\sum_{n=1}^{\infty} \mathbb{P}(A_n)\]</li>
</ol>
</div>
</div>
<div id="outline-container-org09da3ac" class="outline-5">
<h5 id="org09da3ac">Nota Bene 2</h5>
<div class="outline-text-5" id="text-org09da3ac">
<p>
No se excluye la posibilidad de que un punto tenga probabilidad
cero. Esta convenci√≥n parece artificial pero es necesaria para evitar
complicaciones. En espacios discretos probabilidad cero se interpreta
como imposibilidad y cualquier punto muestral del que se sabe que
tiene probabilidad cero puede suprimirse impunemente del espacio
muestral. Sin embargo, frecuentemente los valores num√©ricos de las
probabilidades no se conocen de antemano, y se requieren complicadas
consideraciones para decidir si un determinado punto muestral tiene o
no probabilidad positiva.
</p>
</div>
</div>
<div id="outline-container-orgdd7c42c" class="outline-4">
<h4 id="orgdd7c42c">Distribuci√≥n geom√©trica</h4>
<div class="outline-text-4" id="text-orgdd7c42c">
</div>
<div id="outline-container-orgbab6db4" class="outline-5">
<h5 id="orgbab6db4">Ejemplo 3.2 (Probabilidad geom√©trica)</h5>
<div class="outline-text-5" id="text-orgbab6db4">
<p>
Sea \(p\) un n√∫mero real tal que \(0 < p < 1\). Observando que
</p>

<p>
\[\displaystyle\sum_{n=1}^{\infty}(1 ‚àí p)^{n‚àí1} = \frac{1}{p}\]
</p>

<p>
se deduce que la funci√≥n \(p : N \rightarrow \Re\) definida por
</p>

<p>
\[p(n) := (1‚àíp)^{n‚àí1}p, n = 1, 2, \dots\]
</p>

<p>
define una funci√≥n de probabilidad en \(\Omega = N = \{1, 2, 3, \dots
\}\) que se conoce por el nombre de distribuci√≥n geom√©trica de
par√°metro \(p\). Esta funci√≥n de probabilidades est√° √≠ntimamente
relacionada con la cantidad de veces que debe repetirse un experimento
aleatorio para que ocurra un evento \(A\) (prefijado de antemano) cuya
probabilidad de ocurrencia en cada experimento individual es \(p\).
</p>
</div>
</div>
<div id="outline-container-orga1e72c2" class="outline-5">
<h5 id="orga1e72c2">Ejemplo 3.3</h5>
<div class="outline-text-5" id="text-orga1e72c2">
<p>
El experimento consiste en lanzar una moneda tantas veces como sea
necesario hasta que salga cara. El resultado del experimento ser√° la
cantidad de lanzamientos necesarios hasta que se obtenga cara. Los
resultados posibles son
</p>

<p>
\[\Omega = \{1, 2, 3, \dots \} \cup \{\infty\}\]
</p>

<p>
El s√≠mbolo \(\infty\) est√° puesto para representar la posibilidad de que
todas las veces que se lanza la moneda el resultado obtenido es
ceca. El primer problema que debemos resolver es asignar
probabilidades a los puntos muestrales. Una forma de resolverlo es la
siguiente. Cada vez que se arroja una moneda los resultados posibles
son cara (H) o ceca (T). Sean \(p\) y \(q\) la probabilidad de observar
cara y ceca, respectivamente, en cada uno de los
lanzamientos. Claramente, \(p\) y \(q\) deben ser no negativos y
</p>

<p>
\[p + q = 1\]
</p>

<p>
Suponiendo que cada lanzamiento es independiente de los dem√°s, las
probabilidades se multiplican. En otras palabras, la probabilidad de
cada secuencia determinada es el producto obtenido de reemplazar las
letras H y T por \(p\) y \(q\), respectivamente. As√≠,
</p>

<p>
\[\mathbb{P}(H) = p; \mathbb{P}(T H) = qp; \mathbb{P}(T T H) = qqp;
\mathbb{P}(T T T H) = qqqp\]
</p>

<p>
Puede verse que para cada \(n \in N\) la secuencia formada por \(n‚àí1\)
letras T seguida de la letra H debe tener probabilidad \(q^{n‚àí1}p = (1
‚àíp)^{n‚àí1}p\).
</p>

<p>
El argumento anterior sugiere la siguiente asignaci√≥n de
probabilidades sobre \(\Omega\): para cada \(n \in N, p(n)\), la
probabilidad de que la primera vez que se obtiene cara ocurra en el
n-√©simo lanzamiento de la moneda est√° dada por
</p>

<p>
\[p(n) = (1 ‚àí p) ^{n‚àí1}p\]
</p>

<p>
Como las probabilidades geom√©tricas suman 1 (ver el ejemplo 3.2) al
resultado <i>ceca en todos los tiros</i>  se le debe asignar probabilidad
\(p(\infty) = 0\). Como el espacio muestral es discreto no hay problema
en suprimir el punto \(\infty\).
</p>

<p>
Consideremos el evento \(A\) = <i>se necesitan una cantidad par de tiros
para obtener la primer cara</i>. Entonces,
</p>

<p>
\[A = \{2, 4, 6, 8, \dots \}\]
</p>

<p>
y
</p>

\begin{align*}
\mathbb{P}(A) &= \displaystyle\sum_{\omega \in A} p(\omega) = \displaystyle\sum_{k=1}^{\infty} p(2k) = \displaystyle\sum_{k=1}^{\infty} q^{2k‚àí1} p = pq \displaystyle\sum_{k=0}^{\infty} q^{2k} = pq \left(\frac{1}{1‚àíq^2} \right) \\
&= \frac{pq} {(1 ‚àíq)(1 + q)} = \frac{q}{1 + q} = \frac{1 ‚àí p}{2 ‚àí p}
\end{align*}
</div>
</div>

<div id="outline-container-org94bcdbd" class="outline-5">
<h5 id="org94bcdbd">Ejemplo 3.4</h5>
<div class="outline-text-5" id="text-org94bcdbd">
<p>
Lucas y Monk juegan a la moneda. Lanzan una moneda equili brada al
aire, si sale cara, Lucas le gana un peso a Monk; si sale ceca, Monk
le gana un peso a Lucas. El juego termina cuando alguno gana dos veces
seguidas.
</p>

<p>
El espacio muestral asociado a este experimento aleatorio es
</p>

<p>
\[\Omega = \{HH, T T, HT T, T HH, HT HH, T HT T, \dots \}\]
</p>

<p>
Como podemos tener secuencias de cualquier longitud de caras y cecas
alternadas, el espacio muestral es necesariamente infinito.  El evento
\(A_1 =\) <i>la moneda fue lanzada como m√°ximo tres veces</i>  est√° dado por
todos los elementos de \(\Omega\) que tienen longitud menor o igual que
tres:
</p>

<p>
\[A_1 = \{HH, T T, HT T, T HH\}\]
</p>

<p>
y su probabilidad es
</p>

<p>
\[\mathbb{P}(A_1) = \mathbb{P}(HH) + \mathbb{P}(T T) +
\mathbb{P}(HTT) + \mathbb{P}(THH) = \frac{1}{4} +\frac{1}{4} +
\frac{1}{8} + \frac{1}{8} = \frac{3}{4}\]
</p>

<p>
El evento \(A_2 =\) <i>ceca en el primer lanzamiento</i>  est√° dado por
todos los elementos de \(\Omega\) que comienzan con T :
</p>

<p>
\[A_2 = \{T T, T HH, T HT T, T HT HH, \dots \}\]
</p>

<p>
y su probabilidad es
</p>

<p>
\[\mathbb{P}(A_2) = \mathbb{P}(T T) + \mathbb{P}(T HH) + \mathbb{P}(T
HTT) + \mathbb{P}(T HT HH) + \cdots = \frac{1}{2^2} +
\frac{1}{2^3} + \frac{1}{2^4} + \frac{1}{2^5} + \cdots = \frac{1}{2}\]
</p>

<p>
¬øCu√°l es la probabilidad de que el juego termine alguna vez? Si
definimos los eventos \(A_n\) := <i>el juego termina en la n-√©sima
jugada</i>, \(n \geq 2\), tendremos que el evento <i>el juego termina
alguna vez</i> es la uni√≥n disjunta de los eventos \(A_1, A_2, \dots\) , y
por lo tanto su probabilidad es la suma de las probabilidades de los
eventos \(A_n\). Para cada \(n \geq 2\) la probabilidad de \(A_n\) es
</p>

<p>
\[\mathbb{P}(A_n) = \frac{2}{2^n} = \frac{1}{2^{n‚àí1}}\]
</p>

<p>
En consecuencia la probabilidad de que el juego termine alguna vez es
</p>

<p>
\[\displaystyle\sum_{n \geq 2} \frac{1}{2^{n‚àí1}} =
\displaystyle\sum_{n \geq 1} \frac{1}{2^n} = 1\]
</p>
</div>
</div>
</div>

<div id="outline-container-org1b472bb" class="outline-4">
<h4 id="org1b472bb">Distribuci√≥n de Poisson</h4>
<div class="outline-text-4" id="text-org1b472bb">
</div>
<div id="outline-container-orgc7989f1" class="outline-5">
<h5 id="orgc7989f1">Ejemplo 3.5 (Probabilidad de Poisson)</h5>
<div class="outline-text-5" id="text-orgc7989f1">
<p>
Sea \(\lambda\) un n√∫mero real positivo. Observando que
</p>

<p>
\[e^{\lambda} = \displaystyle\sum_{n=0}^{\infty} \frac{\lambda^n}{n!}\]
</p>

<p>
se deduce que la funci√≥n \(p : N_0 \rightarrow \Re\) definida por
</p>

<p>
\[p(n) := e^{‚àí \lambda} \frac{\lambda^n}{n!} , n = 0, 1, 2, \dots\]
</p>

<p>
define una funci√≥n de probabilidad en \(\Omega = N_0 = \{0, 1, 2, \dots
\}\), conocida como la distribuci√≥n de Poisson de intensidad \(\lambda\).
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org854bdfe" class="outline-2">
<h2 id="org854bdfe">Modelos continuos</h2>
<div class="outline-text-2" id="text-org854bdfe">
</div>
<div id="outline-container-orga951a9b" class="outline-3">
<h3 id="orga951a9b">Puntos al azar sobre un segmento. La distribuci√≥n uniforme</h3>
<div class="outline-text-3" id="text-orga951a9b">
<p>
Elegir un punto al azar dentro de un segmento de recta de longitud
finita es un experimento conceptual intuitivamente claro. Desde el
punto de vista te√≥rico el experimento debe describirse mediante un
espacio de probabilidad \((\Omega, \mathcal{A},\mathbb{P})\).
</p>

<p>
No se pierde generalidad, si se supone que la longitud del segmento es
la unidad y se lo identifica con el intervalo \(\Omega = [0, 1]\). La
\(\sigma\) - √°lgebra de eventos \(\mathcal{A}\) y la medida de
probabilidad \(\mathbb{P} : \mathcal{A} \rightarrow \Re\) se construyen por
etapas.
</p>

<ol class="org-ol">
<li>Definimos \(\mathcal{A}_0\) como la familia de los intervalos
contenidos en \(\Omega\) de la forma \([a, b], [a, b),(a, b]\) o \((a,
   b), a \leq b\) (notar que \(\mathcal{A}_0\) no es un √°lgebra) y
definimos \(\mathbb{P}_0 : \mathcal{A}_0 \rightarrow \Re\) de la siguiente
manera: \[\mathbb{P}_0 (A) := longitud(A) = b ‚àí a\], si los extremos del
intervalo \(A\) son \(a\) y \(b\).</li>
<li>La familia \(\mathcal{A}_1\) de todas las uniones finitas de
conjuntos disjuntos de \(\mathcal{A}_0\) es un √°lgebra de eventos y
la funci√≥n \(P1 : \mathcal{A}_1 \rightarrow \Re\) definida por
\[\mathbb{P}_1(A) := \displaystyle\sum_{i=1}^k \mathbb{P}_0(A_i), \text{ si }A =
   \bigcup{i=1}^k A_i,\] donde \(A_1 , \dots , A_k \in \mathcal{A}_0\) y
\(A_i \cap A_j = \emptyset\) para toda pareja de √≠ndices \(i \neq j\),
es una medida de probabilidad (pues satisface los axiomas I-IV).</li>
<li>El teorema de extensi√≥n se ocupa del resto: la medida de
probabilidad \(\mathbb{P}_1\) definida sobre el √°lgebra \(\mathcal{A}_1\) se
extiende un√≠vocamente a una medida de probabilidad \(P\) definida
sobre la \(\sigma\) - √°lgebra generada por \(\mathcal{A}_1,
   \mathcal{A} := \sigma(A_1)\).</li>
</ol>
</div>

<div id="outline-container-org89e03ec" class="outline-5">
<h5 id="org89e03ec">Nota Bene</h5>
<div class="outline-text-5" id="text-org89e03ec">
<p>
Esta definici√≥n de probabilidad que a cada intervalo \(A \subset [0,
1]\) le asigna su respectiva longitud se llama la <i>distribuci√≥n
uniforme sobre el intervalo</i> \([0, 1]\) y constituye una generalizaci√≥n
de la noci√≥n de equiprobabilidad sobre la que se basa la definici√≥n de
Laplace de la probabilidad para espacios finitos: <i>casos favorables
sobre casos posibles</i>.
</p>
</div>
</div>
</div>

<div id="outline-container-org61ee12e" class="outline-3">
<h3 id="org61ee12e">Geometr√≠a y probabilidad</h3>
<div class="outline-text-3" id="text-org61ee12e">
<p>
Una construcci√≥n completamente an√°loga a la de la secci√≥n anterior
permite describir te√≥ricamente el experimento conceptual,
intuitivamente claro, que consiste en elegir un punto al azar dentro
de una regi√≥n plana, \(\Lambda \subset \Re^2\) , de √°rea finita y no
nula. Para fijar ideas, se puede imaginar que la regi√≥n plana es un
blanco sobre el que se arroja un dardo.
</p>
</div>

<div id="outline-container-org570735e" class="outline-5">
<h5 id="org570735e">Ejemplo 4.1 (Dardos)</h5>
<div class="outline-text-5" id="text-org570735e">
<p>
El juego de dardos consiste en tirar un dardo contra un blanco
circular. Supongamos que disparamos un dardo (que acertamos al blanco)
y observamos d√≥nde se clav√≥. Naturalmente, los resultados posibles de
este experimento son todos los puntos del blanco. No se pierde
generalidad si se supone que el centro del blanco es el origen de
\(\Re^2\) y que su radio es 1. En tal caso el espacio muestral de este
experimento es
</p>

<p>
\[\Omega = \{(x, y) \in \Re^2 : x^2 + y^2 \leq 1\}\]
</p>

<p>
Intuitivamente, la probabilidad de acertarle a un punto predeterminado
(arbitrario) deber√≠a ser cero. Sin embargo, la probabilidad de que el
dardo se clave en cualquier subconjunto (<i>gordo</i> ) \(A\) del blanco
deber√≠a ser proporcional a su √°rea y determinarse por la fracci√≥n del
√°rea del blanco contenida en \(A\). En consecuencia, definimos
</p>

<p>
\[\mathbb{P}(A) := \frac{\text{√°rea de } A}{\text{√°rea del blanco}} =
\frac{\text{√°rea de } A}{\pi}\]
</p>

<p>
Por ejemplo, si \(A = \{(x, y) : x^2 + y^2 \leq r^2\}\) es el evento que el dardo caiga a distancia \(r < 1\)
del centro del blanco, entonces
</p>

<p>
\[\mathbb{P}(A) = \frac{\pi r^2}{\pi} = r^2\]
</p>
</div>
</div>

<div id="outline-container-org2c5c5a5" class="outline-5">
<h5 id="org2c5c5a5">Puntos al azar en regiones planas</h5>
<div class="outline-text-5" id="text-org2c5c5a5">
<p>
Si hacemos abstracci√≥n de la forma circular del blanco y de la
sem√°ntica involucrada en el juego de dardos, obtenemos un modelo
probabil√≠stico para el experimento conceptual que consiste en
<i>sortear</i>  o elegir un punto al azar en una regi√≥n plana \(\Lambda
\subset \Re^2\) de √°rea finita y positiva. El espacio muestral es la
regi√≥n plana, \(\Omega = \Lambda\), la \(\sigma\) - √°lgebra de los eventos,
\(A\), es la familia de todos los subconjuntos de \(\Lambda\) a los que se
les puede medir el √°rea y la probabilidad de cada evento \(A\) es la
fracci√≥n del √°rea de \(\Lambda\) contenida en \(A\). Esto es,
</p>

\begin{equation}\mathbb{P}(A) := \frac{\text{√°rea}(A)}{\text{√°rea}(\Lambda)}\end{equation}

<p>
Esta forma de asignar probabilidades es la equivalente para el caso
continuo de la f√≥rmula casos favorables sobre casos posibles utilizada
en espacios muestrales finitos para modelar experimentos aleatorios
con resultados equiprobables.
</p>
</div>
</div>
<div id="outline-container-org78ea500" class="outline-5">
<h5 id="org78ea500">Nota Bene</h5>
<div class="outline-text-5" id="text-org78ea500">
<p>
Si en lugar de elegir un punto al azar dentro del segmento \([a, b]\)
elegimos dos puntos de manera independiente, el experimento tendr√° por
resultado un par de n√∫meros reales contenidos en \([a, b]\). El espacio
muestral ser√° el cuadrado de lado \([a, b], \Omega = [a, b] \times [a,
b]\).  En este espacio la asignaci√≥n de probabilidades definida en (11)
resulta consistente con la noci√≥n de independencia.
</p>
</div>
</div>

<div id="outline-container-org832ea8c" class="outline-5">
<h5 id="org832ea8c">Ejemplo 4.2.</h5>
<div class="outline-text-5" id="text-org832ea8c">
<p>
Se eligen al azar (y en forma independiente) dos puntos \(x_1\) y \(x_2\)
dentro de un segmento de longitud \(L\). Hallar la probabilidad de que
la longitud del segmento limitado por los puntos \(x_1\) y \(x_2\) resulte
menor que \(L/2\).
</p>

<p>
Figura 6: La regi√≥n sombreada corresponde al evento \(A\) = <i>la
longitud del segmento limitado por los puntos</i> \(x_1\) <i>y</i> \(x_2\) <i>resulte
menor que</i> \(L/2\).
</p>

<p>
El espacio muestral de este experimento es un cuadrado de lado \(L\) que
puede representarse en la forma \(\Omega = \{(x_1 , x_2) : 0 \leq x_1 \leq
L, 0 \leq x_1 \leq L\}\).
</p>

<p>
El evento \(A\) =  <i>la longitud del segmento limitado por los puntos</i>
\(x_1\) <i>y</i> \(x_2\) <i>resulte menor que</i> \(L/2\) puede ocurrir de dos maneras
distintas:
</p>

<ol class="org-ol">
<li>si \(x_1 \leq x_2\) , se debe cumplir la desigualdad \(x_2 ‚àí x_1 < L/2\)</li>
<li>si \(x_2 < x_1\), debe cumplirse la desigualdad \(x_1 ‚àí x_2 < L/2\).</li>
</ol>

<p>
Observando la Figura 6 est√° claro que el √°rea del evento \(A\) se
obtiene restando al √°rea del cuadrado de lado \(L\) el √°rea del cuadrado
de lado \(L/2\):
</p>

<p>
\[\text{√°rea de }A = L^2 - \frac{L^2}{4} = \frac{3}{4}L^2\]
</p>

<p>
Como el √°rea total del espacio muestral es \(L^2\), resulta que
\(\mathbb{P}(A) = 3 / 4\).
</p>
</div>
</div>

<div id="outline-container-org1c72b66" class="outline-5">
<h5 id="org1c72b66">Ejemplo 4.3 (Las agujas de BuÔ¨Äon).</h5>
<div class="outline-text-5" id="text-org1c72b66">
<p>
Una aguja de longitud \(2l\) se arroja sobre un plano dividido por
rectas paralelas. La distancia entre rectas es \(2a\). Suponiendo que \(l
< a\), cu√°l es la probabilidad de que la aguja intersecte alguna de las
rectas?
</p>

<p>
Localizamos la aguja mediante la distancia &rho; de su centro a la
recta m√°s cercana y el √°ngulo agudo \(\theta\) entre la recta y la
aguja: \(0 \leq \rho \leq a\) y \(0 \leq \theta \leq \pi/2\). El
rect√°ngulo determinado por esas desigualdades es el espacio muestral
\(\Omega\). El evento \(A =\) <i>la aguja interesecta l a recta</i>  ocurre si
\(\rho \leq l sen \theta\). La probabilidad de \(A\) es el cociente del
√°rea de la figura determinada por las tres desigualdades \(0 \leq \rho
\leq a, 0 \leq \theta \leq \pi/2\) y \(\rho \leq l sen \theta\) y el √°rea
del rect√°ngulo \(\pi a/2\).
</p>

<p>
El √°rea de la figura es \(\int_0^{\pi/2}l \sin(\theta) d\theta =
l\). Por lo tanto, la probabilidad de intersecci√≥n es
</p>

\begin{equation}\mathbb{P}(A) = \frac{2l}{\pi a}\end{equation}

<p>
La f√≥rmula (12) indica un m√©todo aleatorio para estimar \(\pi\): arrojar
la aguja \(n\) veces sobre el plano y contar \(n(A)\) la cantidad de veces
que la aguja interesect√≥ alguna recta:
</p>

<p>
\[\hat{\pi} = \frac{2l}{a}\frac{n}{n(A)}\]
</p>
</div>
</div>
</div>

<div id="outline-container-org29dc0ee" class="outline-3">
<h3 id="org29dc0ee">Paradoja de Bertrand</h3>
<div class="outline-text-3" id="text-org29dc0ee">
<p>
Se dibuja una cuerda aleatoria CD sobre el c√≠rculo de radio 1. ¬øCu√°l
es la probabilidad que la longitud de la cuerda CD supere \(\sqrt{3}\),
la longitud del lado del tri√°ngulo equil√°tero inscripto en dicho
c√≠rculo?
</p>

<p>
Este es un ejemplo de un problema planteado de manera incompleta. La
pregunta que debe formularse es la siguiente ¬øqu√© significa elegir
<i>aleatoriamente</i> ? Bertrand propuso tres respuestas diferentes a esa
pregunta. Las diferentes respuestas corresponden en realidad a
diferentes modelos probabil√≠sticos, i.e., diferentes espacios de
probabilidad concretos \((\Omega, \mathcal{A},\mathbb{P})\).
</p>
</div>

<div id="outline-container-orga45fae4" class="outline-5">
<h5 id="orga45fae4">Primer modelo</h5>
<div class="outline-text-5" id="text-orga45fae4">
<p>
Sea \(\Omega_1\) la bola de radio 1, \(\Omega_1 = \{(x, y) \in \Re^2:
x^2 + y^2 \leq 1\}\), con la \(\sigma\) - √°lgebra \(\mathcal{A}\) de los
<i>subconjuntos cuya √°rea est√° definida</i> . Para cada \(A \in
\mathcal{A}\),
</p>

<p>
\[_1(A) = \frac{\text{√°rea}(A)}{\text{√°rea}(\Omega)} = {\text{√°rea}(A)}{\pi}\]
</p>

<p>
C y D se construyen del siguiente modo: usando la ley de distribuci√≥n
\(\mathbb{P}_1\) se sortea un punto \(\omega\) sobre la bola de radio 1 y CD es
perpendicular al segmento \(\overline{0\omega}\) cuyos extremos son \((0,
0)\) y \(\omega\). La longitud de CD es una funci√≥n de \(\omega\) que
llamaremos \(\ell(\omega)\). Queremos calcular \(\mathbb{P}_1(\ell(\omega) \geq
\sqrt{3})\). Notar que
</p>

<p>
\[\ell(\omega) \geq \sqrt{3} \iff longitud(\overline{0\omega}) \geq \frac{1}{2}\]
</p>

<p>
Por lo tanto,
</p>

<p>
\[\mathbb{P}_1(\ell(\omega) \geq \sqrt{3}) = \frac{\pi ‚àí \pi/4}{\pi} = \frac{3}{4}\]
</p>
</div>
</div>

<div id="outline-container-org224bbc8" class="outline-5">
<h5 id="org224bbc8">Segundo modelo</h5>
<div class="outline-text-5" id="text-org224bbc8">
<p>
Sea \(\Omega_2\) el c√≠rculo de radio 1, \(\Omega_2 = \{(x, y) \in \Re2 :
x^2+ y^2 = 1\}\), con la \(\sigma\) - √°lgebra \(\mathcal{A}\) de los
<i>subconjuntos cuya longitud est√° definida</i> . Para cada \(A \in
\mathcal{A}\),
</p>

<p>
\[\mathbb{P}_2(A) = \frac{longitud(A)}{longitud(\Omega)} = \frac{longitud(A)}{2
\pi}\]
</p>

<p>
C y D se construyen del siguiente modo: Se fija el punto C; con la ley
\(\mathbb{P}_2\) se sortea un punto \(\omega\) sobre el c√≠rculo de radio 1 y se
pone \(D = \omega\). La longitud de CD es una una funci√≥n de \(\omega\)
que llamaremos \(\ell(\omega)\). El conjunto \(\{\omega : \ell(\omega)
\geq \sqrt{3}\}\) es el segmento del c√≠rculo determinado dos v√©rtices
del tri√°ngulo equil√°tero inscripto en el c√≠rculo, a saber: los del
lado opuesto al v√©rtice C. Por lo tanto,
</p>

<p>
\[\mathbb{P}_2(\ell(\omega) \geq \sqrt{3}) = \frac{2\pi / 3}{2 \pi} = \frac{1}{3}\]
</p>
</div>
</div>

<div id="outline-container-org3d46648" class="outline-5">
<h5 id="org3d46648">Tercer modelo.</h5>
<div class="outline-text-5" id="text-org3d46648">
<p>
Sea \(\Omega_3\) el intervalo \([0, 1]\) con la \(\sigma\) - √°lgebra
\(\mathcal{A}\) de los <i>subconjuntos cuya longitud est√°
definida</i>. Para cada \(A \in \mathcal{A}\),
</p>

<p>
\[\mathbb{P}_3(A) = longitud(A)\]
</p>

<p>
C y D se construyen del siguiente modo: se sortea un punto \(\omega\)
sobre el intervalo \([0, 1]\) del eje \(x\) y CD es la cuerda
perpendicular al eje \(x\) que pasa por \(\omega\). Es claro que,
</p>

<p>
\[\ell(\omega) \geq \sqrt{3} \iff \omega \in [1 / 2, 1]\]
</p>

<p>
Por lo tanto, la tercer respuesta es \(1 / 2\).
</p>
</div>
</div>

<div id="outline-container-org4f2054b" class="outline-5">
<h5 id="org4f2054b">Nota Bene</h5>
<div class="outline-text-5" id="text-org4f2054b">
<p>
Obtuvimos 3 respuestas diferentes: 1 / 4, 1 / 3 y 1 / 2. Sin embargo,
no hay porque sorprenderse debido a que los modelos probabil√≠sticos
correspondientes a cada respuesta son diferentes. Cu√°l de los tres es
el <i>bueno</i>  es otro problema. El modelo correcto depende del
mecanismo usado para dibujar la cuerda al azar. Los tres mecanismos
anteriores son puramente intelectuales, y muy probablemente, no
corresponden a ning√∫n mecanismo f√≠sico.  Para discriminar entre
modelos probabil√≠sticos en competencia se debe recurrir al an√°lisis
estad√≠stico que esencialmente se basa en dos resultados de l a Teor√≠a
de Probabilidad: la ley fuerte de los grandes n√∫meros y el teorema
central del l√≠mite.
</p>
</div>
</div>
</div>
<div id="outline-container-org1aba3ca" class="outline-3">
<h3 id="org1aba3ca">De las masas puntuales a la masa continua</h3>
<div class="outline-text-3" id="text-org1aba3ca">
<p>
Para concluir est√° secci√≥n mostraremos un par de m√©todos para
construir medidas de probabilidad sobre \(\Re^n\).
</p>
</div>

<div id="outline-container-orgbc7e754" class="outline-5">
<h5 id="orgbc7e754">Masas puntuales.</h5>
<div class="outline-text-5" id="text-orgbc7e754">
<p>
Tomamos una sucesi√≥n de puntos \(\{x_1, x_2, \dots \} \in \Re^n\) y una
sucesi√≥n de n√∫meros no negativos \(\{p(x_1), p(x_2), \dots \}\) tales
que
</p>

<p>
\[\displaystyle\sum_{i=1}^{ \infty} p(x_i) = 1\]
</p>

<p>
y para cada \(A \subset \Re^n\) definimos \(\mathbb{P}(A)\) como la suma
de las <i>masas puntuales</i>  , \(p(x_i)\), de los puntos \(x_i\) contenidos
en \(A\):
</p>

<p>
\[\mathbb{P}(A) := \displaystyle\sum_{x_i \in A} p(x_i)\]
</p>
</div>
</div>

<div id="outline-container-org550e230" class="outline-5">
<h5 id="org550e230">Nota Bene</h5>
<div class="outline-text-5" id="text-org550e230">
<p>
El m√©todo de las masas puntuales puede generalizarse de la siguiente
forma: la suma \(\sum_{x_i}\) se reemplaza por la integral \(\int dx\) y
las masas puntuales \(p(x_i)\) por una funci√≥n \(\rho(x)\) denominada
densidad de probabilidades. Esta metodolog√≠a es de uso com√∫n en
mec√°nica: primero se consideran sistemas con masas puntuales discretas
donde cada punto tiene masa finita y despu√©s se pasa a la noci√≥n de
distribuci√≥n de masa continua, donde cada punto tiene masa cero. En el
primer caso, la masa total del sistema se obtiene simplemente sumando
las masas de los puntos individuales; en el segundo caso, las masas se
calculan mediante integraci√≥n sobre densidades de masa. Salvo por las
herramientas t√©cnicas requeridas, no hay diferencias esenciale s entre
ambos casos.
</p>
</div>
</div>
<div id="outline-container-orgea862d8" class="outline-5">
<h5 id="orgea862d8">Definici√≥n 4.4</h5>
<div class="outline-text-5" id="text-orgea862d8">
<p>
Una densidad de probabilidades sobre \(\Re^n\) es una funci√≥n (<i>m√°s o
menos razonable</i>) no negativa \(\rho : \Re^n \rightarrow \Re^{+}\) tal
que
</p>

<p>
\[\int_{\Re^n} \rho(x) dx = 1\]
</p>
</div>
</div>

<div id="outline-container-org2ebe5c4" class="outline-5">
<h5 id="org2ebe5c4">Masa continua.</h5>
<div class="outline-text-5" id="text-org2ebe5c4">
<p>
Tomamos una densidad de probabilidades \(\rho : \Re^n \rightarrow
\Re^{+}\) y para cada subconjunto \(A \subset \Re^n\) (<i>m√°s o menos
razonable</i>) y definimos \(\mathbb{P}(A)\) como la integral de la
densidad \(\rho(x)\) sobre el conjunto \(A\):
</p>

<p>
\[\mathbb{P}(A) := \int_A \rho(x)dx\]
</p>
</div>
</div>

<div id="outline-container-org13a5860" class="outline-5">
<h5 id="org13a5860">Ejemplo 4.5 (Gaussiana)</h5>
<div class="outline-text-5" id="text-org13a5860">
<p>
La funci√≥n \(\rho : \Re^2 \rightarrow \Re^+\) definida por
</p>

<p>
\[\rho (x, y) = \frac{1}{2 \pi} exp\left(‚àí\frac{x^2 +
y^2}{2}\right)\]
</p>

<p>
es una densidad de probabilidades sobre \(\Re^2\) denominada gaussiana
bidimensional. En efecto,
</p>

\begin{align*}
\iint_{\Re^2} 2 \pi \rho(x, y) dx dy &= \iint_{\Re^2} exp\left(‚àí\frac{x^2 + y^2}{2}\right)dxdy\\
&= 2 \iint_{\Re^2}exp\left(‚àíx^2 +y^2\right) dx dy\\
&= 2 \int_0^{2\pi}\left(\int_0^{\infty} e^{-\rho^2}\rho d\rho \right) d\theta\\
&=   \int_0^{2\pi}\left(\int_0^{\infty} e^{-\rho^2}2\rho d\rho \right) d\theta\\
&= 2\pi
\end{align*}
</div>
</div>

<div id="outline-container-orgf2f512a" class="outline-5">
<h5 id="orgf2f512a">Nota Bene</h5>
<div class="outline-text-5" id="text-orgf2f512a">
<p>
Observando con cuidado las identidades (13) se puede ver que
</p>

<p>
\[\int_{\Re} e^{‚àíx^2/2} dx = \sqrt{2 \pi}\]
</p>

<p>
Por lo tanto, la funci√≥n \(\varphi : \Re \rightarrow \Re^+\) definida
por \(\varphi (x) = \frac{1}{\sqrt{2 \pi}}e^{-x^2/2}\) es una densidad
de probabilidades sobre \(\Re\).
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgeccdd41" class="outline-2">
<h2 id="orgeccdd41">Bibliograf√≠a consultada</h2>
<div class="outline-text-2" id="text-orgeccdd41">
<p>
Para redactar estas notas se consultaron los siguientes libros:
</p>
<ol class="org-ol">
<li>Bertsekas, D. P., Tsitsiklis, J. N.: Introduction to
Probability. M.I.T. Lecture Notes. (2000)</li>
<li>Br√©maud, P.: An Introduction to Probabilistic Modeling. Springer,
New York. (1997)</li>
<li>Durrett, R. Elementary Probability for Applications. Cambridge
University Press, New York. (2009)</li>
<li>Feller, W.: An introduction to Probability Theory and Its
Applications. Vol. 1. John Wiley &amp; Sons, New York. (1957)</li>
<li>Grinstead, C. M. &amp; Snell, J. L. Introduction to
Probability. American Mathematical Society. (1997)</li>
<li>Meester, R.: A Natural Introduction to Probability
Theory. Birkhauser, Berlin. (2008)</li>
<li>Meyer, P. L.: Introductory Probability and Statistical
Applications. Addison-Wesley, Massachusetts. (1972)</li>
<li>Ross, S. M: Introduction to Probability and Statistics foe
Engineers and Scientists. Elsevier Academic Press, San
Diego. (2004)</li>
<li>Skorokhod, A. V.: Basic Principles and Applications of Probability
Theory. Springer Verlag, Berlin. (2005)</li>
<li>Soong, T. T.: Fundamentals of Probability and Statistics for
Engineers. John Wile y &amp; Sons Ltd. (2004)</li>
</ol>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
Rigurosamente, \(\mathbb{P}(B | A_n)\) est√° definida cuando \(\mathbb{P}(A_n) > 0\),
por lo cual en la f√≥rmula (4) interpretaremos que \(\mathbb{P}(B | A_n) \mathbb{P}(A_n) = 0\)
cuando \(\mathbb{P}(A_n) = 0\).
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
Last update: 2019-04-24 01:09
</div>
</body>
</html>