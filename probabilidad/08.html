<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-03-06 Wed 22:31 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Normalidad y Teorema Central del LÃ­mite</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/res/org.css"/>
                                        <script type="text/javascript" src="/res/org-info.js">

<script type="text/javascript" src="/style/org-info.js">
/**
 *
 * @source: /style/org-info.js
 *
 * @licstart  The following is the entire license notice for the
 *  JavaScript code in /style/org-info.js.
 *
 * Copyright (C) 2012-2018 Free Software Foundation, Inc.
 *
 *
 * The JavaScript code in this tag is free software: you can
 * redistribute it and/or modify it under the terms of the GNU
 * General Public License (GNU GPL) as published by the Free Software
 * Foundation, either version 3 of the License, or (at your option)
 * any later version.  The code is distributed WITHOUT ANY WARRANTY;
 * without even the implied warranty of MERCHANTABILITY or FITNESS
 * FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.
 *
 * As additional permission under GNU GPL version 3 section 7, you
 * may distribute non-source (e.g., minimized or compacted) forms of
 * that code without the copy of the GNU GPL normally required by
 * section 4, provided you include this license notice and a URL
 * through which recipients can access the Corresponding Source.
 *
 * @licend  The above is the entire license notice
 * for the JavaScript code in /style/org-info.js.
 *
 */
</script>

<script type="text/javascript">

/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/

<!--/*--><![CDATA[/*><!--*/
org_html_manager.set("TOC_DEPTH", "2");
org_html_manager.set("LINK_HOME", "0");
org_html_manager.set("LINK_UP", "0");
org_html_manager.set("LOCAL_TOC", "0");
org_html_manager.set("VIEW_BUTTONS", "0");
org_html_manager.set("MOUSE_HINT", "underline");
org_html_manager.set("FIXED_TOC", "0");
org_html_manager.set("TOC", "1");
org_html_manager.set("VIEW", "showall");
org_html_manager.setup();  // activate after the parameters are set
/*]]>*///-->
</script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Normalidad y Teorema Central del LÃ­mite</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org202e6a1">La distribuciÃ³n normal</a>
<ul>
<li><a href="#org21da136">PresentaciÃ³n</a>
<ul>
<li>
<ul>
<li><a href="#orgacffe57">DefiniciÃ³n 1.1.</a></li>
<li><a href="#orga31c574">Lema 1.2. Para cada x &gt; 0 valen las siguientes desigualdades:}</a></li>
<li><a href="#orgf45e3c8">DemostraciÃ³n</a></li>
<li><a href="#orgef60b61">Nota Bene</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org215b8da">Cuentas con normales</a>
<ul>
<li>
<ul>
<li><a href="#org39becce">Nota Bene</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org508d6a8">Ejemplos</a>
<ul>
<li>
<ul>
<li><a href="#org34edfc9">Ejemplo 1.3.</a></li>
<li><a href="#org0e6c74b">Nota Bene sobre grandes desvÃ­os. Sea X una variable aleatoria con distribuciÃ³n normal}</a></li>
<li><a href="#orgfd1e8ef">Ejemplo 1.4.</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org5f4415d">Suma de normales independientes</a>
<ul>
<li>
<ul>
<li><a href="#orgf3fd656">Lema 1.5. Sean X}</a></li>
<li><a href="#orgc702f87">DemostraciÃ³n</a></li>
<li><a href="#orgc0fc9ec">Nota Bene</a></li>
<li><a href="#org046e23f">Teorema 1.6. Sean X}</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org5275de1">GÃ©nesis de la distribuciÃ³n normal</a>
<ul>
<li><a href="#org3c87427">Teorema lÃ­mite de De Moivre - Laplace</a>
<ul>
<li>
<ul>
<li><a href="#org39cb05d">Teorema 2.1 (Teorema lÃ­mite de De Moivre-Laplace). Consideramos una sucesiÃ³n de en-</a></li>
<li><a href="#orge33141b">DemostraciÃ³n</a></li>
<li><a href="#org5b64e55">Nota Bene</a></li>
<li><a href="#orge1815ba">Nota Bene</a></li>
<li><a href="#org1237073">Ejemplo 2.2.</a></li>
<li><a href="#org62779a1">Ejemplo 2.3.</a></li>
<li><a href="#orgbe981b9">Ejemplo 2.4.</a></li>
<li><a href="#org75ade87">Nota Bene</a></li>
<li><a href="#orgb6a9941">Ejemplo 2.5 (Encuesta electoral). Queremos estimar la proporciÃ³n del electorado que pre</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org39cd828">Teorema central del lÃ­mite</a>
<ul>
<li>
<ul>
<li>
<ul>
<li><a href="#org3f6d056">Teorema 3.1 (Teorema Central del LÃ­mite). Sea X</a></li>
<li><a href="#org14387ab">DemostraciÃ³n</a></li>
<li><a href="#orgfa29bfc">Corolario 3.2. Sea X}</a></li>
<li><a href="#org414e697">DemostraciÃ³n</a></li>
<li><a href="#orgc06cd52">Nota Bene</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org0a87959">Ejemplos</a>
<ul>
<li>
<ul>
<li><a href="#org9d8204c">Ejemplo 3.3 (Suma de uniformes). Puesto que la distribuciÃ³n uniforme sobre}</a></li>
<li><a href="#org808f1a8">Ejemplo 3.4.</a></li>
<li><a href="#org1f2cd38">Ejemplo 3.5 (Suma de exponenciales). La suma S</a></li>
<li><a href="#orgd34090f">Ejemplo 3.6.</a></li>
<li><a href="#org81c1d93">Ejemplo 3.7.</a></li>
<li><a href="#org0acaf94">Ejemplo 3.8.</a></li>
<li><a href="#orga08d2a0">Ejemplo 3.9.</a></li>
<li><a href="#orgd7c6ad0">Ejercicios adicionales</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#orge6bcfd3">Distribuciones relacionadas con la Normal</a>
<ul>
<li><a href="#org5a9e1b9">&Chi;<sup>2</sup> (chi-cuadrado)</a>
<ul>
<li>
<ul>
<li><a href="#org201ae22">DefiniciÃ³n 4.1 (DistribuciÃ³n chi-cuadrado con un grado de libertad). Si Z es una una vari</a></li>
<li><a href="#org93cf5ad">Nota Bene</a></li>
<li><a href="#orgd1af761">DefiniciÃ³n 4.2 (DistribuciÃ³n chi-cuadrado). Si U</a></li>
<li><a href="#org37b3cf3">Nota Bene</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgdecc5b2">t de Student</a>
<ul>
<li>
<ul>
<li><a href="#org2f64816">DefiniciÃ³n 4.3 (La distribuciÃ³n t de Student). Sean Z y U variables aleatorias independientes</a></li>
<li><a href="#orgd485775">ObservaciÃ³n 4.4. Notar que la densidad de t}</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#org80991d2">F de Fisher</a>
<ul>
<li>
<ul>
<li><a href="#org3eb7e40">DefiniciÃ³n 4.5 (DistribuciÃ³n F ). Sean U y V variables aleatorias independientes con dis-</a></li>
<li><a href="#org10b4d99">Nota Bene</a></li>
<li><a href="#orgfbd9d24">ObservaciÃ³n 4.6. Notar que de las igualdades}</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#org60cda05">BibliografÃ­a consultada</a></li>
</ul>
</div>
</div>
<div id="outline-container-org202e6a1" class="outline-2">
<h2 id="org202e6a1">La distribuciÃ³n normal</h2>
<div class="outline-text-2" id="text-org202e6a1">
</div>
<div id="outline-container-org21da136" class="outline-3">
<h3 id="org21da136">PresentaciÃ³n</h3>
<div class="outline-text-3" id="text-org21da136">
</div>
<div id="outline-container-orgacffe57" class="outline-5">
<h5 id="orgacffe57">DefiniciÃ³n 1.1.</h5>
<div class="outline-text-5" id="text-orgacffe57">
<p>
La funciÃ³n definida por}
&varphi; ( x) =}
1
\sqrt{}
2 &pi; 
e
âx
2
/{2}
(1)
se llama la funciÃ³n densidad normal{; su integral
&Phi;(x) =
1
\sqrt{}
2 &pi; 
Z
x
â&infin;
e
ât
2
/{2}
dt (2)
es la funciÃ³n distribuciÃ³n normal.
Folclore. Se sabe que la funciÃ³n e}
âx
2
no admite una primitiva que pueda expresarse medi
ante un nÃºmero finito de funciones elementales: x
&nu;
, sen(x), cos(x), a
x
, etc&#x2026;. (Ver Piskunov,
N., (1983). cÃ¡lculo diferencial e integral, tomo I, Mir, MoscÃº). Sin embargo, usando tÃ©cnicas
de cambio de variables bidimensionales se puede demostrar que
R
&infin;
â&infin;
&varphi; ( x ) dx = 1.
La funciÃ³n &Phi;(x) crece desde 0 hasta 1. Su grÃ¡fico es una curva con forma de S con
&Phi;(âx) = 1 â &Phi;(x). (3)
2
â4 â3 â2 â1 0 1 2 3 4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
(a)
â4 â3 â2 â1 0 1 2 3 4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(b)
Figura 1: (a) La funciÃ³n densidad normal &varphi;(x) :=
1
\sqrt{}
2 &pi; 
e
âx
2
/{2}
; (b) La funciÃ³n distribuciÃ³n
normal &Phi;(x) =
1
\sqrt{}
2 &pi; 
R
x
â&infin;
e
ât
2
/{2}
dt
Tablas. La tabla de valores de la funciÃ³n distribuciÃ³n normal se puede consultar en la}
mayorÃ­a de los libros sobre probabilidad y/o estadÃ­stica. En general se tabulan los valores
de &Phi;(x) para x = d
0
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
d
1
10
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
d
2
100
, donde d
0
&isin; \{0, 1, 2, 3\} y d}
1
, d
2
&isin; \{0, 1, 2, &hellip; , 9}\. Las filas}
de la tabla estÃ¡n indexadas por los nÃºmeros d
0
. d
1
y sus columnas por los nÃºmeros 0.0d
2
</p>
<pre class="example">


</pre>
<p>
en l a posiciÃ³n (d
0
. d
1
, 0.0}d
2
) de la tabla se encuentra el valor &Phi;(d
0
. d
1
d
2
). Por ejemplo, si
se consulta la tabla del libro de Feller, W. (1968). An Introduction to Probability Theory}
and it s Applications, en fila 1.2 y columna de 0.08 puede leerse 0.8997, lo que significa que}
&Phi;(1.28) = 0.8997.
En el Cuadro 1.1 reproducimos algunos de los valores de la tabla del Feller:
</p>
</div>
</div>
<div id="outline-container-orga31c574" class="outline-5">
<h5 id="orga31c574">Lema 1.2. Para cada x &gt; 0 valen las siguientes desigualdades:}</h5>
<div class="outline-text-5" id="text-orga31c574">
<p>
&varphi; ( x ) 
î
1
x
â
1
x
3
î
&lt; 1 â &Phi;(x ) &lt; &varphi; ( x ) 
î
1
x
î
. (4)
3
x 1.28 1.64 1.96 2.33 2.58 3.09 3.29}
&Phi;(x) 0.8997 0.9495 0.975 0.9901 0.9951 0.9990 0.9995
Cuadro 1: En la tabla se muestran algunos valores de &Phi;(x) :=
1
\sqrt{}
2 &pi; 
R
x
â&infin;
e
ât
2
/{2}
dt.
</p>
</div>
</div>
<div id="outline-container-orgf45e3c8" class="outline-5">
<h5 id="orgf45e3c8">DemostraciÃ³n</h5>
<div class="outline-text-5" id="text-orgf45e3c8">
<p>
Usando que}
d
dx
&varphi; ( x) = â} x&varphi; ( x) es fÃ¡cil ver que las derivadas de los miembros}
de las desigualdades (4) satisfacen:
d
dx
î
&varphi; ( x ) 
î
1
x
â
1
x
3
îî
= â{&varphi;}(x)
î
1 â}
3
x
4
î
.
d
dx
[1 â &Phi;(x)] = â{&varphi;}(x).
d
dx
î
&varphi; ( x ) 
î
1
x
îî
= â{&varphi;}(x)
î
1 +
1
x
2
î
.
Por lo tanto,
d
dx
î
â{&varphi; ( x ) 
î
1
x
â
1
x
3
îî
&lt;
d
dx
[&Phi;(x) â 1] &lt;}
d
dx
î
â{&varphi; ( x ) 
î
1
x
îî
(5)
Las desigualdades (4) se obtienen integrando desde x hasta &infin;}.
</p>
</div>
</div>
<div id="outline-container-orgef60b61" class="outline-5">
<h5 id="orgef60b61">Nota Bene</h5>
<div class="outline-text-5" id="text-orgef60b61">
<p>
De las desigualdades (4) se infiere un mÃ©todo de cÃ¡lculo para aproximar los}
valores de 1 â &Phi;(x): promediando los valores de los extremos de las desigualdades se obtiene
una aproximaciÃ³n cuyo error absoluto es menor que la semi-diferencia entre ambos:
î
î
î
î
1 â &Phi;(x) â &varphi;}(x)
î
1
x
â
1
2x
3
î
î
î
î
î
&le;
&varphi; ( x ) 
2x
3
. (6)
De la desigualdad (6) se puede ver que la aproximaciÃ³n
&Phi;(x) &asymp; 1 â &varphi;}(x)
î
1
x
â
1
2x
3
î
(7)
es prÃ¡cticamente inÃºtil para valores /"pequeÃ±os"/de x (i.e., x &isin; (0, 1]) pero va mejorando a
medida que los valores de x /"crecen''. Usando la aproximaciÃ³n dada en (7) se obtienen las
siguientes aproximaciones
x
1.28 1.64 1.96 2.33 2.58 3.09 3.29
&Phi;(x) 0.90454 0.94839 0.97406 0.98970 0.99487 0.99896 0.99948
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{error}</td>
<td class="org-left">&le;</td>
</tr>
</tbody>
</table>
<p>
0.04192 0.01178 0.00388 0.00104 0.00041 0.00005 0.00002
Cuadro 2: Algunos valores de &Phi;(x) obtenidos mediante la estimaciÃ³n (7).
Nota histÃ³rica La distribuciÃ³n normal fue descubierta por De Moivre en 1733 como re
sultado de analizar la forma lÃ­mite de la distribuciÃ³n binomial simÃ©trica y redescubierta
nuevamente por Gauss (1809) y Laplace (1812) quienes la estudiaron en relaciÃ³n con sus tra
bajos sobre la teorÃ­a de los errores de observaciÃ³n. Laplace dio, ademÃ¡s, el primer enunciado
(incompleto) del teorema central del lÃ­mite. (Ver Cramer, H., (1970). MÃ©todos matemÃ¡ticos}
de estadÃ­stica , Aguilar, Madrid.)
4
</p>
</div>
</div>
</div>
<div id="outline-container-org215b8da" class="outline-3">
<h3 id="org215b8da">Cuentas con normales</h3>
<div class="outline-text-3" id="text-org215b8da">
<p>
Sean &mu; &isin; \Re y &sigma; &gt; 0 arbitrarios, pero fijos. Se dice que la variable aleatoria X tiene
distribuciÃ³n normal de parÃ¡metros &mu; y &sigma;}
2
y se denota X &sim; N}(&mu;, &sigma;
2
) si la funciÃ³n densidad
de X es de la forma
&varphi;
&mu;,&sigma;
2
(x) =
1
&sigma;
\sqrt{}
2 &pi; 
exp
î
â
(x â &mu;)
2
2 &sigma; 
2
î
. (8)
</p>
</div>
<div id="outline-container-org39becce" class="outline-5">
<h5 id="org39becce">Nota Bene</h5>
<div class="outline-text-5" id="text-org39becce">
<p>
Un hecho importante sobre las variables aleatorias normales es que si X tiene}
distribuciÃ³n normal N(&mu;, &sigma;
2
), entonces
Z =}
X â &mu;
&sigma;
(9)
tiene distribuciÃ³n normal N(0, 1). En efecto,
\mathbb{P}(Z &le; z) = \mathbb{P}((X â &mu;)/&sigma; &le; z) = \mathbb{P}(X &le; z&sigma; + &mu;)
=
1
&sigma;
\sqrt{}
2 &pi; 
Z
z&sigma;{+}&mu;
â&infin;
exp
î
â
(x â &mu;)
2
2 &sigma; 
2
î
dx
=
1
\sqrt{}
2 &pi; 
Z
z
â&infin;
e
â
1
2
t
2
dt por sustituciÃ³n x = t&sigma; + &mu;.}
Este hecho significa que si trasladamos el origen de las abscisas en &mu; y cambiamos la escala
de manera tal que &sigma; represente la unidad de medida, la distribuciÃ³n normal N(&mu;, &sigma;
2
) se
transforma en la distribuciÃ³n normal N(0, 1). Su importancia prÃ¡ctica radica en que permite
reducir el cÃ¡lculo de probabilidades de las distribuciones normales N(&mu;, &sigma;
2
) al de la distribu
ciÃ³n normal N(0, 1). Motivo por el cual esta Ãºltima recibe el nombre de normal estÃ¡ ndar (o
tÃ­pica). MÃ¡s precisamente, si X tiene distribuciÃ³n normal N(&mu;, &sigma;
2
), su funciÃ³n de distribu
ciÃ³n podrÃ¡ reducirse a la funciÃ³n de distribuciÃ³n normal &Phi;(Â·) definida en (2) de la siguiente
manera:
\mathbb{P}(X &le; x) = P
î
X â &mu;
&sigma;
&le;
x â &mu;
&sigma;
î
= P
î
Z &le;}
x â &mu;
&sigma;
î
= &Phi;
î
x â &mu;
&sigma;
î
. (10)
La identidad (10) resume toda la informaciÃ³n probabilÃ­sticamente relevante sobre la variable
aleatoria X &sim; N}(&mu;, &sigma;
2
) y permite calcular (con ayuda de la tabla de la funciÃ³n de distribuciÃ³n
normal &Phi;(Â·)) la probabilidad de que la variable X se encuentre en cualquier intervalo prefijado
de antemano:
\mathbb{P}(a &lt; X &lt; b) = &Phi;}
î
b â &mu;
&sigma;
î
â &Phi;}
î
a â &mu;
&sigma;
î
. (11)
En particular, cuando el intervalo (a, b) es simÃ©trico con respecto a &mu;, l as cantidades a y b se
pueden expresar en la forma a = &mu; â &epsilon;, b = &mu; + &epsilon;, donde &epsilon; &gt; 0, y la fÃ³rmula (11) adopta la
forma
\mathbb{P}(|X â &mu;}| &lt; &epsilon;}) = &Phi;}
î
&epsilon;
&sigma;
î
â &Phi;}
î
â
&epsilon;
&sigma;
î
= 2&Phi;
î
&epsilon;
&sigma;
î
â 1. (12)
5
Significado de los parÃ¡metros &mu; y &sigma;}
2
. La relaciÃ³n (9) dice que si X es una variable}
aleatoria con distribuciÃ³n normal de parÃ¡metros &mu; y &sigma;}
2
, entonces X = \sigmaZ + &mu; donde Z es
una variable con distribuciÃ³n normal estÃ¡ndar. CÃ¡lculos de rutina muestran que E[Z] = 0
y V(Z) = 1, lo que permite deducir que la media y la varianza de la N(&mu;, &sigma; }
2
) son &mu; y &sigma;
2
,
respectivamente.
</p>
</div>
</div>
</div>
<div id="outline-container-org508d6a8" class="outline-3">
<h3 id="org508d6a8">Ejemplos</h3>
<div class="outline-text-3" id="text-org508d6a8">
</div>
<div id="outline-container-org34edfc9" class="outline-5">
<h5 id="org34edfc9">Ejemplo 1.3.</h5>
<div class="outline-text-5" id="text-org34edfc9">
<p>
Una maquina produce ejes cuyos diÃ¡metros X tienen distribuciÃ³n normal de}
media &mu; = 10 mm y varianza &sigma;}
2
= 0.25 mm. Un eje se considera defectuoso si X &lt; 9.5 mm.
CuÃ¡l es la probabilidad de que un eje elegido al azar resulte defectuoso?
SoluciÃ³n: El problema se resuelve calculando \mathbb{P}(X &lt; 9.5). Poniendo &mu; = 10 y &sigma; = 0.5 en}
la fÃ³rmula (10) obtenemos \mathbb{P}(X &lt; 9.5) = &Phi;
î
9.5{â}10
0.5
î
= &Phi; (â}1) = 0.1587.
Curva peligrosa. De inmediato podrÃ­a surgir una objeciÃ³n al uso de la distribuciÃ³n nor
mal N(10, 0.25) para modelar el diÃ¡metro de los ejes. Al fin y al cabo, los diÃ¡metros deben
ser positivos y la distribuciÃ³n normal adopta valores positivos y negativos. Sin embargo, el
modelo anterior asigna una probabilidad despreciable al evento X &lt; 0. En efecto,\mathbb{P}(X &lt; 0) =
P
î
X{â{10
0.5
&lt;
0{â}10
0.5
î
= \mathbb{P}(Z &lt; â} 20) = &Phi; (â}20) = 1 â &Phi;(20). De acuerdo con la estimaciÃ³n (6)
tenemos que 1 â}&Phi;(20) &asymp; &varphi;}(20)
î
1
20
â
1
2{Â·}20
3
î
= O(10
â{89}
). Este tipo de situaciÃ³n es habitual en
la prÃ¡ctica. Se tiene una variable aleatoria X de la que se sabe que no puede tomar valores
negativos (p.ej. una distancia, una longitud, un Ã¡rea, un peso, una temperatura, un precio,
etc.) y se la modela utilizando una distribuciÃ³n normal N(&mu;, &sigma;
2
); motivados, por ejemplo,
por cuestiones de simetrÃ­a. En principio, el modelo podrÃ¡ ser perfectamente vÃ¡lido sie mpre
y cuando los valores de los parÃ¡metros &mu; y &sigma;}
2
sean tales que la probabilidad \mathbb{P}(X &lt; 0) sea
prÃ¡cticamente 0.
</p>
</div>
</div>
<div id="outline-container-org0e6c74b" class="outline-5">
<h5 id="org0e6c74b">Nota Bene sobre grandes desvÃ­os. Sea X una variable aleatoria con distribuciÃ³n normal}</h5>
<div class="outline-text-5" id="text-org0e6c74b">
<p>
de media &mu; y varianza &sigma;}
2
. Sea t &gt; 0, utilizando la fÃ³rmula (12) podemos ver que
p
t
:= \mathbb{P}(|X â &mu;}| &gt; t&sigma;}) = 1 â \mathbb{P}(|X â &mu;}| &le; t&sigma;}) = 1 â}
î
2&Phi;
î
t&sigma;
&sigma;
î
â 1}
î
= 2 (1 â &Phi; (t)) .
Usando la tabla de la distribuciÃ³n normal &Phi;(Â·) se puede ver que p
1
= 0.3174, p
2
= 0.0454,
p
3
= 0.0028. Estos probabilidades admiten la siguiente interpretaciÃ³n: cerca del 32 % de los
valores de una variable X &sim; N}(&mu;, &sigma;
2
) se desvÃ­an de su media en mÃ¡s de &sigma;}; solamente cerca
de un 5 % lo hacen en mÃ¡s de 2{&sigma; y solamente cerca de un 3 % en mÃ¡s de 3 &sigma; . Esto da lugar
a que en la mayor parte de los problemas de la prÃ¡ctica se consideren casi imposibles las
desviaciones respecto de la media &mu; que superen 3{&sigma; y se consideren limitados por el intervalo
[&mu; â 3{&sigma;, &mu; + 3 &sigma; ] todos los valores prÃ¡cticamente p osibles de la variable X.
</p>
</div>
</div>
<div id="outline-container-orgfd1e8ef" class="outline-5">
<h5 id="orgfd1e8ef">Ejemplo 1.4.</h5>
<div class="outline-text-5" id="text-orgfd1e8ef">
<p>
Sea X una variable aleatoria con distribuciÃ³n normal de media &mu; = 3 y}
varianza &sigma;}
2
= 4. Â¿CuÃ¡l es la probabilidad de que X sea no menor que 1 y no mayor que 7?
SoluciÃ³n: Poner &mu; = 3 y &sigma; = 2 en la fÃ³rmula (11) y usar la tabla de la distribuciÃ³n normal}
&Phi;(Â·): \mathbb{P}(1 &le; X &le; 7) = &Phi;
î
7{â}3
2
î
â &Phi;}
î
1{â}3
2
î
= &Phi;(2) â &Phi;(â}1) = 0.9773 â 0.1587 = 0.8186.
6
</p>
</div>
</div>
</div>
<div id="outline-container-org5f4415d" class="outline-3">
<h3 id="org5f4415d">Suma de normales independientes</h3>
<div class="outline-text-3" id="text-org5f4415d">
</div>
<div id="outline-container-orgf3fd656" class="outline-5">
<h5 id="orgf3fd656">Lema 1.5. Sean X}</h5>
<div class="outline-text-5" id="text-orgf3fd656">
<p>
1
y X<sub>2</sub>
dos variables aleatorias independientes con distribuciÃ³n nor
mal N(&mu; 
1
, &sigma;
2
1
) y N}( &mu; 
2
, &sigma;
2
2
), respectivamente. Entonces X}
1
</p>
<ul class="org-ul">
<li>X<sub>2</sub></li>
</ul>
<p>
tiene distribuciÃ³n normal
N
î
&mu;
1
</p>
<ul class="org-ul">
<li>&mu;}</li>
</ul>
<p>
2
, &sigma;
2
1
</p>
<ul class="org-ul">
<li>&sigma;}</li>
</ul>
<p>
2
2
î
.
</p>
</div>
</div>
<div id="outline-container-orgc702f87" class="outline-5">
<h5 id="orgc702f87">DemostraciÃ³n</h5>
<div class="outline-text-5" id="text-orgc702f87">
<p>
Observando que X}
1
</p>
<ul class="org-ul">
<li>X<sub>2</sub></li>
</ul>
<p>
= (X<sub>1</sub>
â &mu; 
1
) + (X<sub>2</sub>
â &mu; 
2
) + &mu;}
1
</p>
<ul class="org-ul">
<li>&mu;}</li>
</ul>
<p>
2
el problema se
reduce a considerar el caso &mu;}
1
= &mu;}
2
= 0. La prueba se obtiene mostrando que la convoluciÃ³n de
las densidades f
1
(x
1
) =
1
\sqrt{}
2{&pi;&sigma;}
1
exp
î
âx
2
1
/{2}&sigma;
2
1
î
y f
2
(x
2
) =
1
\sqrt{}
2{&pi;&sigma;}
2
exp
î
âx
2
2
/{2}&sigma;
2
2
î
es la densidad
normal de media &mu;}
1
</p>
<ul class="org-ul">
<li>&mu;}</li>
</ul>
<p>
2
y varianza &sigma;}
2
= &sigma;}
2
1
</p>
<ul class="org-ul">
<li>&sigma;}</li>
</ul>
<p>
2
2
. Por definiciÃ³n
(f
1
â f
2
)(x) =
Z
&infin;
â&infin;
f
1
(x â y)f
2
(y) =
1
2{&pi;&sigma;}
1
&sigma;
2
Z
&infin;
â&infin;
exp
î
â
(x â y)
2
2 &sigma; 
2
1
â
y
2
2 &sigma; 
2
2
î
dy (13)
El resultado se obtendrÃ¡ mediante un poco de Ã¡lgebra, bastante paciencia, y un cambio de
variables en la integral del lado derecho de la identidad (13).
exp
î
â
(x â y)
2
2 &sigma; 
2
1
â
y
2
2 &sigma; 
2
2
î
= exp
â
1
2
î
&sigma;
&sigma;
1
&sigma;
2
y â}
&sigma;
2
&sigma;&sigma;
1
x
î
2
â
x
2
2 &sigma; 
2
!
= exp
â
1
2
î
&sigma;
&sigma;
1
&sigma;
2
y â}
&sigma;
2
&sigma;&sigma;
1
x
î
2
!
exp
î
â
x
2
2 &sigma; 
2
î
La primera igualdad se obtuvo completando cuadrados respecto de y en la expresiÃ³n â}
(x{â}y)
2
2 &sigma; 
2
1
â
y
2
2 &sigma; 
2
2
y reagrupando algunos tÃ©rminos. Mediante el cambio de variables z =
&sigma;
&sigma;
1
&sigma;
2
y â}
&sigma;
2
&sigma;&sigma;
1
x, cuya}
diferencial es de la forma dz =
&sigma;
&sigma;
1
&sigma;
2
dy, se puede ver que}
(f
1
â f
2
)(x) =
1
2{&pi;&sigma;}
exp
î
â
x
2
2 &sigma; 
2
î
Z
&infin;
â&infin;
exp
î
â
z
2
2
î
dz =}
1
\sqrt{}
2{&pi; &sigma;}
exp
î
â
x
2
2 &sigma; 
2
î
.
Este resultado se puede generalizar para una suma de n variables aleatorias independientes:
Sean X<sub>1</sub>
, X<sub>2</sub>
, &hellip; , X
n
variables aleatorias independientes con distribuciones normales: X
i
&sim;
N( &mu; }
i
, &sigma;
2
i
), 1 &le; i &le; n. Entonces, 
n
X
{i=1}
X
i
&sim; N
n
X
{i=1}
&mu;
i
,
n
X
{i=1}
&sigma;
2
i
!
.
La prueba se obtiene por inducciÃ³n y utilizando la siguiente propiedad /"hereditaria"/de
familias de variables aleatorias independientes (cuya prueba puede verse en el CapÃ­tulo 1
del libro de Durrett, R.(1996): Probability Theory and Examples): Si X}
1
, X<sub>2</sub>
, &hellip; , X
n
son
variables aleatorias independientes, entonces funciones (medibles) de familias disjunta s de las
X
i
tambiÃ©n son independientes.
</p>
</div>
</div>
<div id="outline-container-orgc0fc9ec" class="outline-5">
<h5 id="orgc0fc9ec">Nota Bene</h5>
<div class="outline-text-5" id="text-orgc0fc9ec">
<p>
Observando que para cada a &isin; \Re y X &sim; N}(&mu;, &sigma;
2
) resulta que aX &sim; N}(a&mu;, a}
2
&sigma;
2
)
se obtiene el siguiente resultado:
7
</p>
</div>
</div>
<div id="outline-container-org046e23f" class="outline-5">
<h5 id="org046e23f">Teorema 1.6. Sean X}</h5>
<div class="outline-text-5" id="text-org046e23f">
<p>
1
, X<sub>2</sub>
, &hellip; , X
n
variables aleato rias independientes con distribuciones
normales: X
i
&sim; N( &mu; }
i
, &sigma;
2
i
), 1 &le; i &le; n y sean a 
1
, a
2
, &hellip; , a
n
nÃºmeros reales cualesquiera.
Entonces,
n
X
{i=1}
a
i
X
i
&sim; N
n
X
{i=1}
a
i
&mu;
i
,
n
X
{i=1}
a
2
1
&sigma;
2
i
!
.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org5275de1" class="outline-2">
<h2 id="org5275de1">GÃ©nesis de la distribuciÃ³n normal</h2>
<div class="outline-text-2" id="text-org5275de1">
</div>
<div id="outline-container-org3c87427" class="outline-3">
<h3 id="org3c87427">Teorema lÃ­mite de De Moivre - Laplace</h3>
<div class="outline-text-3" id="text-org3c87427">
<p>
En 1733, De Moivre observÃ³ que la distribuciÃ³n binomial correspondiente a la cantidad
de Ã© xitos, S}
n
, en n ensayos de Bernoulli simÃ©tricos tiene la forma lÃ­mite de una campana.
Esta observaciÃ³n fue la clave que le permitiÃ³ descubrir la famosa campana de Gauss y allanar
el camino que lo condujo a establecer la primera versiÃ³n del Teorema Ce ntral del LÃ­mite{: la
convergencia de la distribuciÃ³n Binomial(n, 1 / 2) a la distribuciÃ³n normal estÃ¡ndar. En 1801,
Laplace refinÃ³ y generalizÃ³ este resultado al caso de la distribuciÃ³n Binomial(n, p). El Teorema
de De Moivre-Laplace, que enunciamos mÃ¡s abajo, mejora sustancialmente la Ley dÃ©bil de los
grandes nÃºmeros porque proporciona una estimaciÃ³n mucho mÃ¡s precisa de las probabilidades
P
î
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<tbody>
<tr>
</tr>
</tbody>
</table>
<p>
S
n
n
â p| &le; &epsilon;}
î
.
0 2 4 6 8 10 12 14 16
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
Figura 2: RelaciÃ³n entre la distribuciÃ³n Binomial simÃ©trica y la distribuciÃ³n normal. La prob
abilidad de que ocurran k Ã©xitos en n ensayos de Bernoulli estÃ¡ representada por un segmento
paralelo al eje de las abscisas localizado en la ordenada k de al tura igual a \mathbb{P}(S}
n
= k). La curva
continua /"aproxima"/los valores de \mathbb{P}(S}
n
= k). Observar que dichas probabilidades tambiÃ©n
se pueden representar como Ã¡reas de re ctÃ¡ngulos de altura \mathbb{P}(S}
n
= k) y de base unitaria
centrada en k.
8
</p>
</div>
<div id="outline-container-org39cb05d" class="outline-5">
<h5 id="org39cb05d">Teorema 2.1 (Teorema lÃ­mite de De Moivre-Laplace). Consideramos una sucesiÃ³n de en-</h5>
<div class="outline-text-5" id="text-org39cb05d">
<p>
sayos de Bernoulli independi entes . Sean p la probabilidad de Ã©xito en cada ensayo y S}
n
la
cantida d de Ã© xitos observados en los primeros n ensayos. Para cualqu ier x &isin; \Re vale que
lim
n{&rarr;&infin;}
P
S
n
â np}
p
np(1 â p ) 
&le; x
!
= &Phi;(x), (14)
donde &Phi;(x) :=}
R
x
â&infin;
1
\sqrt{}
2 &pi; 
e
ât
2
/{2}
dt es la funciÃ³n distribuciÃ³n normal estÃ¡ndar.
</p>
</div>
</div>
<div id="outline-container-orge33141b" class="outline-5">
<h5 id="orge33141b">DemostraciÃ³n</h5>
<div class="outline-text-5" id="text-orge33141b">
<p>
Ver CapÃ­tulo VII de Feller, W., (1971). An Introduction to Probability
Theory a nd Its Applications, Vol. I, John Wiley &amp; Sons, New York.
Â¿QuÃ© significa el Teorema LÃ­mite de De Moivre-Laplace? Para c ontestar esta pre
gunta vamos a reconstruir las ideas principales de su gÃ©nesis. En otras palabras, vamos a
(re)construir el Teorema. La clave de la construcciÃ³n estÃ¡ /"embutida"/en la Figura 2. La im
agen permite /"capturar de inmediato"/la existencia de una forma lÃ­ mite para la distribuciÃ³n
Binomial en el caso simÃ©trico p = 1 / 2.
Paso 1. El primer paso en la direcciÃ³n del Teorema de De Moivre consiste en darse cuenta}
que la Figura 2 seÃ±ala la existencia de una forma lÃ­mite. En una primera fase (completa
mente abstracta) podemos conjeturar que /"{la distribuc iÃ³n binomial simÃ©trica tiene una forma}
asintÃ³tica . En otras palabras, cuando la cantidad de ensayos de Bernoulli es suficientemente
grande, salvo traslaciones y cambios de escala apropiados, la distribuciÃ³n Binomial se parece
a una funciÃ³n continua par, &varphi; ( x ) , cuyo grÃ¡fico tiene la forma de una campana.{''}
Paso 2. El segundo paso consiste en precisar la naturaleza de la traslaciÃ³n y los cambios de}
escala que permiten /"capturar"/esa forma lÃ­mite. Si se reï¬exiona sobre el significado de la
media y la varianza de una variable aleatoria, parece claro que la forma lÃ­mite se obtendrÃ¡ cen
trando la variable S}
n
en su valor medio, E[S}
n
] =
1
2
n, y adoptando como unidad de medida}
la desviaciÃ³n tÃ­pica de los valores observados respecto de dicho valor, &sigma;(S}
n
) =
1
2
\sqrt{}
n. El sig
nificado geomÃ©trico de esta transformaciÃ³n consiste en (1) trasladar el origen de las abscisas
en
1
2
n y (2) dividirlas por}
1
2
\sqrt{}
n. Para que las Ã¡reas de los rectÃ¡ngulos sigan representando}
probabilidades, las ordenadas deben multiplicarse p or el mismo nÃºmero. Este paso permite
enunciar la siguiente versiÃ³n mejorada de la conjetura inicial: /"{existe una funciÃ³n continua}
&varphi; ( x) tal que
\mathbb{P}(S
n
= k) =
î
n
k
îî
1
2
î
n
&sim;
1
1
2
\sqrt{}
n
&varphi;
k â}
1
2
n
1
2
\sqrt{}
n
!
, (15)
siempre y cuando n sea su ficienteme nte grande.{''}
Paso 3. Establecida la conjetura el problema consiste en /"descubrir"/la expresiÃ³n de la funciÃ³n}
&varphi; ( x) y en precisar cuÃ¡l es el sentido de la relaciÃ³n aproximada que aparece en (15). En este}
punto no queda otra que /"arremangarse y meter la mano en el barro''. Como resultado se
obtiene que la expresiÃ³n de la funciÃ³n &varphi;(x) es
&varphi; ( x) =}
1
\sqrt{}
2 &pi; 
exp
î
â
x
2
2
î
y que la relaciÃ³n &sim; vale para valores de k del orden de
\sqrt{}
n y significa que el cociente de los}
dos lados tiende a 1 cuando n &rarr; &infin;} .
9
\hypertarget{pfa}
</p>
</div>
</div>
<div id="outline-container-org5b64e55" class="outline-5">
<h5 id="org5b64e55">Nota Bene</h5>
<div class="outline-text-5" id="text-org5b64e55">
<p>
La relaciÃ³n (15) expresa matemÃ¡ticamente un hecho que se observa claramente}
en la Figura 2: la campana /"pasa"/por los puntos de base k y altura \mathbb{P}(S}
n
= k). Conviene
observar que la expresiÃ³n que aparece en el lado derecho de la relaciÃ³n (15) es la funciÃ³n
de densidad de la normal N}
î
1
2
n,
1
4
n
î
evaluada en x = k. En la prÃ¡ctica, esto significa que
para obtener una buena aproximaciÃ³n de la probabilidad de observar k Ã©xitos en n ensayos de
Bernoulli independientes, basta con evaluar la densidad de la normal N}
î
1
2
n,
1
4
n
î
en x = k.
Sin temor a equivocarnos, podemos resumir estas observaciones mediante una expresiÃ³n de
la forma S}
n
&sim; N ( E[S
n
], V}(S}
n
) ).
Paso 4. Observar que para cada x}
1
&lt; x
2
vale que
P
x
1
&le;
S
n
â
1
2
n
1
2
\sqrt{}
n
&le; x
2
!
= P
î
1
2
n + x
1
1
2
\sqrt{}
n &le; S
n
&le;
1
2
n + x
2
1
2
\sqrt{}
n
î
=
X
x
1
1
2
\sqrt{}
n{&le;}j{&le;}x
2
1
2
\sqrt{}
n
P
î
S
n
=
1
2
n + j
î
&asymp;
X
x
1
&le;{jh}\leqx
2
h&varphi;  ( jh ) , (16)
donde h =
2
\sqrt{}
n
y la suma se realiza sobre todos los enteros j tales que x
1
&le; jh &le; x
2
. Cada
uno de los sumandos que aparecen en el lado derecho de la aproximaciÃ³n (16) es el Ã¡rea de
un rectÃ¡ngulo de base [kh, (k + 1)h y altura &varphi;(kh). Como la funciÃ³n &varphi;(Â·) es continua, para
valores pequeÃ±os de h la suma total de las Ã¡reas de los rectÃ¡ngulo debe estar prÃ³xima del Ã¡rea
bajo la curva de la densidad normal entre x
1
y x
2
. Por lo tanto, debe valer lo siguiente
lim
n{&rarr;&infin;}
P
x
1
&le;
S
n
â
1
2
n
1
2
\sqrt{}
n
&le; x
2
!
=
Z
x
2
x
1
&varphi; ( t ) dt = &Phi;(x
2
) â &Phi;(x
1
). (17)
Este paso puede hacerse formalmente preciso /"arremangandose y metiendo la mano en &#x2026;''
</p>
</div>
</div>
<div id="outline-container-orge1815ba" class="outline-5">
<h5 id="orge1815ba">Nota Bene</h5>
<div class="outline-text-5" id="text-orge1815ba">
<p>
La variable aleatoria que aparece dentro de la probabilidad del l ado izquierdo}
de (17)
S
â
n
=
S
n
â
1
2
n
1
2
\sqrt{}
n
=
S
n
â E[S 
n
]
&sigma; ( S
n
)
(18)
es una medida de la desviaciÃ³n de S}
n
respecto de la media E[S}
n
] en unidades de la desviaciÃ³n
tÃ­pica &sigma;(S}
n
). El teorema lÃ­mite de De Moivre-Laplace significa que cuando se considera una
cantidad n (suficientemente grande) de ensayos de Bernoulli independientes, la distribuciÃ³n de
la variable aleatoria S}
â
n
es /"prÃ¡cticamente indistinguible"/de la distribuciÃ³n normal estÃ¡ndar
N(0, 1).
Comentario sobre prueba del Teorema 2.1. Si se sigue con cuidado la demostraciÃ³n}
presentada por Feller se puede ver que las herramientas principales de la prueba son el desar
rollo de Taylor (1712) de la funciÃ³n log(1 + t) = t + O(t
2
) y la fÃ³rmula asintÃ³tica de Stirling
(1730) para los nÃºmeros factoriales n! &sim;}
\sqrt{}
2{\pin n}
n
e
ân
. Partiendo de la funciÃ³n de probabilidad
de la Binomial(n, 1 / 2) se /"deduce"/la expresiÃ³n de la funciÃ³n densidad normal (
\sqrt{}
2 &pi; )
â{1}
e
âx
2
/{2}
</p>
<pre class="example">


</pre>
<p>
el factor (
\sqrt{}
2 &pi; )
â{1}
proviene de la fÃ³rmula de Stirling y el factor e
âx
2
/{2}
del desarrollo de Tay
lor. Dejando de lado los recursos tÃ©cnicos utilizados en la prueba, se observa que las ideas
involucradas son simples y /"recorren el camino del descubrimiento"/de De Moivre (1733).
10
\hypertarget{pfb}
</p>
</div>
</div>
<div id="outline-container-org1237073" class="outline-5">
<h5 id="org1237073">Ejemplo 2.2.</h5>
<div class="outline-text-5" id="text-org1237073">
<p>
Se lanza 40 veces una moneda honesta. Hallar la probabilidad de que se}
obtengan exactamente 20 caras. Usar l a aproximaciÃ³n normal y compararla con la soluciÃ³n
exacta.
SoluciÃ³n: La cantidad de caras en 40 lanzamientos de una moneda honesta, S
40
, es una
variable Binomial de parÃ¡metros n = 40 y p = 1 / 2. La aproximaciÃ³n normal (15) establece
que
\mathbb{P}(S
40
= 20) &sim;}
1
1
2
\sqrt{}
40
&varphi;(0) =}
1
\sqrt{}
20 &pi; 
= 0.12615&hellip;
El resultado exacto es
\mathbb{P}(X = 20) =}
î
40
20
îî
1
2
î
40
= 0.12537&hellip;
</p>
</div>
</div>
<div id="outline-container-org62779a1" class="outline-5">
<h5 id="org62779a1">Ejemplo 2.3.</h5>
<div class="outline-text-5" id="text-org62779a1">
<p>
Se dice que los reciÃ©n nacidos de madres fumadoras tienden a ser mÃ¡s pequeÃ±os}
y propensos a una variedad de dolencias. Se conjetura que ademÃ¡s parecen deformes. A un
grupo de enfermeras se les mostrÃ³ una selecciÃ³n de fotografÃ­as de bebÃ©s, la mitad de los
cuales nacieron de madres fumadoras; las enfermeras fueron invitadas a juzgar a partir de la
apariencia de cada uno si la madre era fumadora o no. En 1500 ensayos se obtuvieron 910
respuestas correctas. La conjetura es plausible?
SoluciÃ³n: Aunque superficial, un argumento atendible consiste en afirmar que, si todos los}
bebÃ©s parecen iguales, la cantidad de repuestas correctas S}
n
en n ensayos es una variable
aleatoria con distribuciÃ³n Binomial (n, 1 / 2). Entonces, para n grande
P
S
n
â
1
2
n
1
2
\sqrt{}
n
&gt; 3}
!
= 1 â P}
S
n
â
1
2
n
1
2
\sqrt{}
n
&le; 3}
!
&asymp; 1 â &Phi;(3) &asymp;
1
1000
por el Teorema lÃ­mite de De Moivre-Laplace. Para los valores dados de S}
n
,
S
n
â
1
2
n
1
2
\sqrt{}
n
=
910 â 750
5
\sqrt{}
15
&asymp; 8.
Se podrÃ­a decir que el evento \{X â}
1
2
n &gt;
3
2
\sqrt{}
n{\} es tan improbable que su ocurrencia arroja
dudas sobre la suposiciÃ³n origi nal de que los bebÃ©s parecen iguales. Este argumento otorgarÃ­a
cierto grado de credibilidad a la c onjetura enunciada.
Comentarios sobre el caso general
</p>
<ol class="org-ol">
<li>En el caso general, la probabilidad de Ã©xito en cada ensayo de Bernoulli individual es}</li>
</ol>
<p>
p &isin; (0}, 1). Si S
n
es la cantidad de Ã©xitos observados en los primeros n ensayos, entonces
E[S
n
] = np y V(S}
n
) = np(1 â p). Por lo tanto, la variable aleatoria
S
â
n
:=
S
n
â np}
p
np(1 â p ) 
(19)
es una medida de la desviaciÃ³n de S}
n
respecto de la media E[S}
n
] = np en unidades de la
desviaciÃ³n tÃ­pica &sigma;(S}
n
) =
p
np(1 â p). El teorema lÃ­mite de De Moivre-Laplace significa}
11
\hypertarget{pfc}
que cuando se considera una cantidad n (suficientemente grande) de ensayos de Bernoulli
independientes, la distribuciÃ³n de la variable aleatoria S}
â
n
es /"prÃ¡cticamente indistinguible''
de la distribuciÃ³n normal estÃ¡ndar N(0, 1).
</p>
<ol class="org-ol">
<li>TÃ©cnicamente la prueba del teorema se puede hacer recurriendo a las mismas herramientas}</li>
</ol>
<p>
utilizadas en la prueba del caso simÃ©trico, pero los cÃ¡lculos involucrados son mÃ¡s complica
dos. Sin embargo, el resultado tambiÃ©n es claro si se observan las grÃ¡ficas de la distribuciÃ³n
Binomial(n, p). En la Figura 3 se ilustra el caso n = 16 y p = 1 / 4. Nuevamente es /"evidente''
que la forma lÃ­mite de distribuciÃ³n Binomial debe ser la distribuciÃ³n normal.
0 2 4 6 8 10 12 14 16
0
0.05
0.1
0.15
0.2
Figura 3: GrÃ¡fica de la funciÃ³n de probabilidad binomial con n = 16 y p = 1 / 4. Cerca
del tÃ©rmino central m = np = 4, salvo un cambio de escala (cuya unidad de medida es
p
np(1 â p) =}
\sqrt{}
</p>
<ol class="org-ol">
<li>la grÃ¡fica es /"indistinguible"/de la grÃ¡fica de la densidad normal.</li>
<li>De la Figura 3 deberÃ­a estar claro que, para n suficientemente grande, debe valer lo siguiente}</li>
</ol>
<p>
\mathbb{P}(S
n
= k) =
î
n
k
î
p
k
(1 â p)
n{â}k
&sim;
1
p
np(1 â p ) 
&varphi;
k â np
p
np(1 â p ) 
!
. (20)
</p>
</div>
</div>
<div id="outline-container-orgbe981b9" class="outline-5">
<h5 id="orgbe981b9">Ejemplo 2.4.</h5>
<div class="outline-text-5" id="text-orgbe981b9">
<p>
Para el caso ilustrado en la Figura 3: n = 16 y p = 1 / 4, la aproximaciÃ³n (20)
es bastante buena, incluso con un valor de n peque Ã±o. Para k = 0, &hellip; 4 las probabilidades
\mathbb{P}(S
n
= 4+k) son 0.2252, 0.1802, 0.1101, 0.0524, 0.0197. Las aproximaciones correspondientes
son 0.2303, 0.1950, 0.1183, 0.0514, 0.0160.
</p>
</div>
</div>
<div id="outline-container-org75ade87" class="outline-5">
<h5 id="org75ade87">Nota Bene</h5>
<div class="outline-text-5" id="text-org75ade87">
<p>
El Teorema lÃ­mite de De Moivre-Laplace justifica el uso de los mÃ©todos de la}
curva normal para aproximar probabilidades relacionadas con ensayos de Bernoulli con prob
abilidad de Ã©xito p. La experiencia <i>"indica"</i> que la aproximaciÃ³n es bastante buena siempre
que np &gt; 5 cuando p &le; 1 / 2, y n(1 â p) cuando p &gt; 1 / 2. Un valor muy pequeÃ±o de p junto
con un valor de n moderado darÃ¡n lugar a una media pequeÃ±a y con ello se obtendrÃ¡ una
12
\hypertarget{pfd}
distribuciÃ³n asimÃ©trica. La mayor parte de la distribuciÃ³n se acumularÃ¡ alrededor de 0, im
pidiendo con ello que una curva normal se le ajuste bien. Si la media se aparta por lo menos
5 unidades de una y otra extremidad, la distribuciÃ³n tiene suficiente espacio para que resulte
bastante simÃ©trica. (Ver la Figura 4).
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
(a)
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
(b)
0 1 2 3 4 5 6 7 8 9 10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
(c)
0 1 2 3 4 5 6 7 8 9 10
0
0.05
0.1
0.15
0.2
0.25
0.3
(d)
0 1 2 3 4 5 6 7 8 9 10
0
0.05
0.1
0.15
0.2
0.25
(e)
0 1 2 3 4 5 6 7 8 9 10
0
0.05
0.1
0.15
0.2
0.25
(f)
Figura 4: ComparaciÃ³n entre la distribuciÃ³n Binomial(10, p) y su aproximaciÃ³n por la normal
para distintos valores de p (a) p = 0.025; (b) p = 0.05; (c) p = 0.1; (d) p = 0.2; (e) p = 0.4;
(f) p = 0.5.
</p>
</div>
</div>
<div id="outline-container-orgb6a9941" class="outline-5">
<h5 id="orgb6a9941">Ejemplo 2.5 (Encuesta electoral). Queremos estimar la proporciÃ³n del electorado que pre</h5>
<div class="outline-text-5" id="text-orgb6a9941">
<p>
tende votar a un cierto candidato. Para ello consideramos que el voto de cada elector tiene
una distribuciÃ³n Bernoulli de parÃ¡metro p. Concretamente, queremos encontrar un tamaÃ±o
muestral n suficiente para que con una certeza del 99.99 % podamos garantizar un error mÃ¡xi
mo de 0.02 entre el verdadero valor de p y la proporciÃ³n muestral S}
n
/n. En otras palabras,}
queremos encontrar n tal que
P
î
î
î
î
î
S
n
n
â p
î
î
î
î
&le; 0.02}
î
&ge; 0.9999. (21)
Para acotar la incerteza usaremos la aproximaciÃ³n por la normal provista por el teorema lÃ­mite
de De Moivre - Laplace. Para ello, en lugar de observar la variable S}
n
, debemos observar la
variable normalizada S}
â
n
:= (S}
n
â np ) /}
p
np(1 â p). En primer lugar observamos que, como}
consecuencia del teorema lÃ­mite, tenemos la siguiente aproximaciÃ³n
P
î
î
î
î
î
S
n
â np}
p
np(1 â p ) 
î
î
î
î
î
&le; a
!
&asymp; &Phi;(â{a ) â &Phi;(a) = 2&Phi;(a) â 1 (22)
13
\hypertarget{pfe}
o lo que es equivalente
P
î
î
î
î
S
n
n
â p
î
î
î
î
&le;
a
p
p(1 â p ) 
\sqrt{}
n
!
&asymp; 2&Phi;(a) â 1. (23)
Como el verdadero valor de p es desconocido, la fÃ³rmula (23) no puede aplicarse directamente
ya que no se conoce el valor de
p
p(1 â p). Sin embargo, es fÃ¡cil ver que}
p
p(1 â p) &le; 1}/{2 y}
por lo tanto
P
î
î
î
î
î
S
n
n
â p
î
î
î
î
&le;
a
2
\sqrt{}
n
î
&ge; P
î
î
î
î
S
n
n
â p
î
î
î
î
&le;
a
p
p(1 â p ) 
\sqrt{}
n
!
&asymp; 2&Phi;(a) â 1. (24)
Esta Ãºltima relaciÃ³n es la herramienta con la que podemos resolver nuestro problema.
En primer lugar tenemos que resolver la ecuaciÃ³n 2&Phi;(a) â 1 = 0.9999 o la ecuaciÃ³n
equivalente &Phi;(a) =
1.9999
2
= 0.99995. La soluciÃ³n de estÃ¡ ecuaciÃ³n se obtiene consultando una
tabla de la distribuciÃ³n normal: a = 3.9. Reemplazando este valor de a en (24) obtenemos
P
î
î
î
î
î
S
n
n
â p
î
î
î
î
&le;
3.9
2
\sqrt{}
n
î
&ge; 0.9999.
En segundo lugar tenemos que encontrar los valores de n que satisfacen la desigualdad
3.9
2
\sqrt{}
n
&le; 0.02. (25)
Es fÃ¡cil ver que n satisface la desigualdad (25) si y solo si
n &ge;}
î
3.9
0.04
î
2
= (97.5)
2
= 9506.2
El problema estÃ¡ resuelto.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org39cd828" class="outline-2">
<h2 id="org39cd828">Teorema central del lÃ­mite</h2>
<div class="outline-text-2" id="text-org39cd828">
<p>
Los teoremas sobre normalidad asintÃ³tica de sumas de variables aleatorias se llaman Teo
remas Centrales del LÃ­mite. El Teorema lÃ­mite de De Moivre - Laplace es un Teorema Central
del LÃ­mite para variables aleatorias independientes con distribuciÃ³n Bernoulli(p). Una versiÃ³n
mÃ¡s general es la siguiente:
</p>
</div>
<div id="outline-container-org3f6d056" class="outline-5">
<h5 id="org3f6d056">Teorema 3.1 (Teorema Central del LÃ­mite). Sea X</h5>
<div class="outline-text-5" id="text-org3f6d056">
<p>
1
, X<sub>2</sub>
, &hellip; una sucesiÃ³n de variables aleato
rias independientes idÃ©nticamente distribuidas, cada una con media &mu; y varianza &sigma;}
2
. Entonces
la distribuciÃ³n de
P
n
{i=1}
X
i
â n&mu;}
&sigma;
\sqrt{}
n
tiende a la normal estÃ¡ndar cuando n &rarr; &infin;} . Esto es,
lim
n{&rarr;&infin;}
P
î
P
n
{i=1}
X
i
â n&mu;}
&sigma;
\sqrt{}
n
&le; x
î
= &Phi;(x), 
donde &Phi;(x) :=}
R
x
â&infin;
1
\sqrt{}
2 &pi; 
e
ât
2
/{2}
dt es la funciÃ³n de distribuciÃ³n de una normal de media 0 y}
varianza 1}.
14
\hypertarget{pff}
</p>
</div>
</div>
<div id="outline-container-org14387ab" class="outline-5">
<h5 id="org14387ab">DemostraciÃ³n</h5>
<div class="outline-text-5" id="text-org14387ab">
<p>
Ver CapÃ­tulo XV de Feller, W., (1971). An Introduction to Probability
Theory a nd Its Applications, Vol. II, John Wiley &amp; Sons, New York.
</p>
</div>
</div>
<div id="outline-container-orgfa29bfc" class="outline-5">
<h5 id="orgfa29bfc">Corolario 3.2. Sea X}</h5>
<div class="outline-text-5" id="text-orgfa29bfc">
<p>
1
, X<sub>2</sub>
, &hellip; una sucesiÃ³n de variables aleatorias independientes idÃ©nti
camente distribuidas, cada una con media &mu; y varianza &sigma;}
2
. Si n es suficientemente grande,
para cada valor a &gt; 0 vale la siguiente aproximaciÃ³n
P
î
î
î
î
î
1
n
n
X
{i=1}
X
i
â &mu;}
î
î
î
î
î
&le; a
&sigma;
\sqrt{}
n
!
&asymp; 2&Phi;(a) â 1 (26)
</p>
</div>
</div>
<div id="outline-container-org414e697" class="outline-5">
<h5 id="org414e697">DemostraciÃ³n</h5>
<div class="outline-text-5" id="text-org414e697">
<p>
El teorema central del lÃ­mite establece que si n es suficientemente grande,}
entonces para c ada x &isin; \Re vale que
P
î
P
n
{i=1}
X
i
â n&mu;}
&sigma;
\sqrt{}
n
&le; x
î
&asymp; &Phi;(x) (27)
De la aproximaciÃ³n (27) se deduce que para cada valor a &gt; 0
P
î
î
î
î
î
P
n
{i=1}
X
i
â n&mu;}
&sigma;
\sqrt{}
n
î
î
î
î
&le; a
î
&asymp; &Phi;(a) â &Phi;(â{a) = 2&Phi;(a ) â 1. (28)
El resultado se obtiene de (28) observando que
î
î
î
î
P
n
{i=1}
X
i
â n&mu;}
&sigma;
\sqrt{}
n
î
î
î
î
=
n
&sigma;
\sqrt{}
n
î
î
î
î
î
1
n
n
X
{i=1}
X
i
â &mu;}
î
î
î
î
î
=
\sqrt{}
n
&sigma;
î
î
î
î
î
1
n
n
X
{i=1}
X
i
â &mu;}
î
î
î
î
î
. (29)
</p>
</div>
</div>
<div id="outline-container-orgc06cd52" class="outline-5">
<h5 id="orgc06cd52">Nota Bene</h5>
<div class="outline-text-5" id="text-orgc06cd52">
<p>
Para los usos prÃ¡cticos, especialmente en estadÃ­stica, el resultado lÃ­mite en}
sÃ­ mismo no es de interÃ©s primordial. Lo que interesa es usarlo como una aproximaciÃ³n con
valores finitos de n. Aunque no es posible dar un enunciado consiso sobre cuan buena es la
aproximaciÃ³n, se pueden dar algunas pautas generales y examinando algunos casos especiales
se puede tener alguna idea mÃ¡s precisa del comportamiento de cuan buena es la aproximaciÃ³n.
QuÃ© tan rÃ¡pido la aproximaciÃ³n es buena depende de la distribuciÃ³n de los sumandos. Si
la distribuciÃ³n es bastante simÃ©trica y sus colas decaen rÃ¡pidamente, la aproximaciÃ³n es
buena para valores relativamente pequeÃ±os de n. Si la distribuciÃ³n es muy asimÃ©trica o si
sus col as decaen muy lentamente, se necesitan valores grandes de n para obtener una buena
aproximaciÃ³n.
</p>
</div>
</div>
<div id="outline-container-org0a87959" class="outline-3">
<h3 id="org0a87959">Ejemplos</h3>
<div class="outline-text-3" id="text-org0a87959">
</div>
<div id="outline-container-org9d8204c" class="outline-5">
<h5 id="org9d8204c">Ejemplo 3.3 (Suma de uniformes). Puesto que la distribuciÃ³n uniforme sobre}</h5>
<div class="outline-text-5" id="text-org9d8204c">
<p>
î
â
1
2
,
1
2
î
tiene
media 0 y varianza
1
12
, la suma de 12 variables independientes U}
î
â
1
2
,
1
2
î
tiene media 0 y
varianza 1. La distribuciÃ³n de esa suma estÃ¡ muy cerca de la normal.
</p>
</div>
</div>
<div id="outline-container-org808f1a8" class="outline-5">
<h5 id="org808f1a8">Ejemplo 3.4.</h5>
<div class="outline-text-5" id="text-org808f1a8">
<p>
Para simplificar el cÃ¡lculo de una suma se redondean todos los nÃºmeros al}
entero mÃ¡s cercano. Si el error de redondeo se puede representar como una variable aleatoria
U
î
â
1
2
,
1
2
î
y se suman 12 nÃºmeros, Â¿cuÃ¡l es la probabilidad de que el error de redondeo exceda
1?
15
â4 â3 â2 â1 0 1 2 3 4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
(a)
â3 â2 â1 0 1 2 3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(b)
Figura 5: (a) ComparaciÃ³n entre un histograma de 1000 valores, cada uno de l os cuales es la
suma de 12 variables uniformes U}
î
â
1
2
,
1
2
î
, y la funciÃ³n densidad normal; (b) ComparaciÃ³n
entre la funciÃ³n de distribuciÃ³n empÃ­rica correspondiente a 1000 valores de la suma de 12
uniformes U}
î
â
1
2
,
1
2
î
y la funciÃ³n de distribuciÃ³n normal. El ajuste es sorprendentemente
bueno, especialmente si se tiene en cuenta que 12 no se considera un nÃºmero muy grande.
SoluciÃ³n: El error de redondeo cometido al sumar 12 nÃºmeros se representa por la suma}
P
12
{i=1}
X
i
de 12 variables aleatorias independientes X<sub>1</sub>
, &hellip; , X<sub>12</sub>
cada una con distribuciÃ³n uni
forme sobre el intervalo
î
â
1
2
,
1
2
î
. El error de r edondeo excede 1 si y solamente si
î
î
î
P
12
{i=1}
X
i
î
î
î
&gt; 1.
Puesto que E[X
i
] = 0 y V(X
i
) =
1
12
de acuerdo con el teorema central del lÃ­mite tenemos que
la distribuciÃ³n de
P
12
{i=1}
X
i
â 12{E[X}
i
]
p
12{V(X
i
)
=
12
X
{i=1}
X
i
se puede aproximar por la distribuciÃ³n normal estÃ¡ndar. En consecuencia,
P
î
î
î
î
î
12
X
{i=1}
X
i
î
î
î
î
î
&gt; 1}
!
= 1 â P}
î
î
î
î
î
12
X
{i=1}
X
i
î
î
î
î
î
&le; 1}
!
&asymp; 1 â (&Phi;(1) â &Phi;(â{1))
= 1 â (2&Phi;(1) â 1) = 2 â 2&Phi;(1) = 0.3173&hellip;
16
</p>
</div>
</div>
<div id="outline-container-org1f2cd38" class="outline-5">
<h5 id="org1f2cd38">Ejemplo 3.5 (Suma de exponenciales). La suma S</h5>
<div class="outline-text-5" id="text-org1f2cd38">
<p>
n
de n variables aleatorias independientes
exponenciales de intensidad &lambda; = 1 obedece a una distribuciÃ³n gamma, S}
n
&sim; &Gamma;(n, 1). En la}
siguiente figura se comparan, para distintos valores de n, la funciÃ³n de distribuciÃ³n de la suma
estandarizada
S
n
â{E[S 
n
]
\sqrt{}
V(S
n
)
con la funciÃ³n de distribuciÃ³n normal estÃ¡ndar.
â3 â2 â1 0 1 2 3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figura 6: La normal estÃ¡ndar (sÃ³lida) y las funciones de distribuciÃ³n de las variables &Gamma;(n, 1)
estandarizadas para n = 5 (punteada), n = 10 (quebrada y punteada) y n = 30 (quebrada).
</p>
</div>
</div>
<div id="outline-container-orgd34090f" class="outline-5">
<h5 id="orgd34090f">Ejemplo 3.6.</h5>
<div class="outline-text-5" id="text-orgd34090f">
<p>
La distribuciÃ³n de Poisson de media &lambda; se puede aproximar por la normal para}
valores grandes de &lambda;}: si N &sim; Poisson( &lambda; ), entonces
N â &lambda;
\sqrt{}
&lambda;
&asymp; N(0, 1).
</p>
</div>
</div>
<div id="outline-container-org81c1d93" class="outline-5">
<h5 id="org81c1d93">Ejemplo 3.7.</h5>
<div class="outline-text-5" id="text-org81c1d93">
<p>
Si la emisi Ã³n de una cierta clase de partÃ­culas obedece a un proceso de Poisson}
de intensidad 900 por hora, Â¿cuÃ¡l es la probabilidad de que se emitan mÃ¡s de 950 partÃ­culas
en una hora determinada?
SoluciÃ³n: Sea N una variable Poisson de media 900. Calculamos \mathbb{P}(N &gt; 950) estandarizan
do
\mathbb{P}(N &gt; 950) = P
î
N â 900
\sqrt{}
900
&gt;
950 â 900
\sqrt{}
900
î
&asymp; 1 â &Phi;}
î
5
3
î
= 0.04779.
</p>
</div>
</div>
<div id="outline-container-org0acaf94" class="outline-5">
<h5 id="org0acaf94">Ejemplo 3.8.</h5>
<div class="outline-text-5" id="text-org0acaf94">
<p>
El tiempo de vida de una baterÃ­a es una variable aleatoria de media 40 horas}
y desvÃ­o 20 horas. Una baterÃ­a se usa hasta que falla, momento en el cual se la reemplaza por
17
una nueva. Suponiendo que se dispone de un stock de 25 baterÃ­as, cuyos tiempos de vida son
independientes, aproximar la probabilidad de que pueda obtenerse un uso superior a las 1100
horas.
SoluciÃ³n: Si ponemos X}
i
para denotar el tiempo de vida de la i-Ã©sima baterÃ­a puesta en
uso, lo que buscamos es el valor de p = \mathbb{P}(X<sub>1</sub>
</p>
<ul class="org-ul">
<li>&ctdot; + X<sub>25</sub></li>
</ul>
<p>
&gt; 1000), que puede aproximarse de}
la siguiente manera:
p = P}
P
25
{i=1}
X
i
â 1000}
20
\sqrt{}
25
&gt;
1100 â 1000
20
\sqrt{}
25
!
&asymp; 1 â &Phi;(1) = 0.1587.
</p>
</div>
</div>
<div id="outline-container-orga08d2a0" class="outline-5">
<h5 id="orga08d2a0">Ejemplo 3.9.</h5>
<div class="outline-text-5" id="text-orga08d2a0">
<p>
El peso W (en toneladas) que puede re sistir un puente sin sufrir daÃ±os es
tructurales es una variable aleatoria con distribuciÃ³n normal de media 1400 y desvÃ­o 100. El
peso (en toneladas) de cada camiÃ³n de are na es una variable aleatoria de media 22 y desvÃ­o
0.25. Calcular la probabilidad de que ocurran daÃ±os estructurales cuando hay 64 camiones de
arena sobre el tablero del puente.
SoluciÃ³n: Ocurren daÃ±os estructurales cuando la suma de los pesos de los 64 camiones,}
X<sub>1</sub>
, &hellip; , X
64
, supera al peso W . Por el teorema central del lÃ­mite, la distribuciÃ³n de la suma
P
64
{i=1}
X
i
es aproximadamente una normal de media 1408 y desvÃ­o 2. En consecuencia, W â
P
64
{i=1}
X
i
se distribuye (aproximadamente) como una normal de media 1400 â 1408 = â}8 y
varianza 10000 + 4 = 10004. Por lo tanto,
P
64
X
{i=1}
X
i
&gt; W
!
= P
W â}
64
X
{i=1}
X
i
&lt; 0}
!
= P
W â}
P
64
{i=1}
X
i
</p>
<ul class="org-ul">
<li>8</li>
</ul>
<p>
\sqrt{}
10004
&lt;
8
\sqrt{}
10004
!
&asymp; &Phi;(0.07998&hellip;) = 0.5318&hellip;}
</p>
</div>
</div>
<div id="outline-container-orgd7c6ad0" class="outline-5">
<h5 id="orgd7c6ad0">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-orgd7c6ad0">
<ol class="org-ol">
<li>Un astronauta deberÃ¡ permanecer 435 dÃ­as en el espacio y tiene que optar entre dos}</li>
</ol>
<p>
alternativas. Utilizar 36 tanques de oxÃ­geno de tipo A o 49 tanques de oxigeno de tipo B.
Cada tanque de oxÃ­geno de tipo A tiene un rendimiento de media 12 dÃ­as y desvÃ­o 1 / 4. Cada
tanque de oxÃ­geno de tipo B tiene un rendimiento de media de 8, 75 dÃ­as y desvÃ­o 25 / 28.
Â¿QuÃ© alternativa es la mÃ¡s conveniente?
</p>
<ol class="org-ol">
<li>432 nÃºmeros se redondean al entero mÃ¡s cercano y se suman. Suponiendo que los errores}</li>
</ol>
<p>
individuales de r edondeo se distribuyen uniformemente sobre el i ntervalo (â}0.5, 0.5), aproxi
mar la probabilidad de que la suma de los nÃºmeros redondeados difiera de la suma exacta en
mÃ¡s de 6.
</p>
<ol class="org-ol">
<li>Dos aerolÃ­neas \(A\) y \(B\) que ofrecen idÃ©ntico servicio para viajar de Buenos Aires a San Pablo}</li>
</ol>
<p>
compiten por la misma poblaciÃ³n de 400 clientes, cada uno de los cuales elige una aerolÃ­nea
al azar. Â¿CuÃ¡l es la probabilidad de que la lÃ­nea A tenga mÃ¡s clientes que sus 210 asientos?
18
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orge6bcfd3" class="outline-2">
<h2 id="orge6bcfd3">Distribuciones relacionadas con la Normal</h2>
<div class="outline-text-2" id="text-orge6bcfd3">
<p>
En esta secciÃ³n se presentan tres distribuciones de probabilidad relacionadas con la dis
tribuciÃ³n normal: las distribuciones &Chi;}
2
, t y F . Esas distribuciones aparecen en muchos prob
lemas estadÃ­sticos.
</p>
</div>
<div id="outline-container-org5a9e1b9" class="outline-3">
<h3 id="org5a9e1b9">&Chi;<sup>2</sup> (chi-cuadrado)</h3>
<div class="outline-text-3" id="text-org5a9e1b9">
</div>
<div id="outline-container-org201ae22" class="outline-5">
<h5 id="org201ae22">DefiniciÃ³n 4.1 (DistribuciÃ³n chi-cuadrado con un grado de libertad). Si Z es una una vari</h5>
<div class="outline-text-5" id="text-org201ae22">
<p>
able aleatoria con distribuciÃ³n normal estÃ¡ndar, la distribuciÃ³n de U = Z}
2
se llama la dis
tribuciÃ³n chi-cuadrado con 1 grado de libertad.
0 1 2 3 4 5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Figura 7: GrÃ¡fico de la funciÃ³n densidad de probabilidad de la distribuciÃ³n &Chi;}
2
1
.
CaracterizaciÃ³n de la distribuciÃ³n &Chi;}
2
1
. La funciÃ³n de distribuciÃ³n de la variable U = Z
2
es F}
U
(u) = \mathbb{P}(Z}
2
&le; u), donde Z es N(0, 1). Para cada u &gt; 0, vale que}
F  ( x) = \mathbb{P}(Z
2
&le; u) = \mathbb{P}(}|{Z}| &le;
\sqrt{}
u) = \mathbb{P}(â
\sqrt{}
u &le; Z &le;}
\sqrt{}
u) =}
Z
\sqrt{}
u
â
\sqrt{}
u
1
\sqrt{}
2 &pi; 
e
âz
2
/{2}
dz.
Usando el teorema fundamental del cÃ¡lculo integral y la regla de la cadena obtenemos que
para cada u &gt; 0 vale que
f
U
(u) =
d
du
F
U
(u) =
d
du
Z
\sqrt{}
u
â
\sqrt{}
u
1
\sqrt{}
2 &pi; 
e
âz
2
/{2}
dz
=
1
\sqrt{}
2 &pi; 
î
e
â ( 
\sqrt{}
u ) 
2
/{2}
d
du
(
\sqrt{}
u) â e
â ( â
\sqrt{}
u ) 
2
/{2}
d
du
(â}
\sqrt{}
u ) 
î
=
1
\sqrt{}
2 &pi; 
î
e
â{u/{2
1
2
\sqrt{}
u
</p>
<ul class="org-ul">
<li>e</li>
</ul>
<p>
â{u/{2
1
2
\sqrt{}
u
î
=
1
\sqrt{}
2 &pi; 
î
e
â{u/{2
1
\sqrt{}
u
î
=
(1 / 2)
1
2
\sqrt{}
&pi;
î
u
â{1 / 2}
e
â(1 / 2)u}
î
=
(1 / 2)
1
2
\sqrt{}
&pi;
u
1
2
â{1}
e
â(1 / 2)u}
. (30)
19
La Ãºltima expresiÃ³n que aparece en el lado derecho de la identidad (30) es la expresiÃ³n de la
densidad de la distribuciÃ³n &Gamma;
î
1
2
,
1
2
î
. Por lo tanto,
&Chi;
2
1
= &Gamma;
î
1
2
,
1
2
î
.
</p>
</div>
</div>
<div id="outline-container-org93cf5ad" class="outline-5">
<h5 id="org93cf5ad">Nota Bene</h5>
<div class="outline-text-5" id="text-org93cf5ad">
<p>
Notar que si X &sim; N}(&mu;, &sigma;
2
), entonces
X{â}&mu;
&sigma;
&sim; N(0, 1), y por lo tanto}
î
X{â}&mu;
&sigma;
î
2
&sim;
&Chi;
2
1
.
</p>
</div>
</div>
<div id="outline-container-orgd1af761" class="outline-5">
<h5 id="orgd1af761">DefiniciÃ³n 4.2 (DistribuciÃ³n chi-cuadrado). Si U</h5>
<div class="outline-text-5" id="text-orgd1af761">
<p>
1
, U
2
, &hellip; , U
n
son variables aleatorias inde
pendientes, cada una con distribuciÃ³n &Chi;}
2
1
, la distribuciÃ³n de V =
P
n
{i=1}
U
i
se llama distribuciÃ³n
chi-cuadrado con n grados de libertad y se denota &Chi;
2
n
.
CaracterizaciÃ³n de la distribuciÃ³n chi-cuadrado. La distribuciÃ³n &Chi;
2
n
es un caso par
ticular de la distribuciÃ³n Gamma. MÃ¡s precisamente,
&Chi;
2
n
= &Gamma;
î
n
2
,
1
2
î
.
Basta recordar que la suma de variables &Gamma; i.i.d. tambiÃ©n es &Gamma;. En particular, la funciÃ³n
densidad de V es
f
V
(v) =
(1 / 2)
n
2
&Gamma;
î
n
2
î
v
n
2
â{1}
e
â
1
2
v
1\{v &gt; 0}\}.
</p>
</div>
</div>
<div id="outline-container-org37b3cf3" class="outline-5">
<h5 id="org37b3cf3">Nota Bene</h5>
<div class="outline-text-5" id="text-org37b3cf3">
<p>
La distribuciÃ³n &Chi;
2
n
no es simÃ©trica.
0 5 10 15 20 25
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Figura 8: GrÃ¡fico de la funciÃ³n densidad de probabilidad de la distribuciÃ³n &Chi;}
2
7
.
20
</p>
</div>
</div>
</div>
<div id="outline-container-orgdecc5b2" class="outline-3">
<h3 id="orgdecc5b2">t de Student</h3>
<div class="outline-text-3" id="text-orgdecc5b2">
</div>
<div id="outline-container-org2f64816" class="outline-5">
<h5 id="org2f64816">DefiniciÃ³n 4.3 (La distribuciÃ³n t de Student). Sean Z y U variables aleatorias independientes</h5>
<div class="outline-text-5" id="text-org2f64816">
<p>
con d ist ribuc iones N(0, 1) y &Chi;}
2
n
, respectivamente. La distribuciÃ³n de la variable
T =}
Z
p
U/n
se llama distribuciÃ³n t de Student con n grados de libertad y se denota mediante t
n
.
La funciÃ³n densidad de la t de Student con n grados de libertad es
f
T
(t) =
&Gamma;
î
{n+1}
2
î
\sqrt{}
n&pi;{&Gamma;}
î
n
2
î
î
1 +
t
2
n
î
â
{n+1}
2
.
La fÃ³rmula de la densidad se obtiene por los mÃ©todos estÃ¡ndar desarrollados en las notas
sobre transformaciones de variables.
â5 â4 â3 â2 â1 0 1 2 3 4 5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Figura 9: ComparaciÃ³n de la funciÃ³n densidad de probabilidad de una distribuciÃ³n t
7
(lÃ­nea
solida) con la de la distribuciÃ³n N(0, 1) (lÃ­nea punteada).
</p>
</div>
</div>
<div id="outline-container-orgd485775" class="outline-5">
<h5 id="orgd485775">ObservaciÃ³n 4.4. Notar que la densidad de t}</h5>
<div class="outline-text-5" id="text-orgd485775">
<p>
n
es simÃ©trica respecto del origen. Cuando la
cantidad de grados de libertad, n , es grande la distribuciÃ³n t
n
se aproxima a la la distribuciÃ³n
N(0, 1); de hecho para mÃ¡s de 20 o 30 grados de libertad, las distribuciones son muy cercanas.
.
</p>
</div>
</div>
</div>
<div id="outline-container-org80991d2" class="outline-3">
<h3 id="org80991d2">F de Fisher</h3>
<div class="outline-text-3" id="text-org80991d2">
</div>
<div id="outline-container-org3eb7e40" class="outline-5">
<h5 id="org3eb7e40">DefiniciÃ³n 4.5 (DistribuciÃ³n F ). Sean U y V variables aleatorias independientes con dis-</h5>
<div class="outline-text-5" id="text-org3eb7e40">
<p>
tribuciones &Chi;}
2
m
y &Chi;}
2
n
, respectivamente. La distribuciÃ³n de la variable
W =}
U/m
V/n
21
se llama distribuciÃ³n F con m y n grados de libertad y se denot a por F}
m, n
.
La funciÃ³n densidad de W es
f
W
(w) =
&Gamma;
î
m{+}n
2
î
&Gamma;
î
m
2
î
&Gamma;
î
n
2
î
î
m
n
î
m
2
w
m
2
â{1}
î
1 +
m
n
w
î
â
m{+}n
2
1\{w &ge; 0\}.
W es el cociente de dos variables aleatorias independientes, y su densidad se obtiene usando}
los mÃ©todos estÃ¡ndar desarrollados en las notas sobre transformaciones de variables.
</p>
</div>
</div>
<div id="outline-container-org10b4d99" class="outline-5">
<h5 id="org10b4d99">Nota Bene</h5>
<div class="outline-text-5" id="text-org10b4d99">
<p>
Se puede mostrar que, para n &gt; 2, E[W ] = n/(n â 2). De las definiciones de}
las distribuciones t y F , se deduce que el cuadrado de una variable aleatoria t
n
se distribuye
como una F}
1,n}
.
0 1 2 3 4 5 6 7
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figura 10: GrÃ¡fico tÃ­pico de la funciÃ³n densidad de probabilidad de una distribuciÃ³n F .
Â¿CÃ³mo usar las tablas de las distribuciones F ? Para cada &alpha; &isin; (0, 1), sea F
&alpha;,m,n
el
punto del semieje positivo de las abscisas a cuya derecha la distribuciÃ³n F}
m,n
acumula una
probabilidad &alpha;}:
\mathbb{P}(F
m,n
&gt; F
&alpha;,m,n
) = &alpha;.
</p>
</div>
</div>
<div id="outline-container-orgfbd9d24" class="outline-5">
<h5 id="orgfbd9d24">ObservaciÃ³n 4.6. Notar que de las igualdades}</h5>
<div class="outline-text-5" id="text-orgfbd9d24">
<p>
&alpha; = P}
î
U/m
V/n
&gt; F
&alpha;,m,n
î
= P
î
V/n
U/m
&lt;
1
F
&alpha;,m,n
î
= 1 â P}
î
V/n
U/m
&ge;
1
F
&alpha;,m,n
î
se deduce que
F
1{â{&alpha;,n,m
=
1
F
&alpha;,m,n
. (31)
22
En los manuales de estadÃ­stica se pueden consultar las tablas de los valores F}
&alpha;,m,n
para
diferentes valores de m, n y &alpha; &isin; \}0.01, 0.05{\}. Por ejemplo, segÃºn la tabla que tengo a mi
disposiciÃ³n
1
\mathbb{P}(F
9, 9
&gt; 3.18) = 0.05 y \mathbb{P}(F
9, 9
&gt; 5.35) = 0.01}
Usando esa informaciÃ³n queremos hallar valores Ï}
1
y Ï}
2
tales que
\mathbb{P}(F
9, 9
&gt; Ï
2
) = 0.025 y \mathbb{P}(F}
9, 9
&lt; Ï
1
) = 0.025.
El valor de Ï}
2
se obtiene por interpolaciÃ³n lÃ­neal entre los dos puntos dados en la tabla:
A = (3.18, 0.05) y B = (5.35, 0.01). La ecuaciÃ³n de la rec ta que pasa por ellos es y â 0.01 =}
â
0.04
2.17
(x â} 5.35). En consecuencia, Ï}
2
serÃ¡ la soluciÃ³n de la ecuaciÃ³n 0.025 â}0.01 = â}
0.04
2.17
(Ï}
2
â
5.35). Esto es, Ï}
2
= 4.5362.
El valor de Ï}
1
se obtiene observando que la ecuaciÃ³n \mathbb{P}(F}
9, 9
&lt; Ï
1
) = 0.025 es equivalente
a la ecuaciÃ³n \mathbb{P}(1{/F}
9, 9
&gt; 1}/Ï
1
) = 0.025. Por definiciÃ³n, la distribuciÃ³n de 1{/F}
9, 9
coincide con
la de F}
9, 9
. En consecuencia, Ï}
1
debe satisfacer la ecuaciÃ³n \mathbb{P}(F}
9, 9
&gt; 1}/Ï
1
) = 0.025. Por lo
tanto, Ï}
1
= 1 / 4.5362 = 0.2204.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org60cda05" class="outline-2">
<h2 id="org60cda05">BibliografÃ­a consultada</h2>
<div class="outline-text-2" id="text-org60cda05">
<p>
Para redactar estas notas se consultaron los siguientes libros:
</p>
<ol class="org-ol">
<li>Cramer, H.: MÃ©todos matemÃ¡ticos de estadÃ­stica. Aguilar,
Madrid. (1970)</li>
<li>Durrett R.: Probability. Theory and Examples. Duxbury Press,
Belmont. (1996)</li>
<li>Feller, W.: An introduction to Probability Theory and Its
Applications. Vol. 1. John Wiley &amp; Sons, New York. (1968)</li>
<li>Feller, W.: An introduction to Probability Theory and Its
Applications. Vol. 2. John Wiley &amp; Sons, New York. (1971)</li>
<li>Hoel P. G.: IntroducciÃ³n a la estadÃ­stica matemÃ¡tica. Ariel,
Barcelona. (1980)</li>
<li>Piskunov, N. : CÃ¡lculo diferencial e integral, tomo I. Mir, MoscÃº
(1983)</li>
<li>Rice, J. A.: Mathematical Statistics and Data Analysis. Duxbury
Press, Belmont. (1995)</li>
<li>Ross, S. M: Introduction to Probability and Statistics for
Engineers and Scientists. Elsevier Academic Press, San
Diego. (2004)</li>
<li>Ross, S.: Introduction to Probability Mo del s. Academic Press, San Diego. (2007)</li>
<li>IntroducciÃ³n a la estadÃ­stica matemÃ¡tica. Ariel,
Barcelona. (1980).</li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
Last update: 2019-03-06
</div>
</body>
</html>
