<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-04-02 Thu 19:06 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Transformaciones de variables aleatorias</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/res/org.css"/>
<link rel="stylesheet" type="text/css" href="/home/mk/Documents/blogs/org.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
</head>
<body>
<div id="content">
<div id="outline-container-org136d895" class="outline-2">
<h2 id="org136d895">Funciones de variables aleatorias</h2>
<div class="outline-text-2" id="text-org136d895">
<p>
Sea X una variable aleatoria definida sobre un espacio de probabilidad
\((\Omega, \mathcal{A},\mathbb{P})\). Sea \(g : D \subseteq \Re
\rightarrow \Re\) una función cuyo dominio \(D\) contiene al rango de \(X:
X(\Omega) := \{x(\omega) : \omega \in \Omega\}\).  Entonces \(Y = g(X)\)
está bien definida y será una variable aleatoria si y sólo si
</p>


<p>
\{&omega; &isin; &Omega; : g(X) &le; y \} &isin; A\text{ para todo } y &isin; R. (1)
</p>

<p>
En palabras, si g<sup>-1</sup>
((−&infin;, y]) := \{x &isin; \Re : g(x) &le; y\, el conjunto \{X &isin; g<sup>-1</sup>
(−&infin;, y]\}
</p>

<p>
debe tener asignada probabilidad. Este es típicamente el caso. Por
ejemplo, si \(X\) es discreta, cualquier función \(g\) cuyo dominio
contenga al rango de \(X\) satisface (1). Si \(X\) no es discreta,
cualquier función \(g\) seccionalmente continua cuyo dominio contenga al
rango de \(X\) satisface (1).
</p>
</div>
<div id="outline-container-orgf6b925a" class="outline-3">
<h3 id="orgf6b925a">Método básico: eventos equivalentes</h3>
<div class="outline-text-3" id="text-orgf6b925a">
<p>
Si queremos hallar la función de distribución de Y = g(X) tenemos que calcular
F_Y(y) = \mathbb{P}(Y &le; y) = \mathbb{P}(g(X) &le; y) = \mathbb{P}(X &isin; g<sup>-1</sup>
(−&infin;, y]). (2)
Los siguientes ejemplos ilustran el método básico para hacerlo.}
</p>
</div>
<div id="outline-container-orgb49e938" class="outline-5">
<h5 id="orgb49e938">Ejemplo 1.1 (Del péndulo a la distribución de Cauchy)</h5>
<div class="outline-text-5" id="text-orgb49e938">
<p>
Sea &Theta; el ángulo de un péndulo}
medido desde la vertical cuyo extremo superior se encuentra sostenido del punto (0, 1). Sea
(X, 0) el punto de intersección de la recta que contiene al péndulo y el eje x -ver la Figura 1-.
Trigonometría mediante, sabemos que
X = tan &Theta;}
2
X{0}
1
&Theta;
Figura 1: Péndulo.
Si el ángulo &Theta; es una variable aleatoria uniformemente distribuida sobre el intervalo (−}
&pi;
2
,
&pi;
2
),
cuál es la distribución de X?
Primero observamos que para cada &theta; &isin; (−{&pi;/}2, &pi;/}2) tenemos que
\mathbb{P}(&Theta; &le; &theta;}) =}
&theta; −(−}&pi;/{2)
&pi;/{2 − (−} &pi; /{2)
=
&theta; + &pi;/{2}
&pi;
=
1
2
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
&theta;
&pi;
.
De allí se deduce que
\mathbb{P}(X &le; x) = \mathbb{P}(tan &Theta; &le; x) = \mathbb{P}(&Theta; &le; arctan x) =}
1
2
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
1
&pi;
arctan x,}
y derivando obtenemos que
f_X(x) =
1
&pi;(1 + x
2
)
.
</p>
</div>
</div>
<div id="outline-container-org34f59fd" class="outline-5">
<h5 id="org34f59fd">Teorema 1.2. Sea X una variable aleatoria continua con función de distribución creciente.</h5>
<div class="outline-text-5" id="text-org34f59fd">
<p>
Entonces, Y = F}
X
(X) &sim; \mathcal{U}(0, 1).
</p>
</div>
</div>
<div id="outline-container-org924b7f6" class="outline-5">
<h5 id="org924b7f6">Demostración</h5>
<div class="outline-text-5" id="text-org924b7f6">
<p>
El análisis se reduce a examinar el comportamiento de la función de dis
tribución de Y sobre el intervalo (0, 1). Para cada y &isin; (0, 1) vale que
F_Y(y) = \mathbb{P}(Y &le; y) = \mathbb{P}(F}
X
(X) &le; y) = \mathbb{P}(X &le; F}
−{1}
X
(y)) = F}
X
(F}
−{1}
X
(y)) = y.
</p>
</div>
</div>
<div id="outline-container-org6d73076" class="outline-5">
<h5 id="org6d73076">Corolario 1.3. Sea X una variable aleatoria continua con función de distribución creciente.</h5>
<div class="outline-text-5" id="text-org6d73076">
<p>
Sea Y una variable aleatoria cualquiera. Entonces X puede transformarse en una copia de Y
haciendo lo siguiente:
ˆ
Y = F
−{1}
Y
(F}
X
(X)), donde F
−{1}
Y
es la inversa generalizada de Y .
</p>
</div>
</div>
<div id="outline-container-org9011064" class="outline-5">
<h5 id="org9011064">Ejemplo 1.4.</h5>
<div class="outline-text-5" id="text-org9011064">
<p>
Construir una moneda equilibrada X usando una variable aleatoria T con}
distribución exponencial de intensidad 1.
ˆ
X = 1

1
2
&lt; 1 − e
−{T}
&lt; 1}

.
3
El siguiente ejemplo puede considerarse un prototipo que ilustra cómo tratar con las
funciones de variables aleatorias cuando no son inyectivas.
</p>
</div>
</div>
<div id="outline-container-org97f5d80" class="outline-5">
<h5 id="org97f5d80">Ejemplo 1.5 (Prototipo). Sea X una variable aleatoria cualquiera y sea Y = X}</h5>
<div class="outline-text-5" id="text-org97f5d80">
<p>
2
. Queremos
determinar la distribución de Y .
</p>
<ol class="org-ol">
<li>Cálculo explícito de la función de distribución. La función de distribución de Y se}</li>
</ol>
<p>
calcula observando que g(x) = x
2
y utilizando la fórmula: F}
Y
(y) = P

X &isin; g<sup>-1</sup>
((−&infin;, y])

. En
este caso, el conjunto g<sup>-1</sup>
((−&infin;, y]) adopta la forma
g<sup>-1</sup>
((−&infin;, y]) =

x &isin; R : x
2
&le; y

=

[−}
\sqrt{}
y,
\sqrt{}
y] si y &ge; 0},
&empty; si y &lt; 0.
Por lo tanto,
F_Y(y) = \mathbb{P}(−}
\sqrt{}
y &le; X &le;}
\sqrt{}
y) 1{\}y &ge; 0\} = (}F
X
(
\sqrt{}
y) − F
X
(−}
\sqrt{}
y{−)) 1} \y &ge; 0\}. (3)
En particular, si X es continua, \mathbb{P}(X = x) = 0 para todo x &isin; \Re y la identidad (3) adopta la
forma
F_Y(y) = (F}
X
(
\sqrt{}
y) − F
X
(−}
\sqrt{}
y)) 1{\}y &gt; 0{\} . (4)
</p>
<ol class="org-ol">
<li>Cálculo explícito de la densidad de probabilidades. Si X es absolutamente continua}</li>
</ol>
<p>
con densidad de probabilidades f_X(x), la densidad de probabilidades de Y = X_2
se obtiene
derivando la función de distribución F}
Y
(y). De la identidad (4) se deduce que:
f_Y(y) =
d
dy
F_Y(y) =
</p>

<p>
f
X
(
\sqrt{}
y)
1
2
\sqrt{}
y
− f
X
(−}
\sqrt{}
y)
1
−{2}
\sqrt{}
y

1\{y &gt; 0}\}
=
1
2
\sqrt{}
y
(f
X
(
\sqrt{}
y) + f
X
(−}
\sqrt{}
y)) 1{\}y &gt; 0{\} . (5)
</p>
</div>
</div>
<div id="outline-container-org74fd508" class="outline-5">
<h5 id="org74fd508">Ejemplo 1.6 (De continua a discreta)</h5>
<div class="outline-text-5" id="text-org74fd508">
<p>
Sea U &sim; \mathcal{U} (0, 1]. Hacemos Y = [10 U], donde [x]
representa la parte entera de x &isin; \Re} . Queremos determinar la función de probabilidad de Y .
En primer lugar observamos que la variable aleatoria Y es el primer dígito del desarrollo
decimal de un número elegido al azar sobre el intervalo (0, 1). Los posibles valores de Y son
0, 1, &hellip; , 9. Para cada y &isin; \}0, 1, &hellip; , 9{\} vale que
\mathbb{P}(Y = y) = P
</p>

<p>
y
10
&lt; U &le;}
y + 1}
10

=
1
10
.
En otras palabras, Y &sim; \mathcal{U\}0, 1, &hellip; , 9{\}.
</p>
</div>
</div>
<div id="outline-container-orgfa9fa06" class="outline-5">
<h5 id="orgfa9fa06">Ejemplo 1.7.</h5>
<div class="outline-text-5" id="text-orgfa9fa06">
<p>
Sea T &sim; Exp (&lambda;) la duración en minutos de una llamada telefónica. Se factura}
un pulso cada t_0
minutos o fracción. Queremos determinar la distribución de la cantidad de
pulsos facturados por la llamada.
La cantidad de pulsos facturados por la llamada se describe por:
N =}
X
n &ge; 1
n{1\{ (n −{1)}t_0
&lt; T &le; nt_0
\}.
Notando que N &gt; n \iff T &gt; nt
0
obtenemos que
P(N &gt; n) = e
−{&lambda; nt}
0
=

e
−{&lambda; t}
0

n
= \mathbb{P}(T &gt; t}
0
)
n
.
Por lo tanto, N &sim; Geométrica (\mathbb{P}(T &le; t}
0
)).
4
</p>
</div>
</div>
<div id="outline-container-orgc070884" class="outline-5">
<h5 id="orgc070884">Ejemplo 1.8 (Variables discretas)</h5>
<div class="outline-text-5" id="text-orgc070884">
<p>
Sea X una variable aleatoria discreta a valores (x}
i
)
i &ge; 1
.
De la relación Y = g(X) se deduce que los posibles valores de Y son y
i
= g(x
i
), i &ge; 1. Si la
función de probabilidad de X está dada por p
X
(x
i
) = p
i
, i &ge; 1, la función de probabilidad de
Y se determina por}
p
Y
(y
i
) = \mathbb{P}(Y = y
i
) = \mathbb{P}(X &isin; g<sup>-1</sup>
(y
i
)) =
X
{x &isin; g<sup>-1</sup>}
(y
i
)
p
x
.
</p>
</div>
</div>
<div id="outline-container-org5c597b2" class="outline-5">
<h5 id="org5c597b2">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-org5c597b2">
<ol class="org-ol">
<li>Sea X una variable aleatoria discreta tal que \mathbb{P}(X = −}1) = 1 / 2, \mathbb{P}(X = 0) = 1 / 4 y}</li>
</ol>
<p>
\mathbb{P}(X = 1) = \mathbb{P}(X = 2) = 1 / 8. Hallar la función de probabilidad de Y para Y = 2X + 1 y para}
Y = 2}X_2
</p>
<ul class="org-ul">
<li>1.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-org9f70456" class="outline-3">
<h3 id="org9f70456">Funciones a trozos: dividir y conquistar</h3>
<div class="outline-text-3" id="text-org9f70456">
<p>
Sea X una variable y sea A_1
, A_2
, &hellip; una partición de R tal que \mathbb{P}(X &isin; A_i
) &gt; 0 para todo
i &ge; 1. Consideramos una función a trozos definida por
g (x) =}
X
i &ge; 1
g
i
(x)1\{x &isin; A_i
\,
donde, para cada i &ge; 1, g
i
</p>
<pre class="example">
R \rightarrow R, es una función tal que g

</pre>
<p>
i
(X) es una variable aleatoria. Si
se quiere hallar la distribución de
Y = g (X) =}
X
i &ge; 1
g
i
(X)1\{X &isin; A_i
\}
se puede hacer lo siguiente: considerar las variables truncadas X
i
= X | X &isin; A}
i
, hallar las
distribuciones de las variables Y
i
= g
i
(X
i
) y luego ponderarlas con los pesos \mathbb{P}(X &isin; A}
i
):
F_Y(y) =
X
i &ge; 1
F
Y
i
(y)\mathbb{P}(X &isin; A}
i
). (6)
En efecto, por una parte tenemos que
F_Y(y) = P


X
j &ge; 1
g
j
(X)1\{X &isin; A
j
\} &le; y


=
X
i &ge; 1
P


X
j &ge; 1
g
j
(X)1\{X &isin; A
j
\} &le; y, X &isin; A_i


=
X
i &ge;
\mathbb{P}(g}
i
(X) &le; y, X &isin; A}
i
) =
X
i &ge; 1
P

X &isin; g<sup>-1</sup>
i
(−&infin;, y] &cap; A}
i

. (7)
Por otra parte,
F
Y
i
(y) = \mathbb{P}(g
i
(X
i
) &le; y) = \mathbb{P}(X
i
&isin; g<sup>-1</sup>
(−&infin;, y]) =
\mathbb{P}(X &isin; g<sup>-1</sup>
(−&infin;, y] &cap; A}
i
)
\mathbb{P}(X &isin; A_i
)
.
Equivalentemente,
P(X &isin; g<sup>-1</sup>
(−&infin;, y] &cap; A}
i
) = F}
Y
i
(y)\mathbb{P}(X &isin; A}
i
). (8)
Combinando (7) y (8) se obtiene (6).
5
</p>
</div>
</div>
<div id="outline-container-orgb3db6b7" class="outline-3">
<h3 id="orgb3db6b7">Funciones inyectivas suaves</h3>
<div class="outline-text-3" id="text-orgb3db6b7">
</div>
<div id="outline-container-org2020e83" class="outline-5">
<h5 id="org2020e83">Teorema 1.9 (Cambio de variables). Sea X una variable aleatoria absolutamente continua}</h5>
<div class="outline-text-5" id="text-org2020e83">
<p>
con densidad de probabilidades f_X(x). Sea Y = g(X), donde g es una función monótona
con derivada no nula. Entonces Y es absolutamente continua y admite una densidad de
probabilidades de la forma
f_Y(y) =
f_X(x)
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">g</td>
</tr>
</tbody>
</table>
<p>
′
(x) |




{x=g<sup>-1</sup>
(y)
. (9)
</p>
</div>
</div>
<div id="outline-container-org27377f9" class="outline-5">
<h5 id="org27377f9">Demostración.</h5>
<div class="outline-text-5" id="text-org27377f9">
<ol class="org-ol">
<li>La función g e s creciente{: g(x }</li>
</ol>
<p>
1
) &le; g(x
2
) para x
1
&le; x
2
. En tal caso la función inversa
g<sup>-1</sup>
también es creciente. En consecuencia,
F_Y(y) = \mathbb{P}(Y &le; y) = \mathbb{P}(g(X) &le; y) = P

X &le; g<sup>-1</sup>
(y)

= F}
X

g<sup>-1</sup>
(y)

. (10)
La función F}
Y
(y) es derivable porque es una composición de funciones derivables. Derivando
con respecto a y y usando la regla de la cadena se obtiene
d
dy
F_Y(y) =
d
dy
F
X
(g<sup>-1</sup>
(y)) =
f
X
(g<sup>-1</sup>
(y))
g
′
(g<sup>-1</sup>
(y))
.
</p>
<ol class="org-ol">
<li>La función g es decreciente{: g(x }</li>
</ol>
<p>
1
) &ge; g(x
2
) para x
1
&le; x
2
. En este caso la función inversa
g<sup>-1</sup>
también es decreciente. En consecuencia,
F_Y(y) = \mathbb{P}(Y &le; y) = \mathbb{P}(g(X) &le; y) = P

X &ge; g<sup>-1</sup>
(y)

= 1 − F
X

g<sup>-1</sup>
(y)

. (11)
Derivando con respecto a y se obtiene
d
dy
F_Y(y) =
d
dy

1 −{F
X
(g<sup>-1</sup>
(y))

= −}
f
X
(g<sup>-1</sup>
(y))
g
′
(g<sup>-1</sup>
(y))
.
</p>
</div>
</div>
<div id="outline-container-org3733802" class="outline-5">
<h5 id="org3733802">Corolario 1.10 (Cambio lineal). Dados a &gt; 0 y b &isin; \Re}, la densidad de probabilidades de}</h5>
<div class="outline-text-5" id="text-org3733802">
<p>
Y = aX + b adopta la forma}
f_Y(y) =
1
a
f
X
</p>

<p>
y − b
a

. (12)
En palabras, desde el punto de vista de la densidad de probabilidades, el cambio lineal
y = ax + b efectúa una traslación en b seguida de un cambio de escala de 1 en a sobre la }
densidad original. Cuando e l parámetro a se achica, los valores de Y tienden a estar más
concentrados (alrededor del valor medio) y cuando a se agranda, tienden a dispe rsarse.
</p>
</div>
</div>
<div id="outline-container-org5d1b1ea" class="outline-5">
<h5 id="org5d1b1ea">Ejemplo 1.11</h5>
<div class="outline-text-5" id="text-org5d1b1ea">
<p>
(Variables exponenciales). Se dice que la variable aleatoria Y tiene distribución
exponencial de intensidad &lambda; &gt; 0, y se denota Y &sim; Exp(}&lambda;), si Y =
1
&lambda;
X, donde X es una}
variable aleatoria absolutamente continua que admite una densidad de probabilidades de la
forma f_X(x) = e
−x
1\{x &ge; 0}\. De (12) se deduce que Y admite una densidad de probabilidades
de la forma f_Y(y) = &lambda; e}
−{&lambda; y}
1\{y &ge; 0\}.
</p>
</div>
</div>
<div id="outline-container-orgaf951ae" class="outline-5">
<h5 id="orgaf951ae">Ejemplo 1.12</h5>
<div class="outline-text-5" id="text-orgaf951ae">
<p>
(Variables Normale s). Sean &mu; &isin; \Re y &sigma; &gt; 0. Se dice que la variable aleatoria}
Y tiene distribución normal de parámetros &mu;, &sigma;
2
, y se denota Y &sim; N}(&mu;, &sigma;
2
), si Y = \sigmaX + &mu;,
donde X es una variable aleatoria absolutamente continua con densidad de probabilidades
&varphi; (x) =}
1
\sqrt{}
2 &pi;
e
−x
2
/{2}
. De (12) se deduce que Y admite una densidad de probabilidades de la
forma f_Y(y) =
1
\sqrt{}
2{&pi;&sigma;}
exp

−
(y{−}&mu;)
2
2 &sigma;
2

.
6
</p>
</div>
</div>
</div>
<div id="outline-container-orga973c57" class="outline-3">
<h3 id="orga973c57">Funciones suaves</h3>
<div class="outline-text-3" id="text-orga973c57">
</div>
<div id="outline-container-orgf5cd933" class="outline-5">
<h5 id="orgf5cd933">Nota Bene</h5>
<div class="outline-text-5" id="text-orgf5cd933">
<p>
Las fórmulas (10) y (11) permiten calcular explícitamente la función de dis
tribución, F}
Y
, para transformaciones monótonas (continuas) Y = g(X), independientemente
de la clase de variable que sea X. ¿Qué hacer cuando la transformación g es suave pero no e s
inyectiva?
</p>
</div>
</div>
<div id="outline-container-org8fc7606" class="outline-5">
<h5 id="org8fc7606">Ejemplo 1.13.</h5>
<div class="outline-text-5" id="text-org8fc7606">
<p>
Sea X &sim; N}(0, 1). Según la fórmula (5) la densidad de probabilidades de}
Y = X_2
es f_Y(y) =
1
2
\sqrt{}
y

&varphi; (
\sqrt{}
y) + &varphi;(−
\sqrt{}
y)

1\{y &gt; 0} \, donde &varphi;(x) =
1
\sqrt{}
2 &pi;
e
−x
2
/{2}
. Por lo tanto,
f_Y(y) =
1
\sqrt{}
2 &pi;
y
−{1 / 2}
e
−{y/{2
1\{y &gt; 0}\}.
En otras palabras, si X &sim; N}(0, 1), entonces X_2
&sim; &Gamma;(1 / 2, 1 / 2).
El Teorema 1.9 puede generalizarse del siguie nte modo
</p>
</div>
</div>
<div id="outline-container-org4ab701c" class="outline-5">
<h5 id="org4ab701c">Teorema 1.14 (Cambio de variables II). Sea X una variable aleatoria absolutamente con</h5>
<div class="outline-text-5" id="text-org4ab701c">
<p>
tinua con densidad de probabilidades f_X(x). Sea Y = g(X), donde g es una función deriv
able con derivada no nula (salvo en contables puntos). Si para cada y &isin; \Re}, el conjunto
g<sup>-1</sup>
(y) = \{x &isin; \Re : g(x) = y{\} es discreto, entonces Y es absolutamente continua y admite una
función densidad de probabilidades de la forma
f_Y(y) =
X
{x &isin; g<sup>-1</sup>}
(y)
f_X(x)
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">g</td>
</tr>
</tbody>
</table>
<p>
′
(x) |
.
Se sobreentiende que si g<sup>-1</sup>
(y) = &empty;, f_Y(y) = 0.
</p>
</div>
</div>
<div id="outline-container-org17172ba" class="outline-5">
<h5 id="org17172ba">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-org17172ba">
<ol class="org-ol">
<li>[James p.98] Si X tiene densidad f}</li>
</ol>
<p>
X
(x), cuál es la densidad de Y = cos X?
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org8155e0f" class="outline-2">
<h2 id="org8155e0f">Funciones de vectores aleatorios</h2>
<div class="outline-text-2" id="text-org8155e0f">
</div>
<div id="outline-container-org665e5a0" class="outline-3">
<h3 id="org665e5a0">Método básico: eventos equivalentes</h3>
<div class="outline-text-3" id="text-org665e5a0">
<p>
Sea X = (X_1
, &hellip; , X
n
) un vector aleatorio definido sobre un espacio de probabilidad
(&Omega;, \mathcal{A},\mathbb{P}) y sea g : \Re
n
&rarr; \Re una función cualquiera. Entonces, Y := g(X) será una variable
aleatoria si y solo si \{&omega; &isin; &Omega; : g(X(&omega;)) &le; y\} &isin; A para todo y &isin; \Re} . La función de distribución
de Y , F}
Y
(y), se puede calc ular mediante la función de distribución de X de la siguiente
manera:
F_Y(y) = \mathbb{P}(Y &le; y) = \mathbb{P}(g(X) &le; y) = \mathbb{P}(X &isin; B
y
) , (13)
donde B
y
:= g<sup>-1</sup>
((−&infin;, y]) = \{x &isin; \Re}
n
</p>
<pre class="example">
g(x) \leq y\}.

</pre>
<p>
7
</p>
</div>

<div id="outline-container-org21977c7" class="outline-4">
<h4 id="org21977c7">Caso bidimensional continuo</h4>
<div class="outline-text-4" id="text-org21977c7">
<p>
Sea (X,Y) un vector aleatorio con densidad conjunta
\(f_{X,Y}(x, y)\). Cualquier función continua a valores reales g : \Re
2
&rarr; \Re define una nueva variable
aleatoria Z := g(X,Y). La función de distribución de Z, F}
Z
(z) = \mathbb{P}(Z &le; z), se puede obtener
a partir de la densidad conjunta de X e Y de la siguiente forma:
</p>
<ol class="org-ol">
<li>Para cada z &isin; \Re se determina el conjunto B</li>
</ol>
<p>
z
&sub; R}
2
de todos los puntos (x, y) tales que
g (x, y) &le; z.
</p>
<ol class="org-ol">
<li>Integrando la densidad conjunta f</li>
</ol>
<p>
_{X,Y}
(x, y) sobre el conjunto B
z
se obtiene la función
de distribución de Z}:
F
Z
(z) =
x
B
z
f
_{X,Y}
(x, y)dxdy. (14)
</p>
<ol class="org-ol">
<li>La densidad de Z se obtiene derivando la función de distribución respecto de z.</li>
</ol>
</div>
<div id="outline-container-org84be5eb" class="outline-5">
<h5 id="org84be5eb">Ejemplo 2.1.</h5>
<div class="outline-text-5" id="text-org84be5eb">
<p>
Sean X e Y dos variables aleatorias independientes cada una con distribución}
uniforme sobre el intervalo [−}1, 1]. Se quiere hallar la función de distribución y la densidad
de Z = |X − Y | .
La función de distribución de la variable Z = |X − Y | se puede obtener observando la
Figura 2.
1
1
−{1}
−{1}
y = x + z
2 − z}
y = x − z
y
x
Figura 2: La región sombreada representa los puntos del cuadrado [−}1, 1] &times; [−}1, 1] tales que
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">x −y</td>
<td class="org-left">&le; z, 0 &le; z &le; 2 y su área es 4 − (2 −z)</td>
</tr>
</tbody>
</table>
<p>
2
= 4{z − z}
2
.
Debido a que las variables aleatorias X e Y son independientes y uniformemente dis
tribuidas obre e l intervalo [−}1, 1], tenemos que \mathbb{P}((X,Y) &isin; B) = área(B) / 4, para cualquier
región B contenida en el cuadrado [−}1, 1] &times; [−}1, 1] para la que tenga sentido la noción
de área. En consecuencia, F}
Z
(z) = \mathbb{P}(|X − Y | &le; z) = (4{z − z}
2
) / 4 para to do z &isin; [0, 2].
Derivando esta última expresión respecto de z se obtiene la densidad de Z = |X − Y | :
f
Z
(z) =

2{−z}
2

1\{z &isin; (0, 2)\}.
8
Caso bidimensional discreto. Sea (X,Y) un vector aleatorio discreto sobre un espacio}
de probabilidad (&Omega;, \mathcal{A},\mathbb{P}), con función de probabilidad conjunta p
_{X,Y}
(x, y). Sea g : \Re
2
&rarr;
R una función cualquiera, Z := g(X,Y) es una nueva variable aleatoria, cuya función de}
probabilidad, p
Z
(z), se obtiene de la siguiente manera:
p
Z
(z) = \mathbb{P}(Z = z) = \mathbb{P}(g(X,Y) = z) =
X
(x,y)\inB}
z
p
_{X,Y}
(x, y), (15)
donde B
z
= \(x, y) &isin; X(&Omega;) &times; Y (&Omega;) : g(x, y) = z{\} .
2.1.1. Suma de variables
</p>
</div>
</div>
<div id="outline-container-orgd714230" class="outline-5">
<h5 id="orgd714230">Ejemplo 2.2 (Suma). Sean X, Y dos variables aleatorias con densidad conjunta f}</h5>
<div class="outline-text-5" id="text-orgd714230">
<p>
_{X,Y}
(x, y)
y sea Z = X + Y . Para cada z &isin; \Re}, B
z
= \(x, y) &isin; \Re}
2
</p>
<pre class="example">
y \leq z − x{\} . Usando la fórmula (14)

</pre>
<p>
se obtiene la función de distribución de Z}
F
Z
(z) =
Z
&infin;
−&infin;
</p>

<p>
Z
z{−}x
−&infin;
f
_{X,Y}
(x, y)dy}

dx. (16)
La densidad de Z se obtiene derivando respecto de z la función de distribución F}
Z
(z)
f
Z
(z) =
d
dz
F
Z
(z) =
Z
&infin;
−&infin;
f
_{X,Y}
(x, z − x)dx. (17)
</p>
</div>
</div>
<div id="outline-container-orgde40773" class="outline-5">
<h5 id="orgde40773">Ejemplo 2.3 (Suma de variables independientes)</h5>
<div class="outline-text-5" id="text-orgde40773">
<p>
Sean X, Y dos variables aleatorias contin
uas e independientes con densidad conjunta f
_{X,Y}
(x, y) = f_X(x)f_Y(y). Según la fórmula (17)
la densidad de probabilidades de la suma Z = X + Y es
f
Z
(z) =
Z
&infin;
−&infin;
f
_{X,Y}
(x, z − x)dx =
Z
&infin;
−&infin;
f_X(x)f
Y
(z − x)dx (18)
y se denomina el producto convolución, f
X
∗ f
Y
, de las densidades marginales f
X
y f
Y
.
Si las densidades marginales f_X(x) y f_Y(y) concentran la masa en [0, &infin;}) la fórmula (18)
del producto convolución es un poco más sencilla:
(f
X
∗ f
Y
)(z) =
Z
&infin;
0
f_X(x)f
Y
(z − x)dx =
Z
z
0
f_X(x)f
Y
(z − x)dx. (19)
</p>
</div>
</div>
<div id="outline-container-org21f7ca6" class="outline-5">
<h5 id="org21f7ca6">Ejemplo 2.4 (Suma de exponenciales independientes de igual intensidad)</h5>
<div class="outline-text-5" id="text-org21f7ca6">
<p>
Sean X e Y}
variables aleatorias independientes con distribución exponencial de intensidad &lambda; &gt; 0. La
densidad de la suma X + Y es
f
X{+}Y
(z) =
Z
z
0
&lambda; e
−{&lambda; x}
&lambda; e
z{−}x
dx = &lambda;
2
ze
−{&lambda; z}
. (20)
En el lado derecho de la identidad (20) se puede reconocer la densidad de la distribución
Gamma: &Gamma;(2, &lambda;).
9
\hypertarget{pfa}
2.1.2. Mínimo
Queremos caracterizar la función de distribución del mínimo entre dos variables aleatorias
X e Y , U := mín\{X , Y \}. En pri
mer lugar observamos que para cada u &isin; \Re vale que}
F
U
(u) = \mathbb{P}(U &le; u) = \mathbb{P}(mín\{X, Y \} &le; u) = 1 −\mathbb{P}(mín\{X, Y \} &gt; u})
= 1 −\mathbb{P}(X &gt; u, Y &gt; u). (21)
Si (X,Y) es continuo con función de densidad conjunta f
_{X,Y}
(x, y) tenemos que
F
U
(u) = 1 −}
Z
&infin;
u
Z
&infin;
u
f
_{X,Y}
(x, y)dxdy. (22)
Si (X,Y) es discreto con función de probabilidad conjunta p
_{X,Y}
(x, y) tenemos que
F
U
(u) = 1 −}
X
x&gt;u
X
y&gt;u
p
_{X,Y}
(x, y). (23)
Si X e Y son independientes tenemos que
F
U
(u) = 1 − \mathbb{P}(X &gt; u)\mathbb{P}(Y &gt; u). (24)
Etcétera&#x2026;
</p>
</div>
</div>
<div id="outline-container-orgc41b74a" class="outline-5">
<h5 id="orgc41b74a">Ejemplo 2.5 (Mínimo de exponenciales independientes)</h5>
<div class="outline-text-5" id="text-orgc41b74a">
<p>
Sean X}
1
e X_2
variables aleatorias
exponenciales independientes de intensidades &lambda;}
1
y &lambda;}
2
respectivamente. De acuerdo con la
identidad (24) tenemos que la función de distribución del mínimo U = mín\{X}
1
, X_2
\} es}
F
U
(u) = (1 − e}
− &lambda;
1
u
e
− &lambda;
2
u
)1\{u &ge; 0{\} = (1 − e}
−(&lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)u
)1\{u &ge; 0{\. (25)
En palabras, el mínimo de dos variables exponenciales independientes es una exponencial cuya}
intensidad es la suma de las intensidades de las variables originales.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org07a0079" class="outline-3">
<h3 id="org07a0079">El método del Jacobiano</h3>
<div class="outline-text-3" id="text-org07a0079">
</div>
<div id="outline-container-orgf61b152" class="outline-5">
<h5 id="orgf61b152">Teorema 2.6 (Cambio de variables en la integral múltiple). Sea f : \Re</h5>
<div class="outline-text-5" id="text-orgf61b152">
<p>
n
&rarr; \Re una función
integrable. Sean G}
0
&sub; R}
n
y G &sub; \Re
n
regiones abiertas y sea h : G}
0
&rarr; G, h = (h}
1
, &hellip; , h
n
)
una biyección entre G}
0
y G, cuyas componentes tienen derivadas parciales de primer orden
continuas. Esto es, pa ra todo 1 &le; i, j &le; n}, las funciones
&part; h
i
(y)
&part; y
j
son continuas. Si el Jacobiano
de h es diferente de cero en casi todo punto, entonces,
Z
A
f(x)d{x =
Z
h
−{1}
(A)
f (h(y)) | }J
h
(y)|{dy,
para todo conjunto ab ierto A &sub; G, donde
J
h
(y) = det
</p>

<p>
&part; h
i
(y)
&part; y
j

i,j
!
.
10
\hypertarget{pfb}
El siguiente resultado, que caracteriza la distribución de un cambio de variables aleatorias,
es una consecuencia inmediata del Teorema 2.6.
</p>
</div>
</div>
<div id="outline-container-org30af185" class="outline-5">
<h5 id="org30af185">Corolario 2.7. Sea X un vector aleatorio n-dimensional con función densidad de probabilidad}</h5>
<div class="outline-text-5" id="text-org30af185">
<p>
f_X(x). Sean G}
0
&sub; R}
n
y G &sub; \Re
n
regiones abiertas y sea g : G &rarr; G}
0
una biyección cuya función
inversa h = g<sup>-1</sup>
satisface las hipótesis del Teorema 2.6. Si \mathbb{P}(X &isin; G) = 1, entonces, el vector
aleatorio Y = g(X) tiene función densidad de probabilidad f_Y(y) de la forma:
f_Y(y) = f
X
(g<sup>-1</sup>
(y))|{J
g<sup>-1</sup>
(y)|. (26)
</p>
</div>
</div>
<div id="outline-container-orgabfc8dd" class="outline-5">
<h5 id="orgabfc8dd">Demostración</h5>
<div class="outline-text-5" id="text-orgabfc8dd">
<p>
Cualquiera sea el conjunto abierto B &sub; G
0
tenemos
\mathbb{P}(Y &isin; B}) = \mathbb{P}(g(X) &isin; B) = \mathbb{P}(X &isin; g
−{1}
(B)) =
Z
g<sup>-1</sup>
(B)
f_X(x)dx.
Poniendo f = f
X
y h = g<sup>-1</sup>
en el Teorema 2.6 se obtiene
Z
g<sup>-1</sup>
(B)
f_X(x)dx =
Z
B
f
X
(g<sup>-1</sup>
(y))|{J
g<sup>-1</sup>
(y)|{dy.}
En consecuencia,
\mathbb{P}(Y &isin; B}) =}
Z
B
f
X
(g<sup>-1</sup>
(y))|{J
g<sup>-1</sup>
(y)|{dy.}
Por lo tanto, el vector aleatorio Y tiene función densidad de probabilidad de la forma f_Y(y) =
f
X
(g<sup>-1</sup>
(y))|{J
g<sup>-1</sup>
(y) | .
</p>
</div>
</div>
<div id="outline-container-org4df9fff" class="outline-5">
<h5 id="org4df9fff">Nota Bene</h5>
<div class="outline-text-5" id="text-org4df9fff">
<p>
Operativamente, la fórmula (26) para hallar la densidad conjunta de Y = g(X)
involucra los siguientes pasos: 1. Invertir las variables (i.e., despejar las x's en función de las
y{'s). 2. Calcular el Jacobiano de la inversa de g (i.e., calcular el determinante de la matriz}
formada por las derivadas parciales de las x
i
respecto de las y
j
). 3. Substituir los resultados
obtenidos en los pasos 1. y 2. en la fórmula (26). Aunque mecánico, el método del}
jacobiano es un método de naturaleza analítica muy poderoso.
</p>
</div>
</div>
<div id="outline-container-org64e9293" class="outline-5">
<h5 id="org64e9293">Nota Bene</h5>
<div class="outline-text-5" id="text-org64e9293">
<p>
Con frecuencia es más fácil obtener el jacobiano de y en relación a x, pues Y}
es una función de X. Hay que recordar que los dos jacobianos son recíprocos y que J}
g<sup>-1</sup>
(y) se
puede obtener a partir de J}
g
(x), invirtiendo este último y substituyendo x por g<sup>-1</sup>
(y). Esta
regla es análoga a la regla para la derivada de una función inversa en el caso unidimensional:
dg<sup>-1</sup>
(y)
dy
=
1
g
′
(x)




{x=g<sup>-1</sup>
(y)
=
1
g
′
(g<sup>-1</sup>
(y))
.
</p>
</div>
</div>
<div id="outline-container-orga9dba13" class="outline-5">
<h5 id="orga9dba13">Ejemplo 2.8 (Transformaciones lineales)</h5>
<div class="outline-text-5" id="text-orga9dba13">
<p>
Si (X}
1
, X_2
) = (aY}
1
</p>
<ul class="org-ul">
<li>bY}</li>
</ul>
<p>
2
, cY_1
</p>
<ul class="org-ul">
<li>dY}</li>
</ul>
<p>
2
). Entonces,
f
Y_1
,Y
2
(y
1
, y
2
) = |{ad − bc}|f}
X_1
,X_2
(ay}
1
</p>
<ul class="org-ul">
<li>by}</li>
</ul>
<p>
2
, cy
1
</p>
<ul class="org-ul">
<li>dy}</li>
</ul>
<p>
2
).
En general, si X = AY, donde A &isin; \Re
n{&times;}n
es una matriz inversible, se obtiene
f_Y(y) = | det(A) | f
X
(Ay). (27)
11
\hypertarget{pfc}
</p>
</div>
</div>
<div id="outline-container-orgfe36378" class="outline-5">
<h5 id="orgfe36378">Ejemplo 2.9 (Suma y resta de normales independientes). Sean X}</h5>
<div class="outline-text-5" id="text-orgfe36378">
<p>
1
y X_2
dos variables al eato
rias independientes con distribuciones normales N(&mu;
1
, &sigma;
2
) y N(&mu;
2
, &sigma;
2
), respectivamente. Su
densidad conjunta es
f
X_1
,X_2
(x
1
, x
2
) =
1
2{&pi;&sigma;}
2
exp
</p>

<p>
−
1
2 &sigma;
2

(x
1
− &mu;}
1
)
2
</p>
<ul class="org-ul">
<li>(x</li>
</ul>
<p>
2
− &mu;}
2
)
2


(28)
Consideramos el cambio de variables (y
1
, y
2
) = g(x
1
, x
2
) = (x
1
</p>
<ul class="org-ul">
<li>x</li>
</ul>
<p>
2
, x
1
− x
2
) cuya inversa es
(x
1
, x
2
) = g<sup>-1</sup>
(y
1
, y
2
) =
1
2
(y
1
</p>
<ul class="org-ul">
<li>y</li>
</ul>
<p>
2
, y
1
− y
2
). De acuerdo con la fórmula (27) tenemos que
f
Y_1
,Y
2
(y
1
, y
2
) =
1
4{&pi;&sigma;}
2
exp
−
1
2 &sigma;
2
</p>

<p>
y
1
</p>
<ul class="org-ul">
<li>y</li>
</ul>
<p>
2
2
− &mu;}
1

2
</p>
<ul class="org-ul">
<li></li>
</ul>

<p>
y
1
− y
2
2
− &mu;}
2

2
!!
&prop; exp}
</p>

<p>
−
1
4 &sigma;
2

y
2
1
− 2(&mu; }
1
</p>
<ul class="org-ul">
<li>&mu;}</li>
</ul>
<p>
2
)y
1


exp
</p>

<p>
−
1
4 &sigma;
2

y
2
2
− 2(&mu; }
1
− &mu;}
2
)y
2


&prop; exp}
</p>

<p>
−
(y
1
− (&mu; }
1
</p>
<ul class="org-ul">
<li>&mu;}</li>
</ul>
<p>
2
))
2
2(2 &sigma;
2
)

exp
</p>

<p>
−
(y
2
− (&mu; }
1
− &mu;}
2
))
2
2(2 &sigma;
2
)

. (29)
De la identidad (29) podemos concluir que las variables Y_1
e Y
2
son independientes y que
se distribuyen de la siguiente manera: Y_1
&sim; N(&mu; }
1
</p>
<ul class="org-ul">
<li>&mu;}</li>
</ul>
<p>
2
, 2}&sigma;
2
), Y
2
&sim; N(&mu; }
1
− &mu;}
2
, 2}&sigma;
2
). En
otras palabras, si X}
1
y X_2
son dos variables aleatorias independientes con distribuciones
normales N(&mu;
1
, &sigma;
2
) y N}(&mu;
2
, &sigma;
2
), entonces X}
1
+X_2
y X_1
−X_2
son independientes y X_1
+X_2
&sim;
N(&mu; }
1
</p>
<ul class="org-ul">
<li>&mu;}</li>
</ul>
<p>
2
, 2}&sigma;
2
) y X}
1
− X_2
&sim; N(&mu; }
1
− &mu;}
2
, 2}&sigma;
2
)
</p>
</div>
</div>
<div id="outline-container-orgf84ec1a" class="outline-5">
<h5 id="orgf84ec1a">Nota Bene</h5>
<div class="outline-text-5" id="text-orgf84ec1a">
<p>
Sean X}
1
y X_2
dos variables aleatorias independientes con distribuciones nor
males N(&mu;
1
, &sigma;
2
1
) y N(&mu;
2
, &sigma;
2
2
), respectivamente. Cálculos similares permiten deducir que X_1
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
X_2
&sim; N(&mu; }
1
</p>
<ul class="org-ul">
<li>&mu;}</li>
</ul>
<p>
2
, &sigma;
2
1
</p>
<ul class="org-ul">
<li>&sigma;}</li>
</ul>
<p>
2
2
) y X}
1
− X_2
&sim; N(&mu; }
1
− &mu;}
2
, &sigma;
2
1
</p>
<ul class="org-ul">
<li>&sigma;}</li>
</ul>
<p>
2
2
). Más aún, X}
1
</p>
<ul class="org-ul">
<li>X_2</li>
</ul>
<p>
y X_1
− X_2
son independientes si y solo si &sigma;}
2
1
= &sigma;}
2
2
.
</p>
</div>
</div>
<div id="outline-container-org0309c23" class="outline-5">
<h5 id="org0309c23">Ejemplo 2.10</h5>
<div class="outline-text-5" id="text-org0309c23">
<p>
(Persistencia de la mala suerte). Sean X}
1
y X_2
variables aleatorias inde
pendientes con distribución común exponencial de intensidad &lambda;}. Vamos a hallar la densidad
conjunta de (Y_1
, Y
2
) donde
(Y_1
, Y
2
) = (X_1
</p>
<ul class="org-ul">
<li>X_2</li>
</ul>
<p>
, X_1
/X_2
).
Para ello consideramos la transformación
g (x
1
, x
2
) = (x
1
</p>
<ul class="org-ul">
<li>x</li>
</ul>
<p>
2
, x
1
/x
2
) = (y
1
, y
2
).
La transformación inversa de g es
x
1
=
y
1
y
2
1 + y
2
, x
2
=
y
1
1 + y
2
(30)
y se obtiene resolviendo un sistema de dos ecuaciones en las variables x
1
y x
2
</p>
<pre class="example">


</pre>
<p>

x
1
</p>
<ul class="org-ul">
<li>x</li>
</ul>
<p>
2
= y
1
x
1
/x
2
= y
2
\iff

x
1
</p>
<ul class="org-ul">
<li>x</li>
</ul>
<p>
2
= y
1
x
1
= y
2
x
2
\iff

(1 + y
2
)x
2
= y
1
x
1
= y
2
x
2
\iff
(
x
2
=
y
1
1+y
2
x
1
=
y
1
y
2
1+y
2
El Jacobiano de la transformación inversa J}
g<sup>-1</sup>
(y
1
, y
2
) = det
</p>

<p>

&part; x
i
&part; y
j

i,j

es
J
g<sup>-1</sup>
(y
1
, y
2
) =
&part; x
1
&part; y
1
&part; x
2
&part; y
2
−
&part; x
1
&part; y
2
&part; x
2
&part; y
1
=
</p>

<p>
y
2
1 + y
2

−y
1
(1 + y
2
)
2

−
</p>

<p>
y
1
(1 + y
2
)
2

1
1 + y
2

=
−y
1
y
2
(1 + y
2
)
3
−
y
1
(1 + y
2
)
3
= −}
y
1
(1 + y
2
)
(1 + y
2
)
3
= −}
y
1
(1 + y
2
)
2
. (31)
12
\hypertarget{pfd}
Substituyendo los resultados (30) y (31) en la fórmula (26) se obtiene:
f
Y_1
,Y
2
(y
1
, y
2
) = f
X_1
,X_2
</p>

<p>
y
1
y
2
1 + y
2
,
y
1
1 + y
2

</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">y</td>
</tr>
</tbody>
</table>
<p>
1
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<tbody>
<tr>
</tr>
</tbody>
</table>
<p>
(1 + y
2
)
2
. (32)
Por hipótesis,
f
X_1
,X_2
(x
1
, x
2
) = &lambda; e}
−{&lambda; x}
1
1\{x}
1
&gt; 0{\} &lambda; e
−{&lambda; x}
2
1\{x}
2
&gt; 0{\} = &lambda;
2
e
−{&lambda; (x}
1
+x
2
)
1\{x}
1
&gt; 0, x
2
&gt; 0{\} . (33)
De (32) y (33) se obtiene
f
Y_1
,Y
2
(y
1
, y
2
) = &lambda;}
2
e
−{&lambda; y}
1
y
1
(1 + y
2
)
2
1\{y}
1
&gt; 0, y
2
&gt; 0{\
=

&lambda;
2
y
1
e
−{&lambda; y}
1
1\{y}
1
&gt; 0{\

</p>

<p>
1
(1 + y
2
)
2
1\{y}
2
&gt; 0{\

. (34)
De (34) se deduce que las variables Y_1
e Y
2
son independientes.
</p>
</div>
</div>
<div id="outline-container-org564e121" class="outline-5">
<h5 id="org564e121">Nota Bene sobre la persistencia de la mala suerte. De (34) se deduce que la densidad}</h5>
<div class="outline-text-5" id="text-org564e121">
<p>
del cociente Y
2
= X_1
/X_2
de dos variables exponenciales independientes de igual intensidad
es de la forma
f
Y
2
(y
2
) =
1
(1 + y
2
)
2
1\{y}
2
&gt; 0{\} . (35)
En consecuencia, la variable Y}
2
tiene esperanza infinita. Se trata de un hecho notable que}
ofrece una explicación probabilística de un fenómeno conocido por cualquiera que haya entrado
en una fila de espera denominado la persistencia de la mala suerte}
1
¿Por qué? Supongamos que la variable X_1
representa el tiempo de espera para ser atendi
dos en la fila elegida (a la que llamaremos la fila 1) y que X_2
representa el tiempo de espera
en otra fila que estamos observando mientras esperamos ser atendidos (a la que llamaremos
la fila 2). El cociente X_1
/X_2
representa la proporción del tie mpo esperado en la fila 1 en en
relación al tiempo de espera en fila 2. Por ejemplo, X_1
/X_2
&ge; 3 significa esperamos por lo}
menos el triple del tiempo que hubiésemos esperado en la otra fila.
Integrando (35) se deduce que
\mathbb{P}(Y}
2
&le; y
2
) =
Z
y
2
0
1
(1 + y)
2
dy = 1 −
1
1 + y
2
=
y
2
1 + y
2
, y
2
&ge; 0}
Equivalentemente,
\mathbb{P}(Y}
2
&gt; y
2
) =
1
1 + y
2
, y
2
&ge; 0}
En particular, la probabilidad de que tengamos que esp
er ar por lo menos el triple del tiempo
que hubiésemos esperado en la otra fila es 1 / 4. Aunque de acuerdo con este modelo, en
promedio, la mitad de las veces esperamos menos tiempo que en la otra fila, en la práctica, el
fenómeno de la mala suerte se ve sobredimensionado porque no le prestamos atención a los
tiempos cortos de espera.
1
Basta elegir una fila en las múltiples cajas de un supermercado para sufrir este fenómeno y observar que
en la fila elegida el tiempo de espera es el doble o el triple que el tiempo de espera en las otras filas.
13
\hypertarget{pfe}
Para percibir qué significa el resultado E[X_1
/X_2
] = +{&infin; basta simular algunos valores de
la variable X_1
/X_2
. Por ejemplo, en 10 simulaciones obtuvimos la siguiente muestra:
1.2562, 0.8942, 0.9534, 0.3596, 29.3658, 1.2641, 3.3443, 0.3452, 13.5228, 7.1701.
El lector puede extraer sus propias conclusiones.
</p>
</div>
</div>
<div id="outline-container-orgd498638" class="outline-5">
<h5 id="orgd498638">Ejemplo 2.11</h5>
<div class="outline-text-5" id="text-orgd498638">
<p>
(Gammas y Betas). Sean X}
1
y X_2
variables aleatorias independientes con
distribuciones &Gamma;(&nu;
1
, &lambda;) y &Gamma;(&nu;
2
, &lambda;). Vamos a hallar la densidad conjunta de (Y}
1
, Y
2
) donde
Y_1
= X_1
</p>
<ul class="org-ul">
<li>X_2</li>
</ul>
<p>
, e Y
2
=
X_1
X_1
</p>
<ul class="org-ul">
<li>X_2</li>
</ul>
<p>
.
Para ello consideramos la transformación
g (x
1
, x
2
) =
</p>

<p>
x
1
</p>
<ul class="org-ul">
<li>x</li>
</ul>
<p>
2
,
x
1
x
1
</p>
<ul class="org-ul">
<li>x</li>
</ul>
<p>
2

= (y
1
, y
2
).
La transformación inversa de g es
x
1
= y
1
y
2
, x
2
= y
1
(1 −y}
2
). (36)
El Jacobiano de la transformación inversa es
J
g<sup>-1</sup>
(y
1
, y
2
) =
&part; x
1
&part; y
1
&part; x
2
&part; y
2
−
&part; x
1
&part; y
2
&part; x
2
&part; y
1
= y
2
(−y}
1
) −y}
1
(1 −y}
2
) = −y}
1
(37)
Substituyendo los resultados (36) y (37) en la fórmula (26) se obtiene:
f
Y_1
,Y
2
(y
1
, y
2
) = f
X_1
,X_2
(y
1
y
2
, y
1
(1 −y}
2
)) |y}
1
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">. (38)}</td>
</tr>
</tbody>
</table>
<p>
Por hipótesis,
f
X_1
,X_2
(x
1
, x
2
) = =
&lambda;
&nu;
1
x
&nu;
1
−{1}
1
e
−{&lambda; x}
1
&Gamma;(&nu;
1
)
1\{x}
1
&gt; 0{\
&lambda;
&nu;
2
x
&nu;
2
−{1}
2
e
−{&lambda; x}
2
&Gamma;(&nu;
2
)
1\{x}
2
&gt; 0{\
=
&lambda;
&nu;
1
</p>
<ul class="org-ul">
<li>&nu;</li>
</ul>
<p>
2
x
&nu;
1
−{1}
1
x
&nu;
2
−{1}
2
e
−{&lambda; (x}
1
+x
2
)
&Gamma;(&nu;
1
)&Gamma;(&nu;
2
)
1\{x}
1
&gt; 0, x
2
&gt; 0{\} . (39)
De (38) y (39) se obtiene
f
Y_1
,Y
2
(y
1
, y
2
) =
&lambda;
&nu;
1
</p>
<ul class="org-ul">
<li>&nu;</li>
</ul>
<p>
2
(y
1
y
2
)
&nu;
1
−{1}
(y
1
(1 −y}
2
))
&nu;
2
−{1}
e
−{&lambda; y}
1
&Gamma;(&nu;
1
)&Gamma;(&nu;
2
)
1\{y}
1
y
2
&gt; 0, y
1
(1 −y}
2
) &gt; 0{\}|y}
1
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<tbody>
<tr>
</tr>
</tbody>
</table>
<p>
=
&lambda;
&nu;
1
</p>
<ul class="org-ul">
<li>&nu;</li>
</ul>
<p>
2
y
&nu;
1
</p>
<ul class="org-ul">
<li>&nu;</li>
</ul>
<p>
2
−{1}
1
e
−{&lambda; y}
1
&Gamma;(&nu;
1
</p>
<ul class="org-ul">
<li>&nu;}</li>
</ul>
<p>
2
)
1\{y}
1
&gt; 0{\
!
&times;
&Gamma;(&nu;
1
</p>
<ul class="org-ul">
<li>&nu;}</li>
</ul>
<p>
2
)y
&nu;
1
−{1}
2
(1 −y}
2
)
&nu;
2
−{1}
&Gamma;(&nu;
1
)&Gamma;(&nu;
2
)
1\{0 &lt; y
2
&lt; 1{\
!
. (40)
Por lo tanto, Y_1
e Y
2
son independientes y sus distribuciones son Y_1
&sim; &Gamma;(&nu; }
1
</p>
<ul class="org-ul">
<li>&nu;}</li>
</ul>
<p>
2
, &lambda;), Y
2
&sim;
&beta; (&nu;
1
, &nu;
2
):
f
Y_1
(y
1
) =
&lambda;
&nu;
1
</p>
<ul class="org-ul">
<li>&nu;</li>
</ul>
<p>
2
&Gamma;(&nu;
1
</p>
<ul class="org-ul">
<li>&nu;}</li>
</ul>
<p>
2
)
y
&nu;
1
</p>
<ul class="org-ul">
<li>&nu;</li>
</ul>
<p>
2
−{1}
1
e
−{&lambda; y}
1
1\{y}
1
&gt; 0{\},
f
Y
2
(y
2
) =
&Gamma;(&nu;
1
</p>
<ul class="org-ul">
<li>&nu;}</li>
</ul>
<p>
2
)
&Gamma;(&nu;
1
)&Gamma;(&nu;
2
)
y
&nu;
1
−{1}
2
(1 −y}
2
)
&nu;
2
−{1}
1\{0 &lt; y
2
&lt; 1{\} .
14
\hypertarget{pff}
</p>
</div>
</div>
<div id="outline-container-org707f732" class="outline-5">
<h5 id="org707f732">Nota Bene</h5>
<div class="outline-text-5" id="text-org707f732">
<p>
Algunos autores utilizan (y promueven!) el méto do del Jacobiano como una}
herramienta para obtener la densidad de variables aleatorias de la forma Y_1
= g
1
(X_1
, X_2
).
Hacen lo siguiente: 1. Introducen una variable auxiliar de la forma Y
2
= g
2
(X_1
, X_2
) para
obtener un cambio de variables (g
1
, g
2
) : \Re
2
&rarr; \Re
2
. 2. Utilizan la fórmula del Jacobiano (26)
para obtener la densidad conjunta de (Y_1
, Y
2
) a partir de la densidad conjunta de (X_1
, X_2
).
</p>
<ol class="org-ol">
<li>Obtienen la densidad de Y_1</li>
</ol>
<p>
marginando (i.e., integrando la densidad conjunta de (Y_1
, Y
2
)
con respecto de y
2
). Por ejemplo,
Suma: (X}
1
, X_2
) &rarr; (X_1
</p>
<ul class="org-ul">
<li>X_2</li>
</ul>
<p>
, X_2
) =: (Y_1
, Y
2
). En tal caso, (x
1
, x
2
) = (y
1
− y
2
, y
2
) y el
Jacobiano tiene la forma J(y
1
, y
2
) =
&part; x
1
&part; y
1
&part; x
2
&part; y
2
−
&part; x
1
&part; y
2
&part; x
2
&part; y
1
= 1. De donde se obtiene
f
Y_1
(y
1
) =
Z
R
f
X_1
,X_2
(y
1
− y
2
, y
2
)dy}
2
.
Producto: (X}
1
, X_2
) &rarr; (X_1
X_2
, X_1
) =: (Y_1
, Y
2
). En tal caso, (x
1
, x
2
) = (y
2
, y
1
/y
2
) y el
Jacobiano tiene la forma J(y
1
, y
2
) =
&part; x
1
&part; y
1
&part; x
2
&part; y
2
−
&part; x
1
&part; y
2
&part; x
2
&part; y
1
= −}
1
y
2
. De donde se obtiene
f
Y_1
(y
1
) =
Z
R
f
X_1
,X_2
(y
2
, y
1
/y
2
) | y
2
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<tbody>
<tr>
</tr>
</tbody>
</table>
<p>
−{1}
dy
2
.
Cociente: (X}
1
, X_2
) &rarr; (X_1
/X_2
, X_2
) =: (Y_1
, Y
2
). En tal caso, (x
1
, x
2
) = (y
1
y
2
, y
2
) y el
Jacobiano tiene la forma J(y
1
, y
2
) =
&part; x
1
&part; y
1
&part; x
2
&part; y
2
−
&part; x
1
&part; y
2
&part; x
2
&part; y
1
= y
2
. De donde se obtiene
f
Y_1
(y
1
) =
Z
R
f
X_1
,X_2
(y
1
y
2
, y
2
) | y
2
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{dy}</td>
</tr>
</tbody>
</table>
<p>
2
.
</p>
</div>
</div>
<div id="outline-container-orgfc7ee1e" class="outline-5">
<h5 id="orgfc7ee1e">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-orgfc7ee1e">
<ol class="org-ol">
<li>[James p.97] Si X, Y, Z tienen densidad conjunta}</li>
</ol>
<p>
f
_{X,Y},Z
(x, y, z) =
6
(1 + x + y + z)
4
1\{x &gt; 0, y &gt; 0, z &gt; 0}\}.
Hallar la densidad de la variable aleatoria W = X +Y +{Z de dos maneras diferentes (método
básico y método del Jacobiano)
</p>
</div>
</div>
</div>
<div id="outline-container-orgc638d93" class="outline-3">
<h3 id="orgc638d93">Funciones k a 1</h3>
<div class="outline-text-3" id="text-orgc638d93">
<p>
Si la función \(g : \Re n \rightarrow \Re n\) no es 1 a 1 también
podemos utilizar el método del jacobiano para determinar la
distribución de \(Y = g(X)\). Basta con que g sea 1 a 1 cuando se la
restringe a una de \(k\) regiones abiertas disjuntas cuya unión contiene
al valor de \(X\) con probabilidad 1.
</p>

<p>
Supongamos que G, G}
1
, &hellip; , G
k
son regiones abiertas de R}
n
tales que G}
1
, &hellip; G
k
son dis
juntas dos a dos y que
P
X &isin;}
k
[
{&ell;=1}
G
&ell;
!
= 1.
</p>

<p>
Supongamos además que la restricción de g a G}
&ell;
, g | G}
&ell;
, es una correspondencia 1 a 1 entre
G
&ell;
y G, para todo &ell; = 1, &hellip; , k y que la función inversa de g | G}
&ell;
, denotada por h
(&ell;)
, satisface
todas las condiciones de la función h del Teorema 2.6.
15
</p>
</div>
<div id="outline-container-org57bcac0" class="outline-5">
<h5 id="org57bcac0">Teorema 2.12.</h5>
<div class="outline-text-5" id="text-org57bcac0">
<p>
Bajo las condiciones enunciadas más arriba, si X tiene densidad f_X(x),
entonces Y tiene densidad
f_Y(y) =
k
X
{&ell;=1}
f
X
(h
(&ell;)
(y))|{J
h
(&ell;)
(y)|{1}\{y &isin; G}\. (41)
</p>
</div>
</div>
<div id="outline-container-org47fb684" class="outline-5">
<h5 id="org47fb684">Demostración</h5>
<div class="outline-text-5" id="text-org47fb684">
<p>
Sea B &sub; G,
\mathbb{P}(Y &isin; B) = \mathbb{P}(g(X) &isin; B) =
k
X
{&ell;=1}
\mathbb{P}(g(X) &isin; B, X &isin; G
&ell;
) =
k
X
{&ell;=1}
\mathbb{P}(X &isin; h
(&ell;)
(B))
=
k
X
{&ell;=1}
Z
h
(&ell;)
(B)
f_X(x)dx = (cambio de variables en la integral)
=
k
X
{&ell;=1}
Z
B
f
X
(h
(&ell;)
(y))|{J
h
(&ell;)
(y)|{dy =
Z
B
k
X
{&ell;=1}
f
X
(h
(&ell;)
(y))|{J
h
(&ell;)
(y) |
!
dy.
</p>
</div>
</div>
<div id="outline-container-org8a66557" class="outline-5">
<h5 id="org8a66557">Ejemplo 2.13.</h5>
<div class="outline-text-5" id="text-org8a66557">
<p>
Sean X e Y dos variables aleatorias independientes con distribución común}
N(0, 1). Mostrar que Z = X}
2
+Y
2
y W = X/Y son independientes y hallar sus distribuciones.
Solución. La función g : \Re}
2
&rarr; \Re
2
, definida por g(x, y) = (x
2
</p>
<ul class="org-ul">
<li>y</li>
</ul>
<p>
2
, x/y) = (z, w), es 2 a 1.
Sean G = \(z, w) : z &gt; 0{\, G}
1
= \(x, y) : y &gt; 0{\, G}
2
= \(x, y) : y &lt; 0{\}. Entonces,
las restricciones g | G}
1
y g | G}
2
son correspondencias 1 a 1 entre las regiones abiertas G}
i
y G,
i = 1, 2, y \mathbb{P}((X,Y) &isin; G
1
&cup; G}
2
) = 1.
Tenemos que calcular los jacobianos de las funciones inversas h
(1)
y h
(2)
en G}. Para
ello calculamos los jacobianos de las restric ciones g | G}
1
y g | G}
2
, que son los re cíprocos de los
jacobianos de las inversas, y substituimos el val
or (x, y) por el valor h
(1)
(z, w) o h
(2)
(z, w).
Tenemos
J
1
(z, w) =
</p>

<p>




2x 2y
1
y
−
x
y
2





−{1}
=
</p>

<p>
−{2}
</p>

<p>
x
2
y
2
</p>
<ul class="org-ul">
<li>1</li>
</ul>
<p>

−{1}
= −}
1
2(w
2
</p>
<ul class="org-ul">
<li>1)</li>
</ul>
<p>
y
J
2
(z, w) = −}
1
2(w
2
</p>
<ul class="org-ul">
<li>1)</li>
</ul>
<p>
.
Por lo tanto, la densidad de (Z, W) es
f
Z,W
(z, w) =

f (h
(1)
(z, w)) + f(h
(2)
(z, w))

1
2(w
2
</p>
<ul class="org-ul">
<li>1)</li>
</ul>
<p>
1\(z, w) &isin; G\}.
Como
f (x, y) =}
1
2 &pi;
e
−(x}
2
+y
2
) / 2
=
1
2 &pi;
e
−{z/{2
,
tenemos
f
Z,W
(z, w) = 2
</p>

<p>
1
2 &pi;
e
−{z/{2

1
2(w
2
</p>
<ul class="org-ul">
<li>1)</li>
</ul>
<p>
1\{z &gt; 0, w &isin; \Re\} =
</p>

<p>
1
2
e
−{z/{2
1\{z &gt; 0}\}

1
&pi; (w
2
</p>
<ul class="org-ul">
<li>1)</li>
</ul>
<p>
.
Como la densidad conjunta es el producto de dos densidades, concluimos que Z y W son
independientes, Z &sim; Exp(1 / 2) y W &sim; Cauchy.
16
</p>
</div>
</div>
<div id="outline-container-orgc1852fd" class="outline-5">
<h5 id="orgc1852fd">Ejemplo 2.14</h5>
<div class="outline-text-5" id="text-orgc1852fd">
<p>
(Mínimo y máximo). Sean X}
1
, X_2
dos variables aleatorias con densidad con
junta f
X_1
,X_2
(x
1
, x
2
). Hallar la densidad conjunta de U = mín(X_1
, X_2
) y V = máx(X_1
, X_2
).
La función g(x
1
, x
2
) = (mín(x
1
, x
2
), máx(x
1
, x
2
)), es 2 a 1.
Sean G = \(u, v) : u &lt; v{\}, G}
1
= \(x
1
, x
2
) : x
1
&lt; x
2
\} y G
2
= \(x
1
, x
2
) : x
2
&lt; x
1
\}.
Las restricciones g | G}
1
(x
1
, x
2
) = (x
1
, x
2
) y g | G}
2
(x
1
, x
2
) = (x
2
, x
1
) son correspondencias 1
a 1 entre las regiones abiertas G}
i
y G, i = 1, 2; \mathbb{P}((X,Y) &isin; G
1
&cup; G}
2
) = 1 y los jacobianos de
las funciones inversas h
(1)
y h
(2)
en G valen 1 y −}1, respectivamente. Usando la fórmula (41)
obtenemos la densidad conjunta de (U, V):
f
U,V
(u, v) = (f
X_1
,X_2
(u, v) + f
X_1
,X_2
(v, u)) 1\{u &lt; v\}.}
</p>
</div>
</div>
<div id="outline-container-org218fd01" class="outline-5">
<h5 id="org218fd01">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-org218fd01">
<ol class="org-ol">
<li>La distribución de (X,Y) es uniforme sobre el recinto sombreado}</li>
</ol>
<p>
−{1}
0
−{1}
1
1
Hallar la densidad conjunta de (U, V) = (| 2{Y |, |  3{X |).
</p>
<ol class="org-ol">
<li>[James p.99] Sean X}</li>
</ol>
<p>
1
, &hellip; , X
n
variables aleatorias independientes e idénticamente dis
tribuidas, con densidad común f . Mostrar que la densidad conjunta de
U = mín}
1{\leqi\leqn}
X
i
y V = máx
1{\leqi\leqn}
X
i
es
f
U,V
(u, v) = n(n −} 1)[F(v) − F (u)]
n{−{2
f (u) f (v)1{\}u &lt; v{\}.
(Sugerencia. Primero hallar \mathbb{P}(u &lt; U, V &le; v). Después, calcular las derivadas parciales
cruzadas de la distribución conjunta.)
</p>
<ol class="org-ol">
<li>[James p.99] Sean X}</li>
</ol>
<p>
1
, &hellip; , X
n
variables aleatorias independientes e idénticamente dis
tribuidas, con distribución uniforme sobre el intervalo [0, 1] . Sean
U = mín}
1{\leqi\leqn}
X
i
y V = máx
1{\leqi\leqn}
X
i
17
(a) Mostrar que la densidad conjunta de (U, V) es
f
U,V
(u, v) = n(n −} 1)(v − u)
n{−{2
1\{0 &le; u &lt; v &le; 1\}.
(b) Mostrar que la densidad de W = V − U es
f
W
(w) = n(n − 1)w
n{−{2
(1 −w) 1{\}0 &le; w &le; 1{\}.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgd758021" class="outline-2">
<h2 id="orgd758021">Mínimo y máximo de dos exponenciales independientes</h2>
<div class="outline-text-2" id="text-orgd758021">
</div>
<div id="outline-container-org41207b4" class="outline-5">
<h5 id="org41207b4">Teorema 3.1</h5>
<div class="outline-text-5" id="text-org41207b4">
<p>
Sean \(X_1\) y \(X_2\) dos variables aleatorias independientes con
distribuciones exponenciales de intensidades &lambda; 1 y &lambda; 2
respectivamente. Si U = mín(X_1, X_2), V = máx(X_1, X_2), W = V − U y
J = 1{\}U = X_1\} + 2{1} \{U = X} 2 \, entonces
</p>

<p>
(a) U &sim; Exp (&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
).
(b) \mathbb{P}(J = i) = &lambda;}
i
(&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
)
−{1}
, i = 1, 2.
(c) U y J son independientes.
(d) f
W
(w) = \mathbb{P}(J = 1)f
X_2
(w) + \mathbb{P}(J = 2)f
X_1
(w).
(e) U y W son independientes.
</p>
</div>
</div>
<div id="outline-container-org2a5f36d" class="outline-5">
<h5 id="org2a5f36d">Demostración</h5>
<div class="outline-text-5" id="text-org2a5f36d">
<p>
Primero observamos que para cada u &gt; 0 el evento \{J = 1, U &gt; u{\} equivale}
al evento \{X}
2
&ge; X_1
&gt; u{\. En consecuencia,
\mathbb{P}(J = 1, U &gt; u) =}
Z
&infin;
u
&lambda;
1
e
−{&lambda; x}
1
</p>

<p>
Z
&infin;
x
1
&lambda;
2
e
− &lambda;
2
x
2
dx
2

dx
1
=
Z
&infin;
u
&lambda;
1
e
−{&lambda; x}
1
e
− &lambda;
2
x
1
dx
1
=
&lambda;
1
&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
Z
&infin;
u
(&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
)e
−(&lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)x
1
dx
1
=
</p>

<p>
&lambda;
1
&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2

e
−(&lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)u
. (42)
</p>

<p>
De (42) se deducen (a), (b) y (c).  Si \(g : \(u, v) : 0 < u < v{\}
\rightarrow \(u, w) : u > 0, w > 0{\}\) es la función definida por \(g (
u, v) = (u, v − u)\), tenemos que (U, W) = g (U, V). La función g es
biyectiva y su inversa} \(h (u, w) = (u, u + w)\) tiene jacobiano
idénticamente igual a 1. Aplicar el método del jacobiano} del
Corolario 2.7 obtenemos:
</p>

<p>
f
U,W
(u, w) = f
U,V
(u, u + w). (43)
Por el Ejemplo 2.14
sabemos que la densidad conjunta de U y V es
f
U,V
(u, v) = &lambda;}
1
&lambda;
2

e
−(&lambda; }
1
u{+}&lambda;
2
v)
</p>
<ul class="org-ul">
<li>e</li>
</ul>
<p>
−(&lambda; }
1
v{+}&lambda;
2
u)

1\{0 &lt; u &lt; v} \. (44)
18
Combinando (43) y (44) obtenemos:
f
V,W
(u, w) = &lambda;}
1
&lambda;
2

e
−(&lambda; }
1
u{+}&lambda;
2
(u+w))
</p>
<ul class="org-ul">
<li>e</li>
</ul>
<p>
−(&lambda; }
1
(u+w)+ &lambda;
2
u)

1\{u &gt; 0, w &gt; 0}\}
= &lambda;}
1
&lambda;
2
e
−(&lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)u

e
− &lambda;
2
w
</p>
<ul class="org-ul">
<li>e</li>
</ul>
<p>
− &lambda;
1
w

1\{u &gt; 0, w &gt; 0}\}
= (&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
)e
−(&lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)u
1\{u &gt; 0}\}
&times;
</p>

<p>
&lambda;
1
&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
&lambda;
2
e
− &lambda;
2
w
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
&lambda;
2
&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
&lambda;
1
e
− &lambda;
1
w

1\{w &gt; 0} \. (45)
De (45) se deducen (d) y (e).
</p>
</div>
</div>
<div id="outline-container-org84b01e4" class="outline-5">
<h5 id="org84b01e4">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-org84b01e4">
<ol class="org-ol">
<li>Un avión tiene dos motores cada uno de los cuales funciona durante
un tiempo exponencial de media 10 horas independientemente del
otro. El avión se mantiene volando mientras funcione alguno de sus
motores. Calcular la probabilidad de que el avión se mantenga
volando durante más de cinco horas después de que dejó de funcionar
un motor.</li>
<li>Una cueva será iluminada por dos lámparas L1 y L2 cuyas duraciones
(en horas) son independientes y tienen distribuciones exponenciales
  de medias 8 y 10, respectivamente. Sabiendo que desde que se apagó
  una lámpara la cueva se mantuvo iluminada durante más de una hora
  calcular la probabilidad de que se haya apagado primero la lámpara
  L2.</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgee935a0" class="outline-2">
<h2 id="orgee935a0">Funciones regulares e independencia</h2>
<div class="outline-text-2" id="text-orgee935a0">
</div>
<div id="outline-container-org0f5ba3e" class="outline-5">
<h5 id="org0f5ba3e">Definición 4.1</h5>
<div class="outline-text-5" id="text-org0f5ba3e">
<p>
Una función g se dice regular si existen números &ctdot; &lt; a
−{1}
&lt; a
0
&lt; a
1
&lt; &ctdot;,
con a
i
&rarr; &infin; y a}
−i
&rarr; −&infin;, tales que g es continua y monótona so
bre cada intervalo (a
i
, a
{i+1}
).
</p>
</div>
</div>
<div id="outline-container-orgac86654" class="outline-5">
<h5 id="orgac86654">Ejemplo 4.2</h5>
<div class="outline-text-5" id="text-orgac86654">
<p>
La función sen \(x\) es regular; todos los polinomios son funciones
regulares. Un ejemplo de una función que no es regular es
\(\textbf{1}\{x \in Q\}\).
</p>
</div>
</div>

<div id="outline-container-org9fbce06" class="outline-5">
<h5 id="org9fbce06">Teorema 4.3</h5>
<div class="outline-text-5" id="text-org9fbce06">
<p>
Sean \(X_1, \dots , X_n\) variables aleatorias independientes. Si g
1
, &hellip; , g
n
son funciones regulares, entonces g
1
(X_1
), &hellip; , g
n
(X_n
) son variables aleatorias independientes.
</p>
</div>
</div>
<div id="outline-container-orgafd3f93" class="outline-5">
<h5 id="orgafd3f93">Demostración</h5>
<div class="outline-text-5" id="text-orgafd3f93">
<p>
Para simplificar la prueba supondremos que n = 2. De la regularidad de}
las funciones g
1
y g
2
se deduce que para todo y &isin; \Re podemos escr
ibir
A_1
(y) := \{x : g
1
(x) &le; y\} = &cup;}
i
A_1,i}
(y) y A_2
(y) := \{x : g
2
(x) &le; y\} = &cup;}
i
A_2,i}
(y),
como uniones de intervalos disjuntos dos a dos. Por lo tanto,
\mathbb{P}(g}
1
(X_1
) &le; y}
1
, g
1
(X_2
) &le; y}
2
) =
X
i
X
j
\mathbb{P}(X}
1
&isin; A_1,i}
(y
1
), X}
2
&isin; A_2,i}
(y
2
))
=
X
i
X
j
\mathbb{P}(X}
1
&isin; A_1,i}
(y
1
))\mathbb{P}(X_2
&isin; A_2,i}
(y
2
))
=
X
i
\mathbb{P}(X}
1
&isin; A_1,i}
(y
1
))
X
j
\mathbb{P}(X}
2
&isin; A_2,i}
(y
2
))
= \mathbb{P}(g
1
(X_1
) &le; y}
1
)\mathbb{P}(g
2
(X_2
) &le; y}
2
).
19
En rigor de verdad, vale un resultado mucho más general.
</p>
</div>
</div>
<div id="outline-container-orgcfd19ae" class="outline-5">
<h5 id="orgcfd19ae">Teorema 4.4</h5>
<div class="outline-text-5" id="text-orgcfd19ae">
<p>
Si para 1 &le; i &le; n, 1 &le; j &le; m
i
, X
i,j
son independientes y f
i
</p>
<pre class="example">
\Re

</pre>
<p>
m
i
&rarr; \Re son
medibles entonces f
i
(X
i,{1}
, &hellip; , X
i,m
i
) son independientes.
</p>
</div>
</div>
<div id="outline-container-org76e483d" class="outline-5">
<h5 id="org76e483d">Demostración</h5>
<div class="outline-text-5" id="text-org76e483d">
<p>
Durrett(1996), p.25-27.
</p>

<p>
Un caso concreto que usaremos permanentemente al estudiar sumas es el siguiente: si
X_1
, &hellip; , X
n
son independientes, entonces X = X_1
</p>
<ul class="org-ul">
<li>&ctdot; + X</li>
</ul>
<p>
n{−{1
y X
n
son independientes.
</p>
</div>
</div>
<div id="outline-container-org848938d" class="outline-5">
<h5 id="org848938d">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-org848938d">
<ol class="org-ol">
<li>(Fragmentaciones aleatorias.) Si \(U_1, \dots , U_n\) son
independientes con distribución común</li>
</ol>
<p>
U(0, 1), entonces
</p>

<p>
−{log}
n
Y
{i=1}
U
i
&sim; &Gamma;(n, 1).
</p>

<ol class="org-ol">
<li>Una varilla de 1 metro de longitud es sometida a un proceso de
fragmentación aleatoria. En la primera fase se elige un punto al
azar de la misma y se la divide por el punto elegido en dos
varillas de longitudes L</li>
</ol>

<p>
1
y L
2
. En la segunda fase se elige un punto al azar de la varilla
de longitud L
1
y se la divide por el punto elegido en dos varillas de longitudes L}
1, 1
y L}
1, 2
.
Calcular la probabilidad de que L}
1, 1
sea mayor que 25 centímetros.
</p>
</div>
</div>
</div>
<div id="outline-container-orgfaed797" class="outline-2">
<h2 id="orgfaed797">Bibliografía consultada</h2>
<div class="outline-text-2" id="text-orgfaed797">
<p>
Para redactar estas notas se consultaron los siguientes libros:
</p>
<ol class="org-ol">
<li>Durrett R.:Probability. Theory and Examples. Duxbury Press,
Belmont. (1996).</li>
<li>Feller, W.: An introduction to Probability Theory and Its
Applications. Vol. 2. John Wiley &amp; Sons, New York. (1971).</li>
<li>James, B. R.: probabilidade: um curso em nível intermediario. IMPA,
Rio de Janeiro. (2002).</li>
<li>Meester, R.: A Natural Introduction to Probability
Theory. Birkhauser, Berlin. (2008).</li>
<li>Meyer, P. L.: Introductory Probability and Statistical
Applications. Addison-Wesley, Massachusetts. (1972).</li>
<li>Ross, S.: Introduction to Probability Models. Academic Press, San
Diego. (2007)</li>
<li>Soong, T. T.: Fundamentals of Probability and Statistics for
Engineers. John Wiley &amp; Sons Ltd. (2004).</li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
Last update: 2020-04-02 19:06
</div>
</body>
</html>
