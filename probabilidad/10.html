<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-06-18 mar 00:54 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Estimaci√≥n por intervalo</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/res/org.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2019 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "true" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "true" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "%AUTONUMBER"},
               MultLineWidth: "%MULTLINEWIDTH",
               TagSide: "right",
               TagIndent: "%TAGINDENT"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<div id="outline-container-orgb7cffbc" class="outline-2">
<h2 id="orgb7cffbc">Estimaci√≥n por intervalo</h2>
<div class="outline-text-2" id="text-orgb7cffbc">
<p>
En lo que sigue consideramos el problema de estimaci√≥n de par√°metros utilizando
inter valos de confianza. Consideramos una muestra aleatoria \(X = (X_1 , \dots ,
X_n)\) de la variable aleatoria X cuya funci√≥n de distribuci√≥n \(F (x) :=
\mathbb{P}(X \leq x)\), pertenece a la familia param√©trica de distribuciones
(distinguibles) \(F = \{F \theta: \theta \in \Theta\}, \Theta \subset \Re\). La
idea b√°sica es la siguiente: aunque no podamos determinar exactamente el valor
de \(\theta\) podemos tratar de construir un in tervalo ale atorio \([\theta^‚àí ,
\theta^+]\) tal que con una probabilidad bastante alta, sea capaz de <i>capturar</i>
el valor desconocido \(\theta\).
</p>
</div>
<div id="outline-container-orgae13af6" class="outline-5">
<h5 id="orgae13af6">Definici√≥n 1.1 (Intervalo de confianza)</h5>
<div class="outline-text-5" id="text-orgae13af6">
<p>
Un intervalo de confianza para \(\theta\) de nivel \(\beta\) es un intervalo
aleatorio, \(I(\textbf{X})\), que depende de la muestra aleatoria \(X\), tal que
</p>

<p>
\(\mathbb{P}_{\theta}(\theta \in I(\mathbf{X}))=\beta\)
</p>

<p>
para todo \(\theta \in \Theta\).
</p>
</div>
</div>
<div id="outline-container-org679be40" class="outline-5">
<h5 id="org679be40">Definici√≥n 1.2 (Cotas de confianza)</h5>
<div class="outline-text-5" id="text-org679be40">
<p>
Una cota inferior de confianza para \(\theta\), de nivel \(\beta\), basada en la
muestra aleatoria X, es una variable aleatoria \(\theta_1 (\textbf{X})\) tal que
</p>

<p>
\(\mathbb{P}_{\theta}\left(\theta_{1}(\mathbf{X}) \leq \theta\right)=\beta\)
</p>

<p>
para todo \(\theta \in \Theta\).
</p>

<p>
Una cota superior de confianza para \(\theta\), de nivel \(\beta\), basada en la
muestra aleatoria X, es una variable aleatoria &theta;<sub>2</sub> (X) tal que
</p>

<p>
\(\mathbb{P}_{\theta}\left(\theta \leq \theta_{2}(\mathbf{X})\right)=\beta\)
</p>

<p>
para todo &theta; &isin; &Theta;.
</p>
</div>
</div>
<div id="outline-container-orgf09ec15" class="outline-5">
<h5 id="orgf09ec15">Nota Bene</h5>
<div class="outline-text-5" id="text-orgf09ec15">
<p>
En el caso discreto no siempre se pueden obtener las igualdades (1), (2) o (3).
Para evitar este tipo de problemas se suele definir un intervalo mediante la
condici√≥n m√°s laxa \(\mathbb{P}_{\theta}(\theta \in I(\mathbf{X})) \geq \beta,
\forall \theta\). En este caso el \(\min _{\theta} P_{\theta}(\theta \in
I(\mathbf{X}))\) se llama nivel de confianza.
</p>
</div>
</div>
<div id="outline-container-org2b5cfb7" class="outline-5">
<h5 id="org2b5cfb7">Observaci√≥n 1.3</h5>
<div class="outline-text-5" id="text-org2b5cfb7">
<p>
Sean \(\theta_1
(X)\) una cota inferior de confianza de nivel \(\beta_1 > 1/2\) y \(\theta_2
(X)\) una
cota superior de confianza de nivel \(\beta_2 > 1/2\), tales que
\(\mathbb{P}_{\theta}\left(\theta_{1}(\mathbf{X}) \leq \theta_{2}(\mathbf{X})\right)=1\).
Entonces,
\(I(\mathbf{X})=\left[\theta_{1}(\mathbf{X}), \theta_{2}(\mathbf{X})\right]\)
define un intervalo de confianza para \(\theta\) de nivel \(\beta = \beta_1+
\beta_2 ‚àí 1\). En efecto,
</p>

<p>
\(\begin{aligned} \mathbb{P}_{\theta}(\theta \in I(\mathbf{X}))
&=1-\mathbb{P}_{\theta}\left(\theta<\theta_{1}(\mathbf{X}) \text { o }
\theta>\theta_{2}(\mathbf{X})\right)
\\ &=1-\mathbb{P}_{\theta}\left(\theta<\theta_{1}(\mathbf{X})\right)-\mathbb{P}_{\theta}\left(\theta>\theta_{2}(\mathbf{X})\right)
\\ &=1-\left(1-\beta_{1}\right)-\left(1-\beta_{2}\right)=\beta_{1}+\beta_{2}-1
\end{aligned}\)
</p>

<p>
La identidad (4) muestra que la construcci√≥n de intervalos de confianza se
reduce a la construcci√≥n de cotas inferiores y superiores. M√°s precisamente, si
se quiere construir un intervalo de confianza de nivel \(\beta\), basta construir
una cota inferior de nivel \(\beta_{1}=(1+\beta) / 2\) y una cota superior de
nivel \(\beta_{2}=(1+\beta) / 2\)
</p>

<p>
Las ideas principales para construir intervalos de confianza est√°n contenidas en
el ejemplo siguiente.
</p>
</div>
</div>
<div id="outline-container-org2202855" class="outline-5">
<h5 id="org2202855">Ejemplo 1.4 (Media de la normal con varianza conocida)</h5>
<div class="outline-text-5" id="text-org2202855">
<p>
Sea X = (X<sub>1</sub>, &hellip; , X<sub>n</sub>) una muestra aleatoria de una variable aleatoria X
&sim; N(&mu;, &sigma;<sup>2</sup>), con varianza &sigma;<sup>2</sup> conocida. Para obtenerun intervalo
de confianza de nivel \(\beta\) para &mu;, consideramos el estimador de m√°xima
verosimilitud para \(\mu\)
</p>

<p>
\(\overline{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}\)
</p>

<p>
La distribuci√≥n de \overline{X} se obtiene utilizando los resultados conocidos
sobre sumas de normales independientes y de cambio de escala:
</p>

<p>
\(\overline{X} \sim \mathcal{N}\left(\mu, \frac{\sigma^{2}}{n}\right)\)
</p>

<p>
En consecuencia,
</p>

<p>
\(\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma} \sim \mathcal{N}(0,1)\)
</p>

<p>
Por lo tanto, para cada &mu; &isin; \Re vale que
</p>

<p>
\(\mathbb{P}_{\mu}\left(-z_{(1+\beta) / 2} \leq
\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma} \leq z_{(1+\beta) / 2}\right)=\beta\)
</p>

<p>
Despejando &mu; de las desigualdades dentro de la probabilidad, resulta que
</p>

<p>
\(\mathbb{P}_{\mu}\left(\overline{X}-\frac{\sigma}{\sqrt{n}} z_{(1+\beta) / 2}
\leq \mu \leq \overline{X}+\frac{\sigma}{\sqrt{n}} z_{(1+\beta) /
2}\right)=\beta\)
</p>

<p>
y por lo tanto el intervalo
</p>

<p>
\(I(\mathbf{X})=\left[\overline{X}-\frac{\sigma}{\sqrt{n}} z_{(1+\beta) / 2},
\overline{X}+\frac{\sigma}{\sqrt{n}} z_{(1+\beta) / 2}\right]\)
</p>

<p>
es un intervalo de confianza para &mu; de nivel \(\beta\).
</p>
</div>
</div>
<div id="outline-container-org565722d" class="outline-5">
<h5 id="org565722d">Nota Bene</h5>
<div class="outline-text-5" id="text-org565722d">
<p>
Las ideas principales para construir el intervalo de confianza contenidas en el
ejemplo anterior son las siguientes:
</p>
<ol class="org-ol">
<li>Obtener un estimador del par√°metro y caracterizar su distribuci√≥n.</li>
<li>Transformar el estimador de par√°metro hasta convertirlo en una variable
aleatoria cuya distribuci√≥n <i>conocida</i> que no dependa del par√°metro.</li>
<li>Poner cotas para el estimador transformado y despejar el par√°metro.</li>
</ol>
</div>
</div>
<div id="outline-container-orgc94c2a7" class="outline-3">
<h3 id="orgc94c2a7">El m√©todo del pivote</h3>
<div class="outline-text-3" id="text-orgc94c2a7">
<p>
Cuando se quieren construir intervalos de confianza para \(\theta\) lo m√°s natural
es comenzar la construcci√≥n apoy√°ndose en alg√∫n estimador puntual del par√°metro
\(\hat{\theta}(X)\) (cuya distribuci√≥n depende de \(\theta\)). Una t√©cnica general
para construir intervalos de confianza, llamada el m√©todo del pivote, consiste
en transformar el estimador \(\hat{\theta}(X)\) hasta convertirlo en una variable
aleatoria cuya distribuci√≥n sea <i>conocida</i> y no dependa de \(\theta\). Para que la
transformaci√≥n sea √∫til no debe depender de ning√∫n otro par√°metro desconocido.
</p>
</div>
<div id="outline-container-orgc81ddcd" class="outline-5">
<h5 id="orgc81ddcd">Definici√≥n 1.5 (Pivote)</h5>
<div class="outline-text-5" id="text-orgc81ddcd">
<p>
Una variable aleatoria de la forma \(Q(X, \theta)\) se dice una cantidad pivotal o
un pivote para el par√°metro \(\theta\) si su distribuci√≥n no depende de \(\theta\)
(ni de ning√∫n par√°metro desconocido, cuando hay varios par√°metros).
</p>
</div>
</div>
<div id="outline-container-org62a12e5" class="outline-5">
<h5 id="org62a12e5">Nota Bene</h5>
<div class="outline-text-5" id="text-org62a12e5">
<p>
Por definici√≥n, la distribuci√≥n del pivote \(Q(X, \theta)\) no depende de
\(\theta\). Para cada \(\alpha \in (0, 1)\) notaremos mediante \(q_\alpha\) el
cuantil-\(\alpha\) del pivote. Si el pivote tiene distribuci√≥n continua y su
funci√≥n de distribuci√≥n es estrictamente creciente, \(q_ \alpha\) es la √∫nica
soluci√≥n de la ecuaci√≥n
</p>

<p>
\(\mathbb{P}_{\theta}\left(Q(\mathbf{X}, \theta) \leq q_{\alpha}\right)=\alpha\)
</p>

<p>
M√©todo. Si se consigue construir un pivote Q(X, &theta;) para el par√°metro
&theta;, el problema de la construcci√≥n de intervalos de confianza, de nivel
\(\beta\), se descompone en dos partes:
</p>
<ol class="org-ol">
<li>Encontrar parejas de n√∫meros reales a &lt; b tales que \(\mathbb{P}_{\theta}(a
   \leq Q(\mathbf{X} ; \theta) \leq b)=\beta\). Por ejemplo,
\(a=q_{\frac{1-\beta}{2}}\) y \(b=q_{\frac{1+\beta}{2}}\).</li>
<li>Despejar el par√°metro &theta; de las desigualdades \(a \leq Q(\mathbf{X},
   \theta) \leq b\)</li>
</ol>

<p>
Si el pivote \(Q(X, \theta)\) es una funci√≥n mon√≥tona en &theta; se puede ver que
existen \(\theta_1 (X)\) y \(\theta_2 (X)\) tales que
</p>

<p>
\(a \leq Q(\mathbf{X} ; \theta) \leq b \Leftrightarrow \theta_{1}(\mathbf{X})
\leq \theta \leq \theta_{2}(\mathbf{X})\)
</p>

<p>
y entonces
</p>

<p>
\(\mathbb{P}_{\theta}\left(\theta_{1}(\mathbf{X}) \leq \theta \leq
\theta_{2}(\mathbf{X})\right)=\beta\)
</p>

<p>
de modo que \(I(\mathbf{X})=\left[\theta_{1}(\mathbf{X}),
\theta_{2}(\mathbf{X})\right]\) es un intervalo de confianza para \(\theta\) de
nivel \(\beta\).
</p>
</div>
</div>
<div id="outline-container-orgfef611e" class="outline-4">
<h4 id="orgfef611e">Pivotes decrecientes</h4>
<div class="outline-text-4" id="text-orgfef611e">
<p>
Sea \(Q(X, \theta)\) un pivote para \(\theta\) que goza de las siguientes
propiedades:
</p>
<ol class="org-ol">
<li>la funci√≥n de distribuci√≥n de \(Q(X, \theta)\) es continua y estrictamente
creciente;</li>
<li>para cada x, la funci√≥n \(Q(x, \theta)\) es continua y mon√≥tona decreciente en
la variable &theta;: \[\theta_{1}<\theta_{2} \Longrightarrow
   Q\left(\mathbf{x}, \theta_{1}\right)>Q\left(\mathbf{x}, \theta_{2}\right)\]</li>
</ol>

<p>
Sea \(\gamma \in (0, 1)\), arbitrario pero fijo y sea \(q_\gamma\) el
cuantil-\(\gamma\) del pivote \(Q(X, \theta)\). Para cada \(\mathbf{x}\), sea
\(\theta(x, \gamma)\) la √∫nica soluci√≥n de la ecuaci√≥n en \(\theta\)
</p>

<p>
\[Q(\mathbf{x}, \theta)=q_{\gamma}\]
</p>

<p>
Como el pivote \(Q(X, \theta)\) es decreciente en &theta; tenemos que
</p>

<p>
\[Q(\mathbf{X}, \theta) \leq q_{\gamma} \Longleftrightarrow \theta(\mathbf{X},
\gamma) \leq \theta\]
</p>

<p>
En consecuencia,
</p>

<p>
\[\mathbb{P}_{\theta}(\theta(\mathbf{X}, \gamma) \leq
\theta)=\mathbb{P}_{\theta}\left(Q(\mathbf{X}, \theta) \leq
q_{\gamma}\right)=\gamma, \quad \forall \theta \in \Theta\]
</p>

<p>
Por lo tanto, \(\theta(X, \gamma)\) es una cota inferior de confianza para
\(\theta\) de nivel \(\gamma\) y una cota superior de nivel \(1 ‚àí \gamma\).
</p>
</div>

<div id="outline-container-org0df7da6" class="outline-5">
<h5 id="org0df7da6">M√©todo</h5>
<div class="outline-text-5" id="text-org0df7da6">
<p>
Sea \(\beta \in (0, 1)\). Si se dispone de un pivote Q(X, &theta;) que satisface
las propiedades (i) y (ii) enunciadas m√°s arriba, entonces
</p>
<ul class="org-ul">
<li>la variable aleatoria, &theta;<sub>1</sub>(X), que se obtiene re solviendo la ecuaci√≥n
Q(X, &theta;) = q<sub>&beta;</sub> es una cota inferior de confianza para \(\theta\), de
nivel \(\beta\).</li>
<li>la variable aleatoria, &theta;<sub>2</sub>(X), que se obtiene resolviendo la ecuaci√≥n
Q(X, &theta;) = q<sub>1‚àí&beta;</sub> es una cota superior de confianza para \(\theta\), de
nivel \(\beta\).</li>
<li>el intervalo aleatorio I(X) = [&theta;<sub>1</sub>(X), &theta;<sub>2</sub>(X)] cuyos extremos son
las soluciones respectivas de las ecuaciones \(Q(\mathbf{X},
  \theta)=q_{\frac{1+\beta}{2}}\) y \(Q(\mathbf{X}, \theta)=q_{\frac{1-\beta}{2}}\)
, es un intervalo <i>bilateral</i> de confianza para \(\theta\), de nivel \(\beta\).</li>
</ul>
</div>
</div>
<div id="outline-container-org4b70321" class="outline-5">
<h5 id="org4b70321">Ejemplo 1.6 (Extremo superior de la distribuci√≥n uniforme)</h5>
<div class="outline-text-5" id="text-org4b70321">
<p>
Sea X = (X<sub>1</sub>, &hellip; , X<sub>n</sub>) una
muestra aleatoria de una variable aleatoria \(X \sim \mathcal{U} (0, \theta), \theta > 0\).
</p>

<p>
El estimador de m√°xima verosimilitud para \(\theta\) es \(X_{(n)} = m√°x(X_1 , \dots
, X_n)\) y tiene densidad de la forma
</p>

<p>
\[f(x)=\frac{n x^{n-1}}{\theta^{n}} \mathbf{1}\{0 \leq x \leq \theta\}\]
</p>

<p>
Como la distribuci√≥n de \(X_{(n)}\) depende de \(\theta\), \(X_{(n)}\) no es un pivote
para \(\theta\). Sin embargo, podemos liberarnos de \(\theta\) utilizando un cambio
de variables lineal de la forma \(Q=X_{(n)} / \theta\):
</p>

<p>
\[f_{Q}(q)=n q^{n-1} \mathbf{1}\{0 \leq q \leq 1\}\]
</p>

<p>
Por lo tanto,
</p>

<p>
\[Q(\mathbf{X}, \theta)=X_{(n)} / \theta\]
</p>

<p>
es un pivote para \(\theta\).
</p>

<p>
Figura 1: Forma t√≠pica del gr√°fico de la densidad del pivote \(Q(X, \theta)\).
</p>

<p>
Los cuantiles-&gamma; para Q se obtienen observando que
</p>

<p>
\[\gamma=\mathbb{P}\left(Q(\mathbf{X}, \theta) \leq
q_{\gamma}\right)=\int_{0}^{q_{\gamma}} f_{Q}(q) d q \Longleftrightarrow
q_{\gamma}=\gamma^{1 / n}\]
</p>

<p>
Construyendo un intervalo de confianza. Dado el nivel de confianza \(\beta \in
(0, 1)\), para construir un intervalo de confianza de nivel \(\beta\) notamos que
</p>

<p>
\[\beta=\mathbb{P}_{\theta}\left(q_{1-\beta} \leq Q(\mathbf{X}, \theta) \leq
1\right)=\mathbb{P}_{\theta}\left(q_{1-\beta} \leq X_{(n)} / \theta \leq
1\right)\]
</p>

<p>
Despejando \(\theta\) de las desigualdades dentro de la probabilidad, resulta que
</p>

<p>
\[I(\mathbf{X})=\left[X_{(n)}, \frac{X_{(n)}}{q_{1-\beta}}\right]=\left[X_{(n)},
\frac{X_{(n)}}{(1-\beta)^{1 / n}}\right]\]
</p>

<p>
es un intervalo de confianza para \(\theta\) de nivel \(\beta\).
</p>
</div>
</div>
</div>
<div id="outline-container-org97c193a" class="outline-4">
<h4 id="org97c193a">Pivotes crecientes</h4>
<div class="outline-text-4" id="text-org97c193a">
<p>
Sea \(Q(X, \theta)\) un pivote para \(\theta\) que goza de las siguientes propiedades:
</p>
<ol class="org-ol">
<li>la funci√≥n de distribuci√≥n de \(Q(X, \theta)\) es continua y estrictamente
creciente;</li>
<li>para cada \(\mathbf{x}\), la funci√≥n Q(x, &theta;) es continua y mon√≥tona
creciente en la variable &theta;: \[\theta_{1}<\theta_{2} \Longrightarrow
   Q\left(\mathbf{x},\theta_{1}\right)<Q\left(\mathbf{x}, \theta_{2}\right)\]</li>
</ol>

<p>
Sea \(\gamma \in (0, 1)\), arbitrario pero fijo y sea q<sub>&gamma;</sub>
el cuantil-&gamma; del pivote Q(X, &theta;).
</p>

<p>
Para cada \(\mathbf{x}\), sea \(\theta(x, \gamma)\) la √∫nica soluci√≥n de la ecuaci√≥n
en \(\theta\)
</p>

<p>
\[Q(x, \theta) = q_\gamma\]
</p>

<p>
Como el pivote Q(X, &theta;) es creciente en &theta; tenemos que
</p>

<p>
\[Q(\mathbf{X}, \theta) \leq q_{\gamma} \Longleftrightarrow \theta \leq \theta(\mathbf{X}, \gamma)\]
</p>

<p>
En consecuencia,
</p>

<p>
\[\mathbb{P}_{\theta}(\theta \leq \theta(\mathbf{X},
\gamma))=\mathbb{P}_{\theta}\left(Q(\mathbf{X}, \theta) \leq
q_{\gamma}\right)=\gamma, \qquad \forall \theta \in \Theta\]
</p>

<p>
Por lo tanto, &theta;(X, &gamma;) es una cota superior de confianza para \(\theta\)
de nivel &gamma; y una cota inferior de nivel 1 ‚àí &gamma;.
</p>
</div>

<div id="outline-container-org8f61068" class="outline-5">
<h5 id="org8f61068">M√©todo</h5>
<div class="outline-text-5" id="text-org8f61068">
<p>
Sea &beta; &isin; (0, 1). Si se dispone de un pivote Q(X, &theta;) que satisface las propiedades (i) y (ii')
enunciadas m√°s arriba, entonces
</p>
<ul class="org-ul">
<li>la variable aleatoria, &theta;<sub>1</sub>(X), que se obtiene resolviendo la ecuaci√≥n
Q(X, &theta;) = q<sub>1‚àí&beta;</sub> es una cota inferior de confianza para \(\theta\), de
nivel \(\beta\).</li>
<li>la variable aleatoria, &theta;<sub>2</sub>(X), que se obtiene re solviendo la ecuaci√≥n
Q(X, &theta;) = q<sub>&beta;</sub> es una cota superior de confianza para \(\theta\), de
nivel \(\beta\)}.</li>
<li>el intervalo aleatorio \(I(\mathbf{X})=\left[\theta_{1}(\mathbf{X}),
  \theta_{2}(\mathbf{X})\right]\), cuyos extremos son las soluciones respectivas
de las ecuaciones \(Q(\mathbf{X}, \theta)=q_{\frac{1-\beta}{2}} \mathrm{y}
  Q(\mathbf{X}, \theta)=q_{\frac{1+\beta}{2}}\) , es un intervalo <i>bilateral</i> de
confianza para \(\theta\), de nivel \(\beta\)}.</li>
</ul>
</div>
</div>
<div id="outline-container-org7f03df6" class="outline-5">
<h5 id="org7f03df6">Ejemplo 1.7 (Intensidad de la distribuci√≥n exponencial)</h5>
<div class="outline-text-5" id="text-org7f03df6">
<p>
Sea X = (X<sub>1</sub>, &hellip; , X<sub>n</sub>) una muestra aleatoria de una variable aleatoria \(X
\sim Exp(\lambda), \lambda > 0\).
</p>

<p>
El estimador de m√°xima verosimilitud para &lambda; es 1 / \overline{X}, donde
\(\overline{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}\) . Sabemos que la suma \(n
\overline{X}=\sum_{i=1}^{n} X_{i}\) tiene distribuci√≥n \(\Gamma(n, \lambda)\).
</p>

<p>
Como la distribuci√≥n de \(n\overline{X}\) depende de \(\lambda\),\(n \overline{X}\) no
es un pivote para \(\lambda\). Sin embargo, podemos liberarnos de \(\lambda\)
utilizando un cambio de variables lineal de la forma \(Q = an\overline{X}\), donde
\(a\) es positivo y elegido adecuadamente para nuestros prop√≥sitos. Si \(a > 0\) y
\(Q = an \overline{X}\), entonces \(Q \sim \Gamma\left(n,\frac{\lambda}{a}\right)\).
</p>

<p>
Poniendo \(a = 2 \lambda\), resulta que \(Q=2 \lambda n \overline{X} \sim
\Gamma\left(n, \frac{1}{2}\right)=\chi_{2 n}^{2}\) . (Recordar que
\(\Gamma\left(\frac{n}{2}, \frac{1}{2}\right)=\chi_{n}^{2}\).)
</p>

<p>
Por lo tanto,
</p>

<p>
\[Q(\mathbf{X}, \lambda)=2 \lambda n \overline{X}=2 \lambda \sum_{i=1}^{n} X_{i}
\sim \chi_{2 n}^{2}\]
</p>

<p>
es un pivote para &lambda;.
</p>

<p>
<i>Construyendo una cota superior de confianza</i>. Dado &beta; &isin; (0, 1), para
construir una cota} superior de confianza para &lambda;, de nivel \(\beta\),
primero observamos que el pivote Q(X, &lambda;) = 2&lambda; n \overline{X} es una
funci√≥n continua y decreciente en &lambda;. Debido a que
</p>

<p>
\[2 \lambda n \overline{X}=\chi_{\beta}^{2} \Longleftrightarrow
\lambda=\frac{\chi_{\beta}^{2}}{2 n \overline{X}}\]
</p>

<p>
resulta que
</p>

<p>
\[\lambda_{2}(\mathbf{X})=\frac{\chi_{\beta}^{2}}{2 \sum_{i=1}^{n} X_{i}}\]
</p>

<p>
es una cota superior de confianza para &lambda; de nivel \(\beta\).
</p>

<p>
Ilustraci√≥n. Consideremos ahora las siguientes 10 observaciones
\[0.5380,0.4470,0.2398,0.5365,0.0061\] \[0.3165,0.0086,0.0064,0.1995,0.9008\]
</p>

<p>
En tal caso tenemos \(\sum_{i=1}^{10}=3.1992\). Tomando \(\beta = 0.975\), tenemos
de la tabla de la distribuci√≥n \(\chi_{20}^{2}\) que \(\chi_{20,0.975}^{2}=34.17\) ,
entonces \(\lambda_2(x) = 5.34\) es una cota superior de confianza para &lambda; de
nivel \(\beta = 0.975\).
</p>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-org3f9cf9b" class="outline-2">
<h2 id="org3f9cf9b">Muestras de Poblaciones Normales</h2>
<div class="outline-text-2" id="text-org3f9cf9b">
<p>
En esta secci√≥n estudiaremos la distribuci√≥n de probabilidades de los
estimadores de m√°xi ma verosimilitud para la media y la varianza de poblaciones
normales. La t√©cnica de an√°lisis se basa en la construcci√≥n de pivotes para los
par√°metros desconocidos. Usando esos pivotes mostraremos como construir
intervalos de confianza en los distintos escenarios posibles que se pueden
presentar.
</p>
</div>
<div id="outline-container-orgc5a167d" class="outline-5">
<h5 id="orgc5a167d">Notaci√≥n</h5>
<div class="outline-text-5" id="text-orgc5a167d">
<p>
En todo lo que sigue usaremos la siguiente notaci√≥n: para cada &gamma; &isin; (0,
1), z<sub>&gamma;</sub> ser√° el √∫nico n√∫mero real tal que \(\Phi(z_{ \gamma} ) = \gamma\).
Gr√°ficamente, a izquierda del punto z<sub>&gamma;</sub> el √°rea bajo la campana de Gauss
es igual a &gamma;.
</p>
</div>
</div>
<div id="outline-container-org91b6fa2" class="outline-5">
<h5 id="org91b6fa2">Nota Bene</h5>
<div class="outline-text-5" id="text-org91b6fa2">
<p>
De la simetr√≠a de la campana de Gauss, se deduce que para cada \(\beta \in (0, 1)\)
vale que \(z_{(1-\beta) / 2}=-z_{(1+\beta) / 2}\). Por lo tanto, para \(Z \sim
N(0, 1)\) vale que \[\mathbb{P}\left(-z_{(1+\beta) / 2} \leq Z \leq z_{(1+\beta)
/ 2}\right)=\Phi\left(z_{(1+\beta) / 2}\right)-\Phi\left(-z_{(1+\beta) /
2}\right)=\frac{1+\beta}{2}-\frac{1-\beta}{2}=\beta\]
</p>
</div>
</div>

<div id="outline-container-org6dc5d34" class="outline-3">
<h3 id="org6dc5d34">Media y varianza desconocidas</h3>
<div class="outline-text-3" id="text-org6dc5d34">
<p>
Sea X = (X<sub>1</sub> , &hellip; , X<sub>n</sub>) una muestra aleatoria de una variable aleatoria X
&sim; N(&mu;, &sigma;<sup>2</sup> ), con media &mu; y varianza desconocidas. Los estimadores
de m√°xima verosimilitud para la media y la varianza, basados en \(\mathbf{X}\),
son, respectivamente,
</p>

<p>
\[\hat{\mu}_{m v}(\mathbf{X})=\overline{X}, \qquad \widehat{\sigma^{2}}_{m
v}(\mathbf{X})=\frac{1}{n} \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}\]
</p>
</div>

<div id="outline-container-org9951948" class="outline-4">
<h4 id="org9951948">Teorema llave</h4>
<div class="outline-text-4" id="text-org9951948">
</div>
<div id="outline-container-orgc730cee" class="outline-5">
<h5 id="orgc730cee">Teorema 2.1 (Llave)</h5>
<div class="outline-text-5" id="text-orgc730cee">
<p>
Sea \(X = (X_1, \dots , X_n)\) una muestra aleatoria de una distribuci√≥n
\(N(\mu, \sigma^2)\). Valen las siguientes afirmaciones:
</p>
<ol class="org-ol">
<li>\(Z=\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}\) tiene distribuci√≥n N(0, 1).</li>
<li>\(U=\frac{n-1}{\sigma^{2}} S^{2}=\frac{1}{\sigma^{2}}
   \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}\) tiene distribuci√≥n
\(\chi_{n-1}^{2}\).</li>
<li>\(Z\) y \(U\) son variables aleatorias independientes.</li>
</ol>
</div>
</div>
<div id="outline-container-org4a57a0c" class="outline-5">
<h5 id="org4a57a0c">Nota Bene</h5>
<div class="outline-text-5" id="text-org4a57a0c">
<p>
El calificativo de <i>llave</i> para el Teorema 2.1 est√° puesto para destacar que sus
resultados son la clave fundamental en la construcci√≥n de intervalos de
confianza y de reglas de decisi√≥n sobre hip√≥tesis estad√≠sticas para
distribuciones normales. La prueba de este Teorema puede verse en el Ap√©ndice.
</p>
</div>
</div>
<div id="outline-container-org22d1be3" class="outline-5">
<h5 id="org22d1be3">Corolario 2.2 (Pivotes para la media y la varianza)</h5>
<div class="outline-text-5" id="text-org22d1be3">
<p>
Sea X = (X<sub>1</sub>, &hellip; , X<sub>n</sub>) una muestra aleatoria de una distribuci√≥n N(&mu;,
&sigma;<sup>2</sup>). Sean \(\overline{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i} \mathrm{y}
S^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}\). Vale
que:
</p>
<ol class="org-ol">
<li>\(Q\left(\mathbf{X}, \sigma^{2}\right)=\frac{(n-1)}{\sigma^{2}} S^{2}\) es un
pivote para la varianza \(\sigma^2\) y su distribuci√≥n es una chi cuadrado con
\(n ‚àí 1\) grados de libertad (en s√≠mbolos, Q(X, &sigma;<sup>2</sup>) &sim;
&chi;<sub>n-1</sub><sup>2</sup>)$.</li>
<li>\(Q(\mathbf{X}, \mu)=\frac{\sqrt{n}(\overline{X}-\mu)}{S}\) es un pivote para
la media &mu; y su distribuci√≥n es una t de Student con \(n ‚àí 1\) grados de
libertad (en s√≠mbolos, \(Q(X, \mu) \sim t_{n‚àí1}\)).</li>
</ol>
</div>
</div>
<div id="outline-container-org106f2af" class="outline-5">
<h5 id="org106f2af">Demostraci√≥n</h5>
<div class="outline-text-5" id="text-org106f2af">
<ol class="org-ol">
<li>Inmediato de l a afirmaci√≥n (b) del Teorema 2.1.</li>
<li>La afirmaci√≥n (a) del Teorema 2.1 indica que \(Z=\sqrt{n}(\overline{X}-\mu) /
   \sigma \sim \mathcal{N}(0,1)\). Pero como \(\sigma^2\) es un par√°metro
desconocido, la transformaci√≥n \(\sqrt{n}(\overline{X}-\mu) / \sigma\) es
in√∫til por s√≠ sola para construir un pivote. Sin embargo, la afirmaci√≥n (c)
del Teorema 2.1 muestra que este problema se puede resolver reemplazando la
desconocida \(\sigma^2\) por su estimaci√≥n insesgada \(S^2\) . Concretamente,
tenemos que</li>
</ol>

<p>
\[Q(\mathbf{X}, \mu) = \frac{\sqrt{n}(\overline{X}-\mu)}{S} =
\frac{\sqrt{n}(\overline{X}-\mu) / \sigma}{S / \sigma} =
\frac{\sqrt{n}(\overline{X}-\mu) / \sigma}{\sqrt{S^{2} / \sigma^{2}}} =
\frac{Z}{\sqrt{U /(n-1)}}\],
</p>

<p>
donde \(Z=\sqrt{n}(\overline{X}-\mu) / \sigma \sim \mathcal{N}(0,1) \mathrm{y}
U=\frac{(n-1)}{\sigma^{2}} S^{2} \sim \chi_{n-1}^{2}\) son variables aleatorias
independientes. En consecuencia, Q(X, &mu;}) &sim; t<sub>n-1</sub>.
</p>
</div>
</div>
</div>
<div id="outline-container-org271db9b" class="outline-4">
<h4 id="org271db9b">Cotas e intervalos de confianza para la varianza</h4>
<div class="outline-text-4" id="text-org271db9b">
<p>
Notar que el pivote para la varianza Q(X, &sigma;<sup>2</sup>) definido en (6) goza de las
propiedades enunciadas en la secci√≥n 1.1.1 para pivotes decrecientes:
</p>
<ul class="org-ul">
<li>la funci√≥n de distribuci√≥n de Q(X, &sigma;<sup>2</sup>) es continua y estrictamente
creciente</li>
<li>para cada \(\mathbf{x}\), la funci√≥n Q(x, &sigma;<sup>2</sup>) es continua y mon√≥tona
decreciente respecto de &sigma;<sup>2</sup>.</li>
</ul>
<p>
En consecuencia, las cotas e intervalos de confianza para la varianza se pueden
construir usando el resolviendo la ecuaci√≥n \(Q(X, \sigma^2) = \chi_{n-1,
\gamma}^{2}\) , donde \(c h i_{n-1, \gamma}^{2}\) designa el cuantil-&gamma; de la
distribuci√≥n chi cuadrado con n ‚àí 1 grados de libertad.
</p>

<p>
Observando que
</p>

<p>
\[Q\left(\mathbf{X}, \sigma^{2}\right)=\chi_{n-1, \gamma}^{2}
\Longleftrightarrow \frac{(n-1) S^{2}}{\sigma^{2}}=\chi_{n-1, \gamma}^{2}
\Longleftrightarrow \sigma^{2}=\frac{(n-1) S^{2}}{\chi_{n-1, \gamma}^{2}}\]
</p>

<p>
se deduce que, para cada &beta; &isin; (0, 1),
</p>
<ol class="org-ol">
<li>\[\sigma_{1}^{2}(\mathbf{X})=\frac{(n-1) S^{2}}{\chi_{n-1, \beta}^{2}}\] es
una cota inferior de confianza de nivel \(\beta\) para &sigma;<sup>2</sup>;</li>
<li>\[\sigma_{2}^{2}(\mathbf{X})=\frac{(n-1) S^{2}}{\chi_{n-1,1-\beta}^{2}}\] es
una cota superior de confianza de nivel \(\beta\) para &sigma;<sup>2</sup>;</li>
<li>\[I(\mathbf{X})=\left[\frac{(n-1) S^{2}}{\chi_{n-1,(1+\beta) / 2}^{2}},
   \frac{(n-1) S^{2}}{\chi_{n-1,(1-\beta) / 2}^{2}}\right]\] es un intervalo de
confianza de nivel \(\beta\) para &sigma;<sup>2</sup>.</li>
</ol>
</div>
</div>
<div id="outline-container-orgc663260" class="outline-4">
<h4 id="orgc663260">Cotas e intervalos de confianza para la media</h4>
<div class="outline-text-4" id="text-orgc663260">
<p>
Notar que el pivote para la media Q(X, &mu;) definido en (7) goza de las
propiedades enunciadas en la secci√≥n 1.1.1 para pivotes decrecientes:
</p>
<ul class="org-ul">
<li>la funci√≥n de distribuci√≥n de Q(X, &mu;) es continua y estrictamente creciente;</li>
<li>para cada \(\mathbf{x}\), la funci√≥n Q(x, &mu;) es continua y mon√≥tona
decreciente respecto de &mu;.</li>
</ul>

<p>
En consecuencia, las cotas e intervalos de confianza para la varianza se pueden
construir usando el resolviendo la ecuaci√≥n \(Q(\mathbf{X}, \mu)=t_{n-1,
\gamma}\), donde \(t_{n-1, \gamma}\) designa el cuantil-&gamma; de la distribuci√≥n t
de Student con n ‚àí 1 grados de libertad.
</p>

<p>
Observando que
</p>

<p>
\[Q(\mathbf{X}, \mu)=t_{n-1, \gamma} \Longleftrightarrow
\frac{\sqrt{n}(\overline{X}-\mu)}{S}=t_{n-1, \gamma} \Longleftrightarrow
\mu=\overline{X}-\frac{S}{\sqrt{n}} t_{n-1, \gamma}\]
</p>

<p>
y usando que que la densidad de la distribuci√≥n \(t_{n‚àí1}\) es sim√©trica respecto
del origen (i.e, \(t_{n-1,1-\gamma}=-t_{n-1, \gamma}\)), tenemos que, para cada
&beta; &isin; (0.5, 1),
</p>

<ol class="org-ol">
<li>\[\mu_{1}(\mathbf{X})=\overline{X}-\frac{S}{\sqrt{n}} t_{n-1, \beta}\] es una
cota inferior de confianza de nivel \(\beta\) para &mu;};</li>
<li>\[\mu_{2}(\mathbf{X})=\overline{X}-\frac{S}{\sqrt{n}}
   t_{n-1,1-\beta}=\overline{X}+\frac{S}{\sqrt{n}} t_{n-1, \beta}\] es una cota
superior de confianza de nivel \(\beta\) para &mu;};</li>
<li>\[I(\mathbf{X})=\left[\overline{X}-\frac{S}{\sqrt{n}} t_{n-1,(1+\beta) / 2},
   \overline{X}+\frac{S}{\sqrt{n}} t_{n-1,(1+\beta) / 2}\right]\] es un
intervalo de confianza de nivel \(\beta\) para &mu;.</li>
</ol>
</div>
</div>
<div id="outline-container-org6dd0bda" class="outline-4">
<h4 id="org6dd0bda">Ejemplo</h4>
<div class="outline-text-4" id="text-org6dd0bda">
<p>
Para fijar ideas vamos a construir intervalos de confianza de nivel \(\beta\) =
0.95 para la media y la varianza de una variable normal N(&mu;, &sigma;<sup>2</sup> ),
basados en una muestra aleatoria de volumen n = 8 que arroj√≥ los resultados
siguientes: 9, 14, 10, 12, 7, 13, 11, 12.
</p>

<p>
El problema se resuelve recurriendo a las tablas de las distribuciones &Chi;<sup>2</sup> y
t y haciendo algunas cuentas.
</p>

<p>
Como n = 8 consultamos las tablas de &Chi;<sub>7</sub><sup>2</sup> y de t<sub>7</sub>. Para el nivel \(\beta\) =
0.95 tenemos que $(1+&beta;) / 2=0.975 $ y \((1-\beta) / 2=0.025\). De acuerdo con
las tablas \(\chi_{7,0.975}^{2}=16.0127, \chi_{7,0.025}^{2}= 1.6898\) y \(t_{ 7,
0.975} = 2.3646\). Por otra parte, \(\overline{X} = 11, S^2= 36 / 7 = 5.1428\) y \(S
= 2.2677\).
</p>

<p>
Algunas cuentas m√°s (y un poco de paciencia) permiten rematar este asunto. Salvo
errores de cuentas, \(I_1 = [2.248, 21.304]\) es un intervalo de confianza de
nivel 0.95 para la varianza, mientras que \(I_2 = [9.104, 12.895]\) es un
intervalo de confianza de nivel 0.95 para la media.
</p>
</div>
</div>
</div>
<div id="outline-container-org00094c6" class="outline-3">
<h3 id="org00094c6">Media de la normal con varianza conocida</h3>
<div class="outline-text-3" id="text-org00094c6">
<p>
Sea X = (X<sub>1</sub> , &hellip; , X<sub>n</sub>) una muestra aleatoria de una variable aleatoria X
&sim; N(&mu;, &sigma;<sup>2</sup>), con varianza &sigma;<sup>2</sup> conocida. En el Ejemplo 1.4
mostramos que
</p>

<p>
\[Q(\mathbf{X}, \mu)=\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma} \sim
\mathcal{N}(0,1)\]
</p>

<p>
es un pivote para la media &mu;.
</p>

<p>
Como el pivote para la media goza de las propiedades enunciadas en la secci√≥n
1.1.1 para pivotes decrecientes,
</p>
<ul class="org-ul">
<li>la funci√≥n de distribuci√≥n de Q(X, &mu;) es continua y estrictamente creciente,</li>
<li>para cada x, la funci√≥n Q(x, &mu;}) es continua y mon√≥tona decreciente respecto
de &mu;,</li>
</ul>

<p>
las cotas e intervalos de confianza para la media se pueden construir resolviendo la ecuaci√≥n
Q(X, &mu;) = z<sub>&gamma;</sub>, donde z<sub>&gamma;</sub> designa el cuantil-&gamma; de la
distribuci√≥n normal est√°ndar N(0, 1).
</p>

<p>
Observando que
</p>

<p>
\[Q(\mathbf{X}, \mu)=z_{\gamma} \Longleftrightarrow
\frac{\sqrt{n}(\overline{X}-\mu)}{\sigma}=z_{\gamma} \Longleftrightarrow
\mu=\overline{X}-\frac{\sigma}{\sqrt{n}} z_{\gamma}\]
</p>

<p>
y usando que que la densidad de la distribuci√≥n N(0, 1) es sim√©trica respecto del origen (i.e,
z<sub>1‚àí&gamma;</sub> = ‚àíz<sub>&gamma;</sub>), tenemos que, para cada &beta; &isin; (0.5, 1),
</p>

<ol class="org-ol">
<li>\[\mu_{1}(\mathbf{X})=\overline{X}-\frac{\sigma}{\sqrt{n}} z_{\beta}\] es una
cota inferior de confianza de nivel \(\beta\) para &mu;};</li>
<li>\[\mu_{2}(\mathbf{X})=\overline{X}+\frac{\sigma}{\sqrt{n}} z_{\beta}\] es una
cota superior de confianza de nivel \(\beta\) para &mu;};</li>
<li>\[I(\mathbf{X})=\left[\overline{X}-\frac{\sigma}{\sqrt{n}} z_{(1+\beta) / 2},
   \overline{X}+\frac{\sigma}{\sqrt{n}} z_{(1+\beta) / 2}\right]\] es un
intervalo de confianza de nivel \(\beta\) para &mu;}.</li>
</ol>
</div>
</div>
<div id="outline-container-org3c2712f" class="outline-3">
<h3 id="org3c2712f">Varianza de la normal con media conocida</h3>
<div class="outline-text-3" id="text-org3c2712f">
<p>
Sea X = (X<sub>1</sub> , &hellip; , X<sub>n</sub>) una muestra aleatoria de una variable aleatoria X
&sim; N(&mu;, &sigma;<sup>2</sup>), con media &mu; conocida. El estimador de m√°xima
verosimilitud para &sigma;<sup>2</sup> es
</p>

<p>
\[\widehat{\sigma^{2}}_{m v}(\mathbf{X})=\frac{1}{n}
\sum_{i=1}^{n}\left(X_{i}-\mu\right)^{2}\]
</p>

<p>
Para construir un pivote para la varianza observamos que
</p>

<p>
\[\frac{n}{\sigma^{2}} \widehat{\sigma^{2}}_{m
v}(\mathbf{X})=\sum_{i=1}^{n}\left(\frac{X_{i}-\mu}{\sigma}\right)^{2}=\sum_{i=1}^{n}
Z_{i}^{2}\]
</p>

<p>
donde \(Z_{i}=\frac{X_{i}-\mu}{\sigma}\) son variables independientes cada una con
distribuci√≥n normal est√°ndar N(0, 1). En otras palabras, la distribuci√≥n de la
variable aleatoria \(\frac{n}{\sigma^{2}} \widehat{\sigma^{2}}_{m v}(\mathbf{X})\)
coincide con la distribuci√≥n de una suma de la forma \(\sum_{i=1}^{n} Z_{i}^{2}\),
donde las Z<sub>i</sub> son N(0, 1) independientes. Por lo tanto,
</p>

<p>
\[Q\left(\mathbf{X}, \sigma^{2}\right)=\frac{n \widehat{\sigma^{2}} m
v(\mathbf{X})}{\sigma^{2}} \sim \chi_{n}^{2}\]
</p>

<p>
es un pivote para &sigma;<sup>2</sup>.
</p>

<p>
Como el pivote para la varianza Q(X, &sigma;<sup>2</sup> ) goza de las propiedades
enunciadas en la secci√≥n 1.1.1 para pivotes decrecientes,
</p>
<ul class="org-ul">
<li>la funci√≥n de distribuci√≥n de Q(X, &sigma;<sup>2</sup>) es continua y estrictamente creciente,</li>
<li>para cada x, la funci√≥n Q(x, &sigma;<sup>2</sup>) es continua y mon√≥tona decreciente
respecto de &sigma;<sup>2</sup>,</li>
</ul>

<p>
las cotas e intervalos de confianza para la varianza se pueden construir
resolviendo la ecuaci√≥n
</p>

<p>
\(Q\left(\mathbf{X}, \sigma^{2}\right)=\chi_{n, \gamma}^{2}\)
, donde \(\chi_{n, \gamma}^{2}\)
designa el cuantil-&gamma; de la distribuci√≥n chi cuadrado con n grados
de libertad.
</p>

<p>
Observando que
</p>

<p>
\[Q\left(\mathbf{X}, \sigma^{2}\right)=\chi_{n, \gamma}^{2} \Longleftrightarrow
\frac{n \widehat{\sigma^{2}} m v(\mathbf{X})}{\sigma^{2}}=\chi_{n, \gamma}^{2}
\Longleftrightarrow \sigma^{2}=\frac{n \widehat{\sigma^{2}} m
v(\mathbf{X})}{\chi_{n-1, \gamma}^{2}}\]
</p>

<p>
se deduce que, para cada &beta; &isin; (0, 1),
</p>

<ol class="org-ol">
<li>\[\sigma_{1}^{2}(\mathbf{X})=\frac{n \widehat{\sigma^{2}} m
   v(\mathbf{X})}{\chi_{n, \beta}^{2}}\] es una cota inferior de confianza de
nivel \(\beta\) para &sigma;<sup>2</sup>;</li>
<li>\[\sigma_{2}^{2}(\mathbf{X})=\frac{n \widehat{\sigma^{2}} m
   v(\mathbf{X})}{\chi_{n, 1-\beta}^{2}}\] es una cota superior de confianza de
nivel \(\beta\) para &sigma;<sup>2</sup>;</li>
<li>\[I(\mathbf{X})=\left[\frac{n
   \widehat{\sigma^{2}}_{mv}(\mathbf{X})}{\chi_{n,(1+\beta) / 2}^{2}}, \frac{n
   \widehat{\sigma^{2}}_{mv}(\mathbf{X})}{\chi_{n,(1-\beta) / 2}^{2}}\right]\]
es un intervalo de confianza de nivel \(\beta\) para &sigma;<sup>2</sup>.</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-org7926581" class="outline-2">
<h2 id="org7926581">Intervalos aproximados para ensayos Bernoulli</h2>
<div class="outline-text-2" id="text-org7926581">
<p>
Sea X = (X<sub>1</sub> , &hellip; , X<sub>n</sub>) una muestra aleatoria de una variable aleatoria X &sim; Bernoulli(p),
donde n &gt;&gt; 1. El estimador de m√°xima verosimilitud para p es
\[\overline{X}=\frac{1}{n} \sum_{i=1}^{n} X_{i}\]
</p>

<p>
Para construir un pivote para la varianza observamos que de acuerdo con el
Teorema cen tral del l√≠mite la distribuci√≥n aproximada de \(\sum_{i=1}^{n} X_{i}\)
es una normal N(np, n p(1 ‚àí p)) y en consecuencia
</p>

<p>
\[Q(\mathbf{X}, p)=\frac{\sqrt{n}(\overline{X}-p)}{\sqrt{p(1-p)}} \sim
\mathcal{N}(0,1)\]
</p>

<p>
es un pivote asint√≥tico para p.
</p>

<p>
Usando m√©todos anal√≠ticos se puede mostrar que Q(X, p) es una funci√≥n continua y
de creciente en p &isin; (0, 1). Como el pivote asint√≥tico para p goza de las
propiedades enunciadas en la secci√≥n 1.1.1 para pivotes decrecientes, las cotas
e intervalos de confianza para p se pueden construir resolviendo la ecuaci√≥n
Q(X, p) = z<sub>&gamma;</sub> , donde z<sub>&gamma;</sub> designa el cuantil-&gamma; de la
distribuci√≥n normal est√°ndar N(0, 1).
</p>

<p>
Para resolver la ecuaci√≥n Q(X, p) = z se elevan ambos miembros al cuadrado y se
obtiene una ecuaci√≥n cuadr√°tica en p cuya soluci√≥n es
</p>

<p>
\[p=\frac{z^{2}+2 n \overline{X}}{2 z^{2}+2 n} \pm \frac{z \sqrt{z^{2}+4 n
\overline{X}(1-\overline{X})}}{2 z^{2}+2 n}\]
</p>

<p>
Usando que la densidad de la distribuci√≥n N(0, 1) es sim√©trica respecto del
origen tenemos que, para cada &beta; &isin; (0.5, 1),
</p>

<ol class="org-ol">
<li>\[p_{1}(\mathbf{X})=\frac{z_{\beta}^{2}+2 n \overline{X}}{2 z_{\beta}^{2}+2
   n}-\frac{z_{\beta} \sqrt{z_{\beta}^{2}+4 n \overline{X}(1-\overline{X})}}{2
   z_{\beta}^{2}+2 n}\] es una cota inferior de confianza de nivel \(\beta\) para
p;</li>
<li>\[p_{2}(\mathbf{X})=\frac{z_{\beta}^{2}+2 n \overline{X}}{2 z_{\beta}^{2}+2
   n}+\frac{z_{\beta} \sqrt{z_{\beta}^{2}+4 n \overline{X}(1-\overline{X})}}{2
   z_{\beta}^{2}+2 n}\] es una cota superior de confianza de nivel \(\beta\) para
p;</li>
<li>\[I(\mathbf{X})=\left[\frac{z_{(1+\beta) / 2}^{2}+2 n \overline{X}}{2
   z_{(1+\beta) / 2}^{2}+2 n} \pm \frac{z_{(1+\beta) / 2} \sqrt{z_{(1+\beta) /
   2}^{2}+4 n \overline{X}(1-\overline{X})}}{2 z_{(1+\beta) / 2}^{2}+2
   n}\right]\] donde [a ¬± b] = [a ‚àí b, a + b], es un intervalo de confianza de
nivel \(\beta\) para p.</li>
</ol>
</div>
<div id="outline-container-org64833ab" class="outline-5">
<h5 id="org64833ab">Ejemplo 3.1 (Las agujas de BuÔ¨Äon)</h5>
<div class="outline-text-5" id="text-org64833ab">
<p>
Se arroja al azar una aguja de longitud 1 sobre un plano dividido por rectas
paralelas separadas por una distancia igual a 2.
</p>

<p>
Si localizamos la aguja mediante la distancia &rho; de su centro a la recta m√°s
cercana y el √°ngulo agudo &alpha; entre la recta y la aguja, el espacio muestral
es el r ect√°ngulo 0 &le; &rho; &le; 1 y 0 &le; &alpha; &le; &pi;/2. El evento <i>la
aguja interesecta la recta</i> ocurre cuando &rho; &le; \frac{1}{2} sen &alpha; y su
probabilidad es
</p>

<p>
\[p=\frac{\int_{0}^{\pi / 2} \frac{1}{2} \operatorname{sen} \alpha d \alpha}{\pi
/ 2}=\frac{1}{\pi}\]
</p>

<p>
Con el objeto de estimar &pi; se propone construir un interval o de confianza de
nivel \(\beta\) = 0.95 para p, basado en los resultados de realizar el
experimentos de BuÔ¨Äon con n = 100 agujas.
</p>

<p>
Poniendo en (10) n = 100 y z<sub>(1+ &beta;) / 2</sub> = z<sub>0.975</sub> = 1.96 se obtiene que
</p>

<p>
\[\begin{aligned} I(\mathbf{X}) &=\left[\frac{1.96^{2}+200
\overline{X}}{2(1.96)^{2}+200} \pm \frac{1.96 \sqrt{1.96^{2}+400
X(1-\overline{X})}}{2(1.96)^{2}+200}\right] \\ &=\left[\frac{3.8416+200
\overline{X}}{207.6832} \pm \frac{1.96 \sqrt{3.8416+400
X(1-\overline{X})}}{207.6832}\right] \end{aligned}\]
</p>

<p>
Al realizar el experimento se observ√≥ que 28 de las 100 agujas intersectaron
alguna recta. Con ese dato el estimador de m√°xima verosimilitud para p es
\overline{X} = 0.28 y en consecuencia se obtiene el siguiente intervalo de
confianza para p
</p>

<p>
\[\begin{aligned} I(\mathbf{X}) &=\left[\frac{3.8416+200(0.28)}{207.6832} \pm
\frac{1.96 \sqrt{3.8416+400(0.28)(1-0.28)}}{207.6832}\right] \\ &=[0.28814 \pm
0.08674]=[0.20140,0.37488] \end{aligned}\]
</p>

<p>
De donde se obtiene la siguiente estimaci√≥n: 2.66 &le; &pi; &le; 4.96.
</p>
</div>
</div>
<div id="outline-container-orge8722b0" class="outline-5">
<h5 id="orge8722b0">Nota Bene</h5>
<div class="outline-text-5" id="text-orge8722b0">
<p>
Notando que la longitud del intervalo de confianza de nivel \(\beta\) &gt; 1 / 2 para
p se puede acotar de la siguiente forma
</p>

<p>
\[|I(\mathbf{X})|=\frac{z_{(1+\beta) / 2} \sqrt{z_{(1+\beta) / 2}^{2}+4 n
\overline{X}(1-\overline{X})}}{z_{(1+\beta) / 2}^{2}+n} \leq \frac{z_{(1+\beta)
/ 2} \sqrt{z_{(1+\beta) / 2}^{2}+n}}{z_{(1+\beta) / 2}^{2}+n}<\frac{z_{(1+\beta)
/ 2}}{\sqrt{n}}\]
</p>

<p>
se puede mostrar que para garantizar que \(|I(\mathbf{X})|<\epsilon\), donde
&epsilon; es positivo y <i>peque√±o</i> basta tomar \(n \geq\left(z_{(1+\beta) / 2} /
\epsilon\right)^{2}\).
</p>
</div>
</div>
<div id="outline-container-org9590674" class="outline-5">
<h5 id="org9590674">Ejemplo 3.2 (Las agujas de BuÔ¨Äon (continuaci√≥n))</h5>
<div class="outline-text-5" id="text-org9590674">
<p>
¬øCu√°ntas agujas deben arrojarse si se desea estimar &pi; utilizando un intervalo
de confianza para p, de nivel 0.95, cuyo margen de error sea 0.01? De acuerdo
con la observaci√≥n anterior basta tomar n &ge; (1.96 / 0.01)<sup>2</sup> = 38416.
</p>

<p>
Simulando 38416 veces el expe rimento de BuÔ¨Äon obtuvimos 12222 √©xitos. Con ese
dato el estimador de m√°xima verosimilitud para p es 0.31814&#x2026; y el intervalo
para p es
</p>

<p>
\[I(X) = [0.31350, 0.32282]\]
</p>

<p>
De donde se obtiene la siguiente estimaci√≥n: 3.09766 &le; &pi; &le; 3.18969.
</p>
</div>
</div>
</div>
<div id="outline-container-org1ed66b6" class="outline-2">
<h2 id="org1ed66b6">Comparaci√≥n de dos muestras normales</h2>
<div class="outline-text-2" id="text-org1ed66b6">
<p>
Supongamos que \(X = (X_1 , \dots , X_m)\) es una muestra aleatoria de tama√±o m de
una distribuci√≥n normal N(&mu;<sub>X</sub> , &sigma;<sub>X</sub><sup>2</sup>), y que Y = (Y<sub>1</sub>, &hellip; , Y<sub>n</sub>) es
una muestra aleatoria de tama√±o n de una distribuci√≥n normal N(&mu;<sub>Y</sub>,
&sigma;<sub>Y</sub><sup>2</sup>). M√°s a√∫n, supongamos que las muestras X e Y son independientes.
Usualmente los par√°metros &mu;<sub>X</sub>, &mu;<sub>Y</sub>, &sigma;<sub>X</sub><sup>2</sup> y &sigma;<sub>Y</sub><sup>2</sup> son
desconocidos.
</p>
</div>

<div id="outline-container-org5b63847" class="outline-3">
<h3 id="org5b63847">Cotas e intervalos de confianza para la diferencia de medias</h3>
<div class="outline-text-3" id="text-org5b63847">
<p>
Queremos estimar \(\Delta = \mu_X ‚àí \mu_Y\).
</p>
</div>
<div id="outline-container-orgd83a019" class="outline-4">
<h4 id="orgd83a019">Varianzas conocidas</h4>
<div class="outline-text-4" id="text-orgd83a019">
<p>
Para construir un pivote para la diferencia de medias, &Delta;, cuando las
varianzas &sigma;<sub>X</sub><sup>2</sup> y &sigma;<sub>Y</sub><sup>2</sup> son conocidas, observamos que el estimador de
m√°xima verosimilitud para &Delta; = &mu;<sub>X</sub> ‚àí &mu;<sub>Y</sub> es
\overline{X} ‚àí \overline{Y} y que
</p>

<p>
\[\overline{X}-\overline{Y} \sim \mathcal{N}\left(\Delta,
\frac{\sigma_{X}^{2}}{m}+\frac{\sigma_{Y}^{2}}{n}\right)\]
</p>

<p>
En consecuencia,
</p>

<p>
\[Q(\mathbf{X}, \mathbf{Y},
\Delta)=\frac{\overline{X}-\overline{Y}-\Delta}{\sqrt{\frac{\sigma_{X}^{2}}{m}+\frac{\sigma_{Y}^{2}}{n}}}
\sim \mathcal{N}(0,1)\]
</p>

<p>
es un pivote para la diferencia de medias &Delta;.
</p>

<p>
Como el pivote para la diferencia de medias, Q(X, Y, &Delta;), goza de las
propiedades enunciadas en la secci√≥n 1.1.1 las cotas e intervalos de confianza
para &Delta; se pueden construir resolviendo la ecuaci√≥n Q(X, Y, &Delta;) =
z<sub>&gamma;</sub>, donde z<sub>&gamma;</sub> designa el cuantil-&gamma; de la distribuci√≥n N(0,
1).
</p>
</div>
</div>
<div id="outline-container-orgd90681b" class="outline-4">
<h4 id="orgd90681b">Varianzas desconocidas</h4>
<div class="outline-text-4" id="text-orgd90681b">
<p>
Supongamos ahora que las varianzas &sigma;<sub>X</sub><sup>2</sup> y &sigma;<sub>Y</sub><sup>2</sup> son desconocidas. Hay
dos posibilidades: las varianzas son iguales o las varianzas son distintas.
</p>
</div>
<div id="outline-container-org7114a17" class="outline-5">
<h5 id="org7114a17">Caso 1: Varianzas iguales</h5>
<div class="outline-text-5" id="text-org7114a17">
<p>
Supongamos que &sigma;<sub>X</sub><sup>2</sup> = &sigma;<sub>Y</sub><sup>2</sup> = &sigma;<sup>2</sup>. En tal caso
</p>

<p>
\[Z=\frac{\overline{X}-\overline{Y}-\Delta}{\sqrt{\frac{\sigma^{2}}{m}+\frac{\sigma^{2}}{n}}}=\frac{\overline{X}-\overline{Y}-\Delta}{\sqrt{\sigma^{2}}
\sqrt{\frac{1}{m}+\frac{1}{n}}} \sim \mathcal{N}(0,1)\]
</p>

<p>
La varianza desconocida &sigma;<sup>2</sup> se puede estimar ponderando <i>adecuadamente</i> los
estimadores de varianza \(S_{X}^{2}=\frac{1}{m-1}
\sum\left(X_{i}-\overline{X}\right)^{2} \mathrm{y} S_{Y}^{2}=\frac{1}{n-1}
\sum\left(Y_{j}-\overline{Y}\right)^{2}\)
</p>

<p>
\[S_{P}^{2} :=\frac{m-1}{m+n-2} S_{X}^{2}+\frac{n-1}{m+n-2}
S_{Y}^{2}=\frac{(m-1) S_{X}^{2}+(n-1) S_{Y}^{2}}{m+n-2}\]
</p>

<p>
Se puede mostrar que
</p>

<p>
\[U :=\frac{(n+m-2)}{\sigma^{2}} S_{P}^{2}=\frac{(m-1) S_{X}^{2}+(n-1)
S_{Y}^{2}}{\sigma^{2}} \sim \chi_{n+m-2}\]
</p>

<p>
Como las variables Z y U son independientes, se obtiene que
</p>

<p>
\[T=\frac{Z}{\sqrt{U
/(m+n-2)}}=\frac{\overline{X}-\overline{Y}-\Delta}{\sqrt{S_{P}^{2}}
\sqrt{\frac{1}{m}+\frac{1}{n}}} \sim t_{m+n-2}\]
</p>

<p>
Por lo tanto,
</p>

<p>
\[Q(\mathbf{X}, \mathbf{Y},
\Delta)=\frac{\overline{X}-\overline{Y}-\Delta}{\sqrt{S_{P}^{2}}
\sqrt{\frac{1}{m}+\frac{1}{n}}}\]
</p>

<p>
es un pivote para la diferencia de medias &Delta;. Debido a que el pivote goza de
las propiedades enunciadas en la secci√≥n 1.1.1, las cotas e intervalos de
confianza para &Delta; se pueden construir resolviendo la ecuaci√≥n Q(X, Y,
&Delta;) = t<sub>m+n‚àí2, &gamma;</sub>, donde t<sub>m+n‚àí2 &gamma;</sub> designa el cuantil-&gamma; de
la distribuci√≥n t de Student con m + n ‚àí 2 grados de libertad.
</p>
</div>
</div>
<div id="outline-container-orgb5b20b5" class="outline-5">
<h5 id="orgb5b20b5">Caso 2: Varianzas distintas</h5>
<div class="outline-text-5" id="text-orgb5b20b5">
<p>
En varios manuales de Estad√≠stica (el de Walpole, por
ejemplo) se afirma que la distribuci√≥n de la variable
</p>

<p>
\[Q(\mathbf{X}, \mathbf{Y},
\Delta)=\frac{\overline{X}-\overline{Y}-\Delta}{\sqrt{\frac{S_{X}^{2}}{m}+\frac{S_{X}^{2}}{n}}}\]
</p>

<p>
es una t de Student con &nu; grados de libertad, donde
</p>

<p>
\[\nu=\frac{\left(\frac{S_{X}^{2}}{m}+\frac{S_{Y}^{2}}{n}\right)^{2}}{\frac{\left(\frac{S_{X}^{2}}{m}\right)^{2}}{m-1}+\frac{\left(\frac{S_{Y}^{2}}{n}\right)^{2}}{n-1}}\]
</p>

<p>
Es de suponer que este <i>misterioso</i> valor de \(\nu\) es el resultado de alguna
controversia entre Estad√≠sticos profesionales con suficiente experiencia para
traducir semejante jerogl√≠fico. Sin embargo,ninguno de los manuales se ocupa de
revelar este misterio.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orga867fbc" class="outline-3">
<h3 id="orga867fbc">Cotas e intervalos de confianza para el cociente de varianzas</h3>
<div class="outline-text-3" id="text-orga867fbc">
<p>
Queremos estimar el cociente de las varianzas R = &sigma;<sub>X</sub><sup>2</sup>/&sigma;<sub>Y</sub><sup>2</sup>.
</p>

<p>
Si las medias &mu;<sub>X</sub> y &mu;<sub>Y</sub> son desconocidas, las varianzas &sigma;<sub>X</sub><sup>2</sup> y
&sigma;<sub>Y</sub><sup>2</sup> se pueden estimar mediante sus estimadores insesgados
\(S_{X}^{2}=\frac{1}{m-1} \sum_{i=1}^{m}\left(X_{i}-\overline{X}\right)^{2}
\mathrm{y} S_{Y}^{2}=\frac{1}{n-1}
\sum_{j=1}^{n}\left(Y_{j}-\overline{Y}\right)^{2}\).
</p>

<p>
Debido a que las variables
</p>

<p>
\[U :=\frac{(m-1)}{\sigma_{X}^{2}} S_{X}^{2} \sim \chi_{m-1}^{2} \qquad
\mathrm{y} \qquad V :=\frac{(n-1)}{\sigma_{Y}^{2}} S_{Y}^{2} \sim
\chi_{n-1}^{2}\]
</p>

<p>
son independientes, tenemos que el cociente
</p>

<p>
\[\frac{U /(m-1)}{V /(n-1)}=\frac{S_{X}^{2} / \sigma_{X}^{2}}{S_{Y}^{2} /
\sigma_{Y}^{2}}=\frac{1}{R}\left(\frac{S_{X}^{2}}{S_{Y}^{2}}\right)\]
</p>

<p>
se distribuye como una F de Fisher con m ‚àí 1 y n ‚àí 1 grados de libertad.
Por lo tanto,
</p>

<p>
\[Q(\mathbf{X}, \mathbf{Y},
R)=\frac{1}{R}\left(\frac{S_{X}^{2}}{S_{Y}^{2}}\right) \sim F_{m-1, n-1}\]
</p>

<p>
es un pivote para el cociente de varianzas \(R = \sigma_X^2/\sigma_Y^2\). Debido a
que el pivote goza de las propiedades enunciadas en la secci√≥n 1.1.1, las cotas
e intervalos de confianza para R se pueden construir resolviendo la ecuaci√≥n
\(Q(\mathbf{X}, \mathbf{Y}, R)=F_{m-1, n-1, \gamma}\) , donde \(F_{m-1, n-1
\gamma}\) designa el cuantil-&gamma; de la distribuci√≥n F de Fisher con m ‚àí 1 y n
‚àí 1 grados de libertad.
</p>
</div>
</div>
</div>
<div id="outline-container-orge7b7c59" class="outline-2">
<h2 id="orge7b7c59">Comparaci√≥n de dos muestras</h2>
<div class="outline-text-2" id="text-orge7b7c59">
</div>
<div id="outline-container-org4deb98d" class="outline-3">
<h3 id="org4deb98d">Planteo general</h3>
<div class="outline-text-3" id="text-org4deb98d">
<p>
Supongamos que tenemos dos muestras aleatorias independientes \(X = (X_1 , \dots,
X_m)\) e \(Y = (Y_1, \dots , Y_n)\) con distribuciones dependientes de los
par√°metros \(\chi\) y $&eta;,$ respectivamente.
</p>

<p>
Queremos estimar la diferencia \[\Delta = \chi ‚àí \eta\]
</p>

<p>
En lo que sigue mostraremos que, bajo ciertas hip√≥tesis, podemos construir cotas
e intervalos de confianza (aproximados) basados en el comportamiento de la
diferencia \(\hat{\xi}_{m}-\hat{\eta}_{n}\) , donde \(\hat{\xi}_{m} =
\hat{\xi}(\mathbf{X})\) y \(\hat{\eta}_{n}=\hat{\eta}(\mathbf{Y})\) son estimadores
de los par√°metros \(\chi\) y \(\eta\), respectivamente.
</p>

<p>
En todo lo que sigue vamos a suponer que los estimadores \(\hat{\xi}_{m}\) y $
\hat{\eta}<sub>n</sub>$ tienen la propiedad de normalidad asint√≥tica. Esto es,
</p>

<p>
\[\begin{array}{ll}{\sqrt{m}\left(\hat{\xi}_{m}-\xi\right) \rightarrow
\mathcal{N}\left(0, \sigma^{2}\right)} & {\text { cuando } m \rightarrow \infty}
\\ {\sqrt{n}\left(\hat{\eta}_{n}-\eta\right) \rightarrow \mathcal{N}\left(0,
\tau^{2}\right)} & {\text { cuando } n \rightarrow \infty}\end{array}\]
</p>

<p>
donde &sigma;<sup>2</sup> y &tau;<sup>2</sup> pueden depender de &chi; y &eta;, respectivamente. Sea N =
m + n y supongamos que para alg√∫n 0 &lt; &rho; &lt; 1,
</p>

<p>
\(\frac{m}{N} \rightarrow \rho, \frac{n}{M} \rightarrow 1-\rho \qquad\) cuando \(m\)
y \(n \rightarrow \infty\)
</p>

<p>
de modo que, cuando N &rarr; &infin; tenemos
</p>

<p>
\(\sqrt{N}\left(\hat{\xi}_{m}-\xi\right) \rightarrow \mathcal{N}\left(0,
\frac{\sigma^{2}}{\rho}\right) \quad \mathrm{y} \qquad
\sqrt{N}\left(\hat{\eta}_{n}-\eta\right) \rightarrow \mathcal{N}\left(0,
\frac{\tau^{2}}{1-\rho}\right)\)
</p>

<p>
Entonces, vale que
</p>

<p>
\[\sqrt{N}\left[\left(\hat{\xi}_{m}-\xi\right)-\left(\hat{\eta}_{n}-\eta\right)\right]
\rightarrow \mathcal{N}\left(0,
\frac{\sigma^{2}}{\rho}+\frac{\tau^{2}}{1-\rho}\right)\]
</p>

<p>
o, equivalentemente, que
</p>

<p>
\[\frac{\left(\hat{\xi}_{m}-\hat{\eta}_{n}\right)-\Delta}{\sqrt{\frac{\sigma^{2}}{m}+\frac{\tau^{2}}{n}}}
\rightarrow \mathcal{N}(0,1)\]
</p>

<p>
Si &sigma;<sup>2</sup> y &tau;<sup>2</sup> son conocidas, de (14) resulta que
</p>

<p>
\[Q(\mathbf{X}, \mathbf{Y},
\Delta)=\frac{\left(\hat{\xi}_{m}-\hat{\eta}_{n}\right)-\Delta}{\sqrt{\frac{\sigma^{2}}{m}+\frac{\tau^{2}}{n}}}\]
</p>

<p>
es un pivote (aproximado) para la diferencia &Delta;.
</p>

<p>
Si &sigma;<sup>2</sup> y &tau;<sup>2</sup> son desconocidas y \(\widehat{\sigma^{2}}\) y
\(\widehat{\tau^{2}}\) son estimadores consistentes para &sigma;<sup>2</sup> y &tau;<sup>2</sup>, se
puede demostrar que la relaci√≥n (14) conserva su validez cuando &sigma;<sup>2</sup> y
&tau;<sup>2</sup> se reemplazan por \(\widehat{\sigma^{2}}\) y \(\widehat{\tau^{2}}\),
respectivamente y entonces
</p>

<p>
\[Q(\mathbf{X}, \mathbf{Y},
\Delta)=\frac{\left(\hat{\xi}_{m}-\hat{\eta}_{n}\right)-\Delta}{\sqrt{\frac{\widehat{\sigma^{2}}}{m}+\frac{\widehat{\tau^{2}}}{n}}}\]
</p>

<p>
es un pivote (aproximado) para la diferencia &Delta;.
</p>

<p>
Para mayores detalles se puede consultar el libro Lehmann, E. L. (1999) Elements
of Large-Sample Theory. Springer, New York.
</p>
</div>
<div id="outline-container-orgf4c9126" class="outline-5">
<h5 id="orgf4c9126">Nota Bene</h5>
<div class="outline-text-5" id="text-orgf4c9126">
<p>
Notar que el argumento anterior proporciona un m√©todo general de naturaleza
asint√≥tica. En otras palabras, en la pr√°ctica los resultados que se obtienen son
aproximados. Dependiendo de los casos particulares existen diversos
refinamientos que permiten mejorar esta primera aproximaci√≥n.
</p>
</div>
</div>
</div>
<div id="outline-container-orgefd1204" class="outline-3">
<h3 id="orgefd1204">Problema de dos muestras binomiales</h3>
<div class="outline-text-3" id="text-orgefd1204">
<p>
Sean X = (X<sub>1</sub> , &hellip; , X<sub>m</sub>) e Y = (Y<sub>1</sub>, &hellip; , Y<sub>n</sub>) dos muestras aleatorias independientes de dos
variables aleatorias X e Y con distribuci√≥n Bernoulli de par√°metros p<sub>X</sub>
y p<sub>Y</sub>, respectivamente.
</p>

<p>
Queremos estimar la diferencia
</p>

<p>
\[\Delta = p_X= p_Y\]
</p>

<p>
Para construir cotas e intervalos de confianza usaremos los estimadores de m√°xima verosimil
itud para las probabilidades p<sub>X</sub>
y p<sub>Y</sub>
</p>

<p>
\[\hat{p}_{X}=\overline{X}=\frac{1}{m} \sum_{i=1}^{m} X_{i}, \qquad
\hat{p}_{Y}=\overline{Y}=\frac{1}{n} \sum_{j=1}^{n} Y_{j}\]
</p>

<p>
Vamos a suponer que los vol√∫menes de las muestras, m y n, son suficientemente
grandes y que ninguna de las dos variables est√° sobre representada (i.e. m y n
son del mismo orden de magnitud).
</p>

<p>
Debido a que los estimadores \overline{X} y \overline{Y} son consistentes para
las p<sub>X</sub> y p<sub>Y</sub> , resulta que los estimadores \overline{X}(1‚àí\overline{X}) y
\overline{Y} (1-\overline{Y}) son consistentes para las varianzas
\(p_{X}\left(1-p_{X}\right)\) y \(p_{Y}\left(1-p_{Y}\right)\) , respectivamente. Por
lo tanto,
</p>

<p>
\[Q(\mathbf{X}, \mathbf{Y},
\Delta)=\frac{\overline{X}-\overline{Y}-\Delta}{\sqrt{\frac{1}{m}
\overline{X}(1-\overline{X})+\frac{1}{n} \overline{Y}(1-\overline{Y})}}\]
</p>

<p>
es un pivote (aproximado) para &Delta;.
</p>
</div>
<div id="outline-container-orge24b4c7" class="outline-5">
<h5 id="orge24b4c7">Ejemplo 5.1</h5>
<div class="outline-text-5" id="text-orge24b4c7">
<p>
Se toma una muestra aleatoria de 180 argentinos y resulta que 30 est√°n desocu
pados. Se toma otra muestra aleatoria de 200 uruguayos y resulta que 25 est√°n
desocupados. ¬øHay evidencia suficiente para afirmar que la tasa de desocupaci√≥n
de la poblaci√≥n Argentina es superior a la del Uruguay?
</p>
</div>
</div>
<div id="outline-container-org66baf5d" class="outline-5">
<h5 id="org66baf5d">Soluci√≥n</h5>
<div class="outline-text-5" id="text-org66baf5d">
<p>
La poblaci√≥n desocupada de la Argentina puede modelarse con una variable
aleatoria X &sim; Bernoulli(p<sub>X</sub>) y la del Uruguay con una variable aleatoria Y
&sim; Bernoulli(p<sub>Y</sub>).
</p>

<p>
Para resolver el problema utilizaremos una cota inferior de nivel de
significaci√≥n &beta; = 0.95 para la diferencia &Delta; = p<sub>X</sub> ‚àí p<sub>Y</sub> basada en dos
muestras aleatorias independientes X e Y de vol√∫menes m = 180 y n = 200,
respectivamente.
</p>

<p>
En vista de que el pivote definido en (17) goza de las propiedades enunciadas en
la secci√≥n 1.1.1, la cota inferior de nivel \(\beta\) = 0.95 para &Delta; se
obtiene resolviendo la ecuaci√≥n \(Q(\mathbf{X}, \mathbf{Y}, \Delta)= z_{0.95}\).
</p>

<p>
Observando que
</p>

<p>
\[\begin{aligned} Q(\mathbf{X}, \mathbf{Y}, \Delta)=z_{0.95} &
\Longleftrightarrow \frac{\overline{X}-\overline{Y}-\Delta}{\sqrt{\frac{1}{180}
\overline{X}(1-\overline{X})+\frac{1}{200} \overline{Y}(1-\overline{Y})}}=1.64
\\ & \Longleftrightarrow \Delta=\overline{X}-\overline{Y}-1.64
\sqrt{\frac{1}{180} \overline{X}(1-\overline{X})+\frac{1}{200}
\overline{Y}(1-\overline{Y})} \end{aligned}\]
</p>

<p>
De cuerdo con los datos observados, \(\overline{X}=\frac{30}{180}=\frac{1}{6}\) y
\(\overline{Y}=\frac{25}{200}=\frac{1}{8}\) . Por lo tanto, la cota inferior para
&Delta; adopta la forma
</p>

<p>
\(\Delta(\mathbf{x}, \mathbf{y})=\frac{1}{6}-\frac{1}{8}-1.64
\sqrt{\frac{1}{180}\left(\frac{1}{6}\right)\left(\frac{5}{6}\right)+\frac{1}{200}\left(\frac{1}{8}\right)\left(\frac{7}{8}\right)}
= -0.0178\dots\)
</p>

<p>
De este modo se obtiene la siguiente estimaci√≥n p<sub>X</sub> ‚àí p<sub>Y</sub> &gt; ‚àí0.0178 y de all√≠ no se puede
concluir que p<sub>X</sub> &gt; p<sub>Y</sub>.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgb82db6c" class="outline-2">
<h2 id="orgb82db6c">Ap√©ndice: Demostraci√≥n del Teorema llave</h2>
<div class="outline-text-2" id="text-orgb82db6c">
</div>
<div id="outline-container-orgaf9b63f" class="outline-3">
<h3 id="orgaf9b63f">Preliminares de An√°lisis y √Ålgebra</h3>
<div class="outline-text-3" id="text-orgaf9b63f">
<p>
En la prueba del Teorema 2.1 se usar√°n algunas nociones de √Ålgebra L√≠neal<sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
y el Teorema de cambio de variables para la integral m√∫ltiple<sup><a id="fnr.2" class="footref" href="#fn.2">2</a></sup>.
</p>
</div>
<div id="outline-container-org6fc5e3c" class="outline-5">
<h5 id="org6fc5e3c">Teorema 6.1 (Cambio de variables en la integral m√∫ltiple)</h5>
<div class="outline-text-5" id="text-org6fc5e3c">
<p>
Sea f : \Re<sup>n</sup> &rarr; \Re una funci√≥n integrable. Sea g : \Re<sup>n</sup> &rarr;
\Re<sup>n</sup> , g = (g<sub>1</sub>, &hellip; , g<sub>n</sub>) una aplicaci√≥n biyectiva, cuyas componentes
tienen derivadas parciales de primer orden continuas. Esto es, para todo 1 &le;
i, j &le; n, las funciones \frac{\partial}{\partial y_j} g<sub>i</sub> (\mathbf{y}) son
continuas. Si el Jacobiano de g es diferente de cero en casi todo punto,
entonces,
</p>

<p>
\[\int_{A} f(\mathbf{x}) d \mathbf{x}=\int_{g^{-1}(A)}
f(g(\mathbf{y}))\left|J_{g}(\mathbf{y})\right| d \mathbf{y}\]
</p>

<p>
para todo conjunto abierto A &sub; \Re<sup>n</sup> , donde J<sub>g</sub>(\mathbf{y}) =
\(\operatorname{det}\left(\left(\frac{\partial g_{i}(\mathbf{y})}{\partial
y_{j}}\right)_{i, j}\right)\).
</p>

<p>
El siguiente resultado, que caracteriza la distribuci√≥n de un cambio de
variables aleatorias, es una consecuencia inmediata del Teorema 6.1.
</p>
</div>
</div>
<div id="outline-container-org8ee0638" class="outline-5">
<h5 id="org8ee0638">Corolario 6.2</h5>
<div class="outline-text-5" id="text-org8ee0638">
<p>
Sea X un vector aleatorio n-dimensional con funci√≥n densidad de probabilidad
f<sub>X</sub>(x). Sea &varphi; : \Re<sup>n</sup> &rarr; \Re<sup>n</sup> una aplicaci√≥n que satisface las
hip√≥tesis del Teorema 6.1. Entonces, el vector aleatorio \(\mathbf{Y}=\varphi(\mathbf{X})\) tiene
funci√≥n densidad de probabilidad f<sub>Y</sub>(y) de la forma:
</p>

<p>
\[f_{\mathbf{Y}}(\mathbf{y})=f_{\mathbf{X}}\left(\varphi^{-1}(\mathbf{y})\right)\left|J_{\varphi^{-1}}(\mathbf{y})\right|\]
</p>
</div>
</div>

<div id="outline-container-org6dcace7" class="outline-5">
<h5 id="org6dcace7">Demostraci√≥n</h5>
<div class="outline-text-5" id="text-org6dcace7">
<p>
Cualquiera sea el conjunto abierto A se tiene que
</p>

<p>
\[\mathbb{P}(\mathbf{Y} \in A)=\mathbb{P}(\varphi(\mathbf{X}) \in
A)=\mathbb{P}\left(\mathbf{X} \in \varphi^{-1}(A)\right)=\int_{\varphi^{-1}(A)}
f_{\mathbf{X}}(\mathbf{x}) d \mathbf{x}\]
</p>

<p>
Aplicando el Teorema 6.1 para g = &varphi;<sup>‚àí1</sup> se obtiene
</p>

<p>
\(\int_{\varphi^{-1}(A)} f_{\mathbf{X}}(\mathbf{x}) d \mathbf{x}=\int_{A}
f_{\mathbf{X}}\left(\varphi^{-1}(\mathbf{y})\right)\left|J_{\varphi^{-1}}(\mathbf{y})\right|
d \mathbf{y}\)
</p>

<p>
Por ende
</p>

<p>
\(\mathbb{P}(\mathbf{Y} \in A)=\int_{A}
f_{\mathbf{X}}\left(\varphi^{-1}(\mathbf{y})\right)\left|J_{\varphi^{-1}}(\mathbf{y})\right|
d \mathbf{y}\)
</p>

<p>
Por lo tanto, el vector aleatorio Y tiene funci√≥n densidad de probabilidad de la
forma \(f_{\mathbf{Y}}(\mathbf{y}) =
f_{\mathbf{X}}\left(\varphi^{-1}(\mathbf{y})\right)\left|J_{\varphi^{-1}}(\mathbf{y})\right|\)
</p>
</div>
</div>
</div>
<div id="outline-container-org8d1e824" class="outline-3">
<h3 id="org8d1e824">Lema previo</h3>
<div class="outline-text-3" id="text-org8d1e824">
</div>
<div id="outline-container-orgc5f3abc" class="outline-5">
<h5 id="orgc5f3abc">Observaci√≥n 6.3</h5>
<div class="outline-text-5" id="text-orgc5f3abc">
<p>
Sea X = (X<sub>1</sub>, &hellip; , X<sub>n</sub>) una muestra aleatoria de una distrib uci √≥n N(0,
&sigma;<sup>2</sup>).
</p>

<p>
Por independencia, la distribuci√≥n conjunta de las variables X<sub>1</sub> , &hellip; , X<sub>n</sub>
tiene funci√≥n densidad de probabilidad de la forma
</p>

<p>
\[\begin{aligned} f(\mathbf{x}) &=\prod_{i_{1}}^{n} \frac{1}{\sqrt{2 \pi}
\sigma} \exp \left(-\frac{1}{2 \sigma^{2}} x_{i}^{2}\right)=\frac{1}{(2 \pi)^{n
/ 2} \sigma^{n}} \exp \left(-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{n}
x_{i}^{2}\right) \\ &=\frac{1}{(2 \pi)^{n / 2} \sigma^{n}} \exp
\left(-\frac{1}{2 \sigma^{2}}\|\mathbf{x}\|_{2}^{2}\right) \end{aligned}\]
</p>

<p>
De la observaci√≥n anterior es claro que la distribuci√≥n conjunta de las
variables X<sub>1</sub> , &hellip; , X<sub>n</sub> es invariante por rotaciones. M√°s concretamente vale
el siguiente resultado:
</p>
</div>
</div>
<div id="outline-container-org7995c8a" class="outline-5">
<h5 id="org7995c8a">Lema 6.4 (Isotrop√≠a)</h5>
<div class="outline-text-5" id="text-org7995c8a">
<p>
Sea X = (X<sub>1</sub>, &hellip; , X<sub>n</sub>) una muestra al eatoria d e una variable N(0, &sigma;<sup>2</sup>
) y sea B &isin; \Re<sup>n &times; n</sup> una matriz ortogonal, i.e. B<sup>TB</sup> = BB<sup>T</sup> = I<sub>n</sub>. Si
\(\underline{X} = [X_1 \dots X_n]^T\) , entonces \underline{Y}= [Y<sub>1</sub> &hellip; Y<sub>n</sub>]<sup>T</sup>
= B\underline{X} tiene la misma distribuci√≥n conjunta que \underline{X}. En
particular las variables aleatorias Y<sub>1</sub>, &hellip; , Y<sub>n</sub> son independientes y son
todas N(0, &sigma;<sup>2</sup>).
</p>
</div>
</div>
<div id="outline-container-orgfa14130" class="outline-5">
<h5 id="orgfa14130">Demostraci√≥n</h5>
<div class="outline-text-5" id="text-orgfa14130">
<p>
Es consecuencia inmediata del Teorema de cambio de variables para \(\mathbf{y} =
g(\mathbf{x}) = B\mathbf{x}\). Debido a que B es una matriz ortogonal, \(g^{‚àí1}
(\mathbf{y}) = B^T\mathbf{y} y J_{g^{-1}}(\mathbf{y}) =
\operatorname{det}\left(B^{T}\right)=\pm 1\)
</p>

<p>
\[\begin{aligned} f_{\underline{Y}}(\mathbf{y}) &=f_{\underline{X}}\left(B^{T}
\mathbf{y}\right)\left|\operatorname{det}\left(B^{T}\right)\right|=\frac{1}{(2
\pi)^{n / 2} \sigma^{n}} \exp \left(-\frac{1}{2 \sigma^{2}}\left\|B^{T}
\mathbf{y}\right\|_{2}^{2}\right)\left|\operatorname{det}\left(B^{T}\right)\right|
\\ &=\frac{1}{(2 \pi)^{n / 2} \sigma^{n}} \exp \left(-\frac{1}{2
\sigma^{2}}\|\mathbf{y}\|_{2}^{2}\right) \end{aligned}\]
</p>

<p>
En la √∫ltima igualdad usamos que \(\left\|B^{T}
\mathbf{y}\right\|_{2}=\|\mathbf{y}\|_{2}\) debido a que las transformaciones
ortogonales preservan longitudes.
</p>
</div>
</div>
</div>
<div id="outline-container-org83c5f34" class="outline-3">
<h3 id="org83c5f34">Demostraci√≥n del Teorema.</h3>
<div class="outline-text-3" id="text-org83c5f34">
<p>
Sin perder generalidad se puede suponer que &mu; = 0. Sea B =
\(\mathcal{B}=\left\{b_{1}, b_{2}, \ldots, b_{n}\right\}\) una base ortonormal de
\Re<sup>n</sup>, donde \(b_{1}=\frac{1}{\sqrt{n}}[1 \ldots 1]^{T}\) . Sea B &isin; \Re<sup>n
&times; n</sup> la matriz ortogonal cuya i-√©sima fila es b<sub>i</sub><sup>T</sup>. De acuerdo con el Lema
6.4 el vector aleatorio \underline{Y} = [Y<sub>1</sub> &hellip; Y<sub>n</sub>]<sup>T</sup> = B\underline{X} tiene
la misma distribuci√≥n que \underline{X} .
</p>

<p>
En primer lugar, observamos que
</p>

<p>
\[Y_{1}=b_{1}^{T} \underline{X}=\frac{1}{\sqrt{n}} \sum_{i=1}^{n} X_{i}=\sqrt{n}(\overline{X})\]
</p>

<p>
En segundo lugar,
</p>

<p>
\(\sum_{i=1}^{n} Y_{i}^{2}=\underline{Y}^{T} \underline{Y}=(B \underline{X})^{T}
B \underline{X}=\underline{X}^{T} B^{T} B \underline{X}=\underline{X}^{T}
\underline{X}=\sum_{i=1}^{n} X_{i}^{2}\)
</p>

<p>
En consecuencia,
</p>

<p>
\[\sum_{i=2}^{n} Y_{i}^{2}=\sum_{i=1}^{n} X_{i}^{2}-Y_{1}^{2}=\sum_{i=1}^{n}
X_{i}^{2}-n \overline{X}^{2}=\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}\]
</p>

<p>
Las variables Y<sub>1</sub>, &hellip; , Y<sub>n</sub> son independientes. Como \sqrt{n}(\overline{X})
depende de Y<sub>1</sub>, mientras que \(\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}\)
depende de Y<sub>2</sub>, &hellip; , Y<sub>n</sub>, resulta que \overline{X} y S<sup>2</sup> son independientes
(lo que prueba la parte (c)). Adem√°s, \sqrt{n}(\overline{X}) = Y<sub>1</sub> &sim; N(0,
&sigma;<sup>2</sup>), por lo tanto Z = \frac{\sqrt{n}(\overline{X})}{&sigma;} &sim; N(0, 1)
(lo que prueba la parte (a)). La parte (b) se deduce de que
</p>

<p>
\[\frac{(n-1) S^{2}}{\sigma^{2}}=\frac{1}{\sigma^{2}}
\sum_{i=1}^{n}\left(X_{i}-\overline{X}\right)^{2}=\sum_{i=2}^{n}\left(\frac{Y_{i}}{\sigma}\right)^{2}
\sim \chi_{n-1}^{2}\]
</p>

<p>
pues las n ‚àí 1 variables Y<sub>2/&sigma;</sub>, &hellip; , Y<sub>n/&sigma;</sub> son independientes y
con distribuci√≥n N(0, 1).
</p>
</div>
</div>
</div>
<div id="outline-container-org17ac584" class="outline-2">
<h2 id="org17ac584">Bibliograf√≠a consultada</h2>
<div class="outline-text-2" id="text-org17ac584">
<p>
Para redactar estas notas se consultaron los siguientes libros:
</p>
<ol class="org-ol">
<li>Bolfarine, H., Sandoval, M. C.: Introdu¬∏cÀúao `a InferÀÜencia Estat√≠stica. SBM,
Rio de Janeiro. (2001).</li>
<li>Borovkov, A. A.: Estad√≠stica matem√°tica. Mir, Mosc√∫. (1984).</li>
<li>Cramer, H.: M√©todos matem√°ticos de estad√≠stica. Aguilar, Madrid. (1970).</li>
<li>Hoel P. G.: Introducci√≥n a la estad√≠stica matem√°tica. Ariel, Barcelona.
(1980).</li>
<li>Lehmann, E. L .: Elements of Large-Sample Theory. Springer, New York. (1999)</li>
<li>Maronna R.: Probabilidad y Estad√≠stica Elementales para Estudiantes de
Ciencias. Editorial Exacta, La Plata. (1995).</li>
<li>Meyer, P. L.: Introductory Probability and Statistical Applications.
Addison-Wesley, Massachusetts. (1972).</li>
<li>Walpole, R. E.: Probabilidad y estad√≠stica para ingenieros, 6a. ed., Prentice
Hall, M√©xico. (1998)</li>
</ol>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara">
La noci√≥n de base ortonormal respecto del producto interno can√≥nico en \Re<sup>n</sup> y
la noci√≥n de matriz ortogonal.
</p>

<p class="footpara">
Si lo desea, aunque no es del todo cierto, puede pensar que las matrices
ortogonales corresponden a rotaciones espaciales.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2">2</a></sup> <div class="footpara"><p class="footpara">
Sobre la nomenclatura: Los vectores de \Re<sup>n</sup> se piensan como vectores columna y se notar√°n en negrita
\(\mathbf{x} = [x_1 \dots x_n]^T\).
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
Last update: 2019-06-18 00:54
</div>
</body>
</html>
