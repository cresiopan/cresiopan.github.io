<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2019-03-18 Mon 00:16 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Ensayos Bernoulli</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/res/org.css"/>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "left",
        displayIndent: "0",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "%LINEBREAKS" },
                        webFont: "%FONT"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "%LINEBREAKS" },
              font: "%FONT"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "%AUTONUMBER"},
               MultLineWidth: "%MULTLINEWIDTH",
               TagSide: "right",
               TagIndent: "%TAGINDENT"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_CHTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Ensayos Bernoulli</h1>
<div id="outline-container-org1e19b13" class="outline-2">
<h2 id="org1e19b13">Ensayos Bernoulli</h2>
<div class="outline-text-2" id="text-org1e19b13">
<p>
Se trata de ensayos repetidos en forma independiente en los que hay
sólo dos resultados posibles, usualmente denominados /"éxito"/y
/"fracaso'', cuyas probabilidades, p y 1 − p, se mantienen constantes
a lo largo de todos los ensayos.  El espacio muestral de cada ensayo
individual está formado por dos puntos S y F . El espacio muestral de
n ensayos Bernoulli contiene 2 n
</p>

<p>
puntos o secuencias de n símbolos S y F , cada punto representa un
resultado posible del experimento compuesto. Como los ensayos} son
independientes las probabilidades se multiplican. En otras palabras,
la probabilidad de cada sucesión particular es el producto que se
obtiene reemplazando los símbolo s S y F por p y 1 − p,
respectivamente. Así,
</p>

<p>
\mathbb{P}(SSF SF &hellip; F F S) = pp(1 − p)p(1 − p)  &ctdot;  (1 − p)(1 − p)p.
</p>
</div>
<div id="outline-container-orgd482354" class="outline-5">
<h5 id="orgd482354">Ejemplo 1.1.</h5>
<div class="outline-text-5" id="text-orgd482354">
<p>
Si repetimos en forma independiente un experimento aleatorio y estamos
in teresados en la ocurrencia del evento A al que consideramos
/"éxito'', tenemos ensayos Bernoulli con p = \mathbb{P}(A).
</p>
</div>
</div>

<div id="outline-container-org3f58586" class="outline-5">
<h5 id="org3f58586">Modelando ensayos Bernoulli.</h5>
<div class="outline-text-5" id="text-org3f58586">
<p>
Los ensayos Bernoulli (con probabilidad de éxito p) se describen
mediante una sucesión de variables aleatorias independientes e
idénticamente distribuidas \((X_i: i \in N )\) cada una con distribución
\(Bernoulli(p)\),
</p>

<p>
\mathbb{P}(X}
i
= x
i
) = p
x
i
(1 − p)
1{−x}
i
, x
i
&isin; \{0, 1}\. (1)}
Esto es, \mathbb{P}(X
i
= 1) = p y \mathbb{P}(X
i
= 0) = 1 − p}. En este contexto, X
i
= 1 significa que /"el}
resultado del i-ésimo ensayo es éxito''.
</p>
</div>
</div>

<div id="outline-container-org500c0ab" class="outline-5">
<h5 id="org500c0ab">Preguntas elementales.</h5>
<div class="outline-text-5" id="text-org500c0ab">
<p>
Se pueden formular varios tipos de preguntas relacionadas con los
ensayos Bernoulli. Las más sencillas son las siguientes:
</p>
<ol class="org-ol">
<li>¿Cuál es la cantidad total de éxitos en los primeros n ensayos?</li>
<li>¿En n ensayos, cuál es el número de éxitos más probable?</li>
<li>¿Cuánto /"tiempo"/hay que esperar para observar el primer éxito?</li>
<li>¿Cuánto /"tiempo"/hay que esperar para observar el k-ésimo éxito?</li>
</ol>

<p>
En lo que sigue expresaremos las preguntas (a)-(d) en términos de las variables aleatorias
X
i
, i &ge; 1, que describen los ensayos Bernoulli.
</p>

<p>
La cantidad de éxitos en los primeros n ensayos se describe mediante
la suma de las primeras variables X<sub>1</sub>, &hellip; , X<sub>n</sub>
</p>

<p>
S<sub>n</sub>
:=
n
X
{i=1}
X
i
. (2)
3
La pregunta (a) interroga por la distribución de probabilidades de la variable aleatoria
S<sub>n</sub>
definida en (2). Esto es, para cada k = 0, &hellip; , n, se trata de determinar cuánto valen
las probabilidades \mathbb{P}(S<sub>n</sub>
= k). En cambio, la pregunta (b) interroga por el valor de k que
maximiza a la función de k, \mathbb{P}(S<sub>n</sub>
= k).
El tiempo de espera hasta el primer é xito se describe mediante la variable aleatoria
T<sub>1</sub>
:= mín\{i &isin; N : X
i
= 1{\, (3)
y en general, el tiempo de espera h ast a el k-ésimo éxito, k &ge; 1 se describe, recursivamente,
mediante
T
k
:= mín\{i &gt; T
k{−{1
</p>
<pre class="example">
X

</pre>
<p>
i
= 1{\. (4)
La pregunta (c) interroga por la distribución de probabilidades de la variable T}
1
definida en
(3): cuánto valen las probabilidades \mathbb{P}(T}
1
= n), n &isin; N ? Finalmente, la pregunta (d) interroga
por la distribución de probabilidades de las variables T}
k
, k &ge; 2, definidas en (4): cuánto valen
las probabilidades \mathbb{P}(T}
k
= n), n &ge; k}?
</p>
</div>
</div>
<div id="outline-container-orga3239b3" class="outline-3">
<h3 id="orga3239b3">La distribución binomial: cantidad de éxitos en n ensayos</h3>
<div class="outline-text-3" id="text-orga3239b3">
<p>
La cantidad de éxitos puede ser 0, 1, &hellip; , n. El primer problema es
determinar las corre spondientes probabilidades. El evento en n
ensayos resultaron k éxitos y n − k fra casos
</p>

<p>
(
(X<sub>1</sub>
, &hellip; , X
n
) = (x
1
, &hellip; , x
n
) :
n
X
{i=1}
x
i
= k
)
puede ocurrir de tantas formas distintas como k símbolos 1 se puedan ubicar en n lugares.
En otras palabras, el evento considerado contiene

n
k

puntos, cada uno de probabilidad
P
n
 &setminus; 
{i=1}
\{X
i
= x
i
\}
!
=
n
Y
{i=1}
p
x
i
(1 − p)
1{−x}
i
= p
P
n
{i=1}
x
i
(1 − p)
n{−}
P
n
{i=1}
x
i
= p
k
(1 − p)
n{−}k
.
Por lo tanto,
\mathbb{P}(S<sub>n</sub>
= k) =
</p>

<p>
n
k

p
k
(1 − p)
n{−}k
0 &le; k &le; n. (5)
En particular, la probabilidad de que no ocurra ningún éxito en n ensayos es (1 − p)
n
y la
probabilidad de que ocurra al menos un éxito es 1 − (1 − p)
n
.
</p>

<p>
La distribución de \(S_n\), determinada en (5), se denomina la
distribución binomial de parámetros n y p y se denota Binomial(n, p).
</p>
</div>
<div id="outline-container-orgc01d3d3" class="outline-5">
<h5 id="orgc01d3d3">Nota Bene</h5>
<div class="outline-text-5" id="text-orgc01d3d3">
<p>
Por definición, la distribución binomial de parámetros n y p es la
distribución de una suma de n variables aleatorias independientes cada
con distribución Bernoulli de parámetro p.
</p>
</div>
</div>

<div id="outline-container-orgfe145eb" class="outline-5">
<h5 id="orgfe145eb">Ejemplo 1.2</h5>
<div class="outline-text-5" id="text-orgfe145eb">
<p>
Se tira un dado equilibrado 11 veces y en cada tiro se apuesta al 6,
¿cuál es la probabilidad de ganar exactamente 2 veces? Como el dado es
equilibrado, la probabilidad de éxito es 1 / 6 y la cantidad de éxitos
en 11 tiros tiene distribución Binomial (11, 1 / 6). Por lo tanto, la
probabilidad requerida es
</p>

<p>
11 2  1 6  2  5 6  9 = 0.2960 &hellip;
</p>
</div>
</div>
<div id="outline-container-orgeb781c7" class="outline-5">
<h5 id="orgeb781c7">Ejemplo 1.3</h5>
<div class="outline-text-5" id="text-orgeb781c7">
<p>
Cada artículo producido por una máquina será defectuoso con
probabilidad 0.1, independientemente de los demás. En una muestra de
3, ¿cuál es la probabilidad de encontrar a lo sumo un defectuoso?
</p>

<p>
Si X es la cantidad de artículos defectuosos en la muestra, entonces
\(X \sim Binomial(3, 0.1)\).  En consecuencia,
</p>

<p>
\mathbb{P}(X &le; 1) = \mathbb{P}(X = 0) + \mathbb{P}(X = 1) =}
</p>

<p>
3
0

(0.1)
0
(0.9)
3
</p>
<ul class="org-ul">
<li></li>
</ul>

<p>
3
1

(0.1)
1
(0.9)
2
= 0.972.
</p>
</div>
</div>
<div id="outline-container-org886faf9" class="outline-5">
<h5 id="org886faf9">Ejemplo 1.4.</h5>
<div class="outline-text-5" id="text-org886faf9">
<p>
Un avión se mantendrá en vuelo mientras funcionen al menos el 50 % de
sus} motores. Si cada motor del avión en vuelo puede fallar con
probabilidad 1 − p independien temente de los demás, ¿para cuáles
valores de p &isin; (0, 1) es más seguro un avión de 4 motores que uno de
2?
</p>

<p>
Como cada motor puede fallar o funcionar independientemente de los
demás, la cantidad de motores que siguen funcionando es una variable
aleatoria con distribución binomial. La probabilidad de que un avión
de 4 motores realice un vuelo exitoso es
</p>


<p>
4
2

p
2
(1 − p)
2
</p>
<ul class="org-ul">
<li></li>
</ul>

<p>
4
3

p
3
(1 − p) +
</p>

<p>
4
4

p
4
= 6p
2
(1 − p)
2
</p>
<ul class="org-ul">
<li>4p</li>
</ul>
<p>
3
(1 − p) + p
4
,
mientras que la correspondiente probabilidad para un avión de 2 motores es
</p>

<p>
2
1

p(1 − p) +}
</p>

<p>
2
2

p
2
= 2p(1 − p) + p
2
.
En consecuencia, el avión de 4 motores es más seguro que el de 2 si
6p
2
(1 − p)
2
</p>
<ul class="org-ul">
<li>4p</li>
</ul>
<p>
3
(1 − p) + p
4
&gt; 2}p(1 − p) + p
2
lo que es equivalente a las siguientes expresiones simplificadas
3p
3
− 8p}
2
</p>
<ul class="org-ul">
<li>7{p − 2 &gt; 0 \iff 3(p − 2 / 3)(p − 1)}</li>
</ul>
<p>
2
&gt; 0 \iff p &gt; 2}/{3}. 
Por lo tanto, el avión de 4 motores es más seguro cuando la probabilidad de que cada motor
se mantenga en funcionamiento es mayor que 2 / 3, mientras que el avión de 2 motores es más
seguro cuando esa probabilidad es menor que 2 / 3.
</p>
</div>
</div>
<div id="outline-container-orgb056662" class="outline-5">
<h5 id="orgb056662">Ejemplo 1.5.</h5>
<div class="outline-text-5" id="text-orgb056662">
<p>
Si la probabilidad de éxito es p = 0.01, cuántos ensayos se deb en realizar para}
asegurar que la probabilidad de que ocurra por lo menos un éxito sea al menos 1 /}2?
Buscamos el menor entero n tal que 1 − (0.99)
n
&ge;
1
2
, o equivalentemente
1
2
&ge; (0.99)
n
.
Tomando logaritmos − log 2 &ge; n log(0.99) y despejando n resulta n &ge; − log(2)/ log(0.99) &asymp;}
68.96. Por lo tanto, n = 69.
5
</p>
</div>
</div>
</div>
<div id="outline-container-orgcfd76f9" class="outline-3">
<h3 id="orgcfd76f9">Término central</h3>
<div class="outline-text-3" id="text-orgcfd76f9">
<p>
De la fórmula (5) se puede ver que
\mathbb{P}(S<sub>n</sub>
= k)
\mathbb{P}(S<sub>n</sub>
= k − 1)
=

n
k

p
k
(1 − p)
n{−}k

n
k{−{1

p
k{−{1
(1 − p)
n{−}{k+1}
=
(k − 1)!(n − k + 1)!p
k{!(n − k)!(1 − p ) 
=
(n − k + 1)p
k(1 − p ) 
= 1 +
(n + 1)p − k}
k(1 − p ) 
. (6)
De (6) se deduce que \mathbb{P}(S<sub>n</sub>
= k) crece cuando k &lt; (n + 1)p y decrece cuando k &gt; (n + 1)p. Si
(n + 1)p es un número entero, entonces \mathbb{P}(S<sub>n</sub>
= (n + 1)p) = \mathbb{P}(S<sub>n</sub>
= (n + 1)p − 1). En otras
palabras, la cantidad más probable de éxitos en n ensayos es m := [(n + 1)p]. Salvo e
n el caso
en que m = (n + 1)p, donde también lo es m − 1.
Cuando p =
1
2
el resultado anterior se puede observar directamente en el triángulo de
Pascal: en el centro de las filas pares está el máximo. En la región central de las filas impares
hay dos máximos.
</p>
</div>
<div id="outline-container-org79657db" class="outline-5">
<h5 id="org79657db">Ejemplo 1.6.</h5>
<div class="outline-text-5" id="text-org79657db">
<p>
Se tira un dado equilibrado n veces y en cada tiro se apuesta al 6. ¿Cuál es la}
cantidad más probable de éxitos cuando n = 12? y cuando n = 11?
La cantidad de éxitos tiene distribución Binomial (n, p), donde p = 1 / 6. Cuando n = 12,
(n + 1)p = 13 / 6 = 2.16&#x2026; y entonces la cantidad más probable de éxitos es m = 2. Cuando
n = 11, (n + 1)p = 2 y entonces la cantidad más probable de éxitos es m = 1 o m = 2.
</p>
</div>
</div>
</div>
<div id="outline-container-org0799079" class="outline-3">
<h3 id="org0799079">La distribución geométrica: tiempo de espera hasta el primer éxito</h3>
<div class="outline-text-3" id="text-org0799079">
<p>
El tiempo que hay que esperar para observar el primer éxito en una sucesión de ensayos
Bernoulli puede ser n = 1, 2, &hellip; . El evento T}
1
= 1 significa que se obtuvo éxito en el primer
ensayo y tiene probabilidad p. Para cada n &ge; 2, el evento T}
1
= n significa que en los primeros
n − 1 ensayos se obtuvieron fracasos y que en el n-ésimo se obtuvo éxito, lo que tiene proba}
bilidad (1 − p)
n{−{1
p. Por lo tanto, la distribución de T<sub>1</sub>
es
\mathbb{P}(T<sub>1</sub>
= n) = (1 − p)
n{−{1
p, n &isin; N} . (7)
El evento T}
1
&gt; n significa que los primeros n ensayos de la sucesión resultaron fracaso. Por}
lo tanto,
\mathbb{P}(T<sub>1</sub>
&gt; n) = (1 − p ) 
n
, n &ge; 1} . (8)
La distribución de T}
1
se denomina distribución geométrica de parámetro p y se designa me- 
diante Geométrica(p).
</p>
</div>
<div id="outline-container-org87dc7de" class="outline-5">
<h5 id="org87dc7de">Ejemplo 1.7.</h5>
<div class="outline-text-5" id="text-org87dc7de">
<p>
Se arroja repetidamente un dado equilibrado. ¿Cuál es la probabilidad de}
que el primer 6 aparezca antes del quinto tiro?. La probabilidad de obtener 6 es 1 / 6 y la
cantidad de tiros hasta obtener el primer as tiene distribución Geométrica(1 / 6). Por lo tanto,
la probabilidad requerida es
1 / 6 + (5 / 6)(1 / 6) + (5 / 6)
2
(1 / 6) + (5 / 6)
3
(1 / 6) = (1 / 6)
</p>

<p>
1 − (5 / 6)
4
1 − (5 / 6)

= 1 − (5 / 6)
4
= 0.5177 &hellip;}
6
</p>
</div>
</div>
<div id="outline-container-orga96800b" class="outline-5">
<h5 id="orga96800b">Ejemplo 1.8 (Ocurrencias casi seguras).</h5>
<div class="outline-text-5" id="text-orga96800b">
<p>
Si al realizarse un experimento aleatorio un evento
A tiene probabilidad positiva de ocurrir, entonces en una sucesión de experimentos indepen
dientes el evento A ocurrirá casi seguramente.
En efecto, el tiempo de espera hasta que ocurra el evento A es una variable aleatoria T}
A
con distribución geométrica de parámetro p = \mathbb{P}(A). Si se observa que
\{T}
A
&gt; 1{\} ⊇ \}T
A
&gt; 2{\} ⊇ \}T
A
&gt; 3{\} ⊇  &ctdot; 
y que
\{T}
A
= &infin;\} =
 &setminus; 
n &ge; 1
\{T}
A
&gt; n{\}
y se usa la propiedad de continuidad de P, se obtiene que
\mathbb{P}(T
A
= &infin;) = P


 &setminus; 
n &ge; 1
\{T}
A
&gt; n{\}


= lim<sub>n  &rarr; &infin;</sub>
\mathbb{P}(T
A
&gt; n) = lím}
{n &rarr; &infin;}
(1 − p)
n
= 0.
Por lo tanto, \mathbb{P}(T}
A
&lt; &infin;) = 1.}
Pérdida de memoria
La variable aleatoria, T , con distribución geométrica de paráme tro p tiene la propiedad
de pérdida de memoria, 
\mathbb{P}(T &gt; n + m | T &gt; n) = \mathbb{P}(T &gt; m) n, m &isin; N (9)
La identidad (9) se obtiene de (8) y de la fórmula de probabilidad condicional:
\mathbb{P}(T &gt; n + m | T &gt; n) =}
\mathbb{P}(T &gt; n + m, T &gt; n)
\mathbb{P}(T &gt; n)
=
\mathbb{P}(T &gt; n + m)
\mathbb{P}(T &gt; n)
=
(1 − p)
n{+}m
(1 − p)
n
= (1 − p)
m
= \mathbb{P}(T &gt; m).
De hecho, la propiedad de pérdida de memoria definida en (9) caracteriza a la distribución
geométrica.
</p>
</div>
</div>
<div id="outline-container-orga147102" class="outline-5">
<h5 id="orga147102">Teorema 1.9.</h5>
<div class="outline-text-5" id="text-orga147102">
<p>
Si T es una variable aleatoria a valores en N con la propiedad de pérdida de}
memoria, entonces T &sim; Geométrica(p), donde p = \mathbb{P}(T = 1).
</p>
</div>
</div>
<div id="outline-container-orgaa04141" class="outline-5">
<h5 id="orgaa04141">Demostración</h5>
<div class="outline-text-5" id="text-orgaa04141">
<p>
Sea G(n) := \mathbb{P}(T &gt; n). Si T pierde memoria, tenemos que}
G ( n + m) = G ( n ) G ( m) (10)
De (10) sigue que G(2) = G(1)G(1) = G(1)
2
, G(3) = G(2)G(1) = G(1)
3
y en general
G ( n) = G(1)
n
cualquiera sea n &isin; N . En otros términos, la distribución de T es tal que
\mathbb{P}(T &gt; n) = G(1)
n
.
Por lo tanto,
\mathbb{P}(T = n) = \mathbb{P}(T &gt; n − 1) − \mathbb{P}(T &gt; n ) = G(1)
n{−{1
− G(1)}
n
= G(1)
n{−{1
(1 − G}(1)).
7
</p>
</div>
</div>
</div>
<div id="outline-container-orgb03738a" class="outline-3">
<h3 id="orgb03738a">La distribución Pascal: tiempo de espera hasta el k-ésimo éxito</h3>
<div class="outline-text-3" id="text-orgb03738a">
<p>
Si se quieren observar k-éxitos en una sucesión de ensayos Bernoulli lo mínimo que se
debe esperar es k ensayos. ¿Cuándo ocurre el evento T}
k
= n, n &ge; k}? El n-ésimo ensayo debe
ser éxito y en los n − 1 ensayos anteriores deben oc urrir exactamente k − 1 éxitos. Hay

n{−{1
k{−{1

formas distintas de ubicar k − 1 símbolos 1 en n − 1 lugares. Por lo tanto,
\mathbb{P}(T
k
= n) =
</p>

<p>
n − 1
k − 1

p
k
(1 − p)
n{−}k
n &ge; k. (11)
La distribución de T}
k
se denomina distribución Pascal de parámetros k y p y se designa 
mediante Pascal(k, p).
La dist ribu ción Pascal de parámetros k y p es la distribución de una suma de k variables
aleatorias independientes cada una con ley Geométrica(p). Lo cual es intuitivamente claro si}
se piensa en el modo que arribamos a su definición.
En efecto, definiendo T}
0
:= 0 vale que
T
k
=
k
X
{i=1}
(T}
i
− T}
{i-1}
).
Basta ver que para cada i = 1, &hellip; , k las diferencias T}
i
− T}
{i-1}
son independientes y todas se
distribuyen como T}
1
&sim; Geométrica(p). De acuerdo con la regla del producto}
P

&cap;
k
{i=1}
\{T}
i
− T}
{i-1}
= m
i
\}

= \mathbb{P}(T}
1
= m
1
)
&times;
n{−{1
Y
{i=2}
P

T
i
− T}
{i-1}
= m
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&cap;</td>
</tr>
</tbody>
</table>
<p>
{i-1}
{j=1}
\{T}
j
− T}
j{−{1
= m
j
\}

. (12)
Si se sabe que T}
1
= m
1
, &hellip; , T
{i-1}
− T}
i{−{2
= m
{i-1}
, entonces el evento T}
i
− T}
{i-1}
= m
i
depende
las variables aleatorias X
P
{i-1}
{j=1}
m
j
+1
, &hellip; , X
P
i
{j=1}
m
j
y equivale a decir que las primer as m
i
− 1}
de esas variables valen 0 y la última vale 1. En consecuencia,
P

T
i
− T}
{i-1}
= m
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&cap;</td>
</tr>
</tbody>
</table>
<p>
{i-1}
{j=1}
\{T}
j
− T}
j{−{1
= m
j
\}

= (1 − p)
m
i
−{1}
p. (13)
De (12) y (13) se deduce que
P

&cap;
k
{i=1}
\{T}
i
− T}
{i-1}
= m
i
\}

=
k
Y
{i=1}
(1 − p)
m
i
−{1}
p. (14)
De la factorización (14) se deduce que T}
1
, T<sub>2</sub>
− T}
1
, &hellip; , T
k
− T}
k{−{1
son independientes y que
cada una tiene distribución geométrica de parámetro p.
</p>
</div>
<div id="outline-container-org5ccce63" class="outline-5">
<h5 id="org5ccce63">Ejemplo 1.10.</h5>
<div class="outline-text-5" id="text-org5ccce63">
<p>
Lucas y Monk disputan la final de un campeonato de ajedrez. El primero}
que gane 6 partidas (no hay tablas) resulta ganador. La probabilidad de que Lucas gane
cada partida es 3 / 4. ¿Cuál es la probabilidad de que Lucas gane el campeonato en la novena
partida? La cantidad de partidas que deben jugarse hasta que Lucas gane el campeonato tiene
distribución Pascal(6, 3 / 4). Por lo tanto, la probabilidad requerida es
</p>

<p>
8
5

3
4

6
</p>

<p>
1
4

3
= 0.1557 &hellip;}
8
</p>
</div>
</div>
<div id="outline-container-org74b1a6c" class="outline-5">
<h5 id="org74b1a6c">Ejemplo 1.11.</h5>
<div class="outline-text-5" id="text-org74b1a6c">
<p>
En una calle hay tres parquímetros desocupados. Se estima que en los próxi
mos 10 minutos pasarán 6 coches por esa calle y, en media, el 80 % tendrá que estacionarse
en alguno de ellos. Calcular la probabilidad de que los tres parquímetros sean ocupados en
los próximos 10 minutos.
La probabilidad requerida es la probabilidad de que la cantidad, N, de ensayos hasta el
tercer éxito sea menor o igual que 6. Como N tiene distribución Pascal(3, 0.8) resulta que
\mathbb{P}(N &le; 6) =}
6
X
{n=3}
\mathbb{P}(N = n) =}
6
X
{n=3}
</p>

<p>
n − 1
2

(0.8)
3
(0.2)
n{−{3
= (0.8)
3

2
2

(0.2)
0
</p>
<ul class="org-ul">
<li></li>
</ul>

<p>
3
2

(0.2)
1
</p>
<ul class="org-ul">
<li></li>
</ul>

<p>
4
2

(0.2)
2
</p>
<ul class="org-ul">
<li></li>
</ul>

<p>
5
2

(0.2)
3

= (0.8)
3

1 + 3(0.2) + 6(0.2)
2
</p>
<ul class="org-ul">
<li>10(0.2)</li>
</ul>
<p>
3

= 0.983 &hellip;}
Notar que una forma alternativa de obtener el mismo resultado es sumar las probabilidades
de observar 3, 4, 5, 6 éxitos en 6 ensayos Bernoulli.
Relación entre las distribuciones B inomial y Pascal. Sean S<sub>n</sub>
&sim; Binomial(n, p) y}
T
k
&sim; Pascal(k, p). Vale que}
\mathbb{P}(S<sub>n</sub>
&ge; k) = \mathbb{P}(T}
k
&le; n ) . (15)}
En efecto, decir que en n ensayos Bernoulli ocurren por lo menos k éxitos es l o mismo que
decir que el tiempo de espera hasta observar el k-ésimo éxito no supera a n.
</p>
</div>
</div>
</div>
<div id="outline-container-orga503202" class="outline-3">
<h3 id="orga503202">La distribución multinomial</h3>
<div class="outline-text-3" id="text-orga503202">
<p>
La distribución binomial se puede generalizar al caso de n ensayos independientes donde
cada ensayo puede tomar uno de varios resultados. Sean 1, 2, &hellip; , r los resultados posibles de
cada ensayo y supongamos que para cada k &isin; \}1, 2, &hellip; , r{\} la probabilidad p
k
de observar el
valor k se mantiene constante a lo largo de los ensayos. La pregunta es: ¿Cuántas veces o curre
cada uno de los resultados en los primeros n ensayos?
Consideramos una sucesión X<sub>1</sub>
, X<sub>2</sub>
, &hellip; de variables aleatorias independientes e idénti
camente distribuidas a valores \1, 2, &hellip; , r{\} tal que \mathbb{P}(X
i
= k) = p
k
. Fijado n, para cada
k = 1, &hellip; , r definimos la variables M
k
=
P
n
{i=1}
1\{X}
i
= k{\} . La variable M}
k
cuenta la cantidad
de veces que ocurre el resultado k en n ensayos. La probabilidad de que en n ensayos el
resultado 1 ocurra m
1
veces, el resultado 2 ocurra m
2
veces, etc. es
\mathbb{P}(M
1
= m
1
, M
2
= m
2
, &hellip; , M
r
= m
r
) =
n{!}
m
1
!m
2
!  &ctdot;  m}
r
!
p
m
1
1
p
m
2
2
 &ctdot;  p
m
r
r
, (16)
donde los m
k
son enteros no negativos sujetos a la condición m
1
</p>
<ul class="org-ul">
<li>m</li>
</ul>
<p>
2
</p>
<ul class="org-ul">
<li>&ctdot;  + m</li>
</ul>
<p>
r
= n.
Si r = 2, entonces (16) se reduce a la distribución Binomial con p
1
= p, p
2
= 1 − p, k
1
= k
y k
2
= n − k .
9
\hypertarget{pfa}
</p>
</div>
</div>
<div id="outline-container-org7d20702" class="outline-3">
<h3 id="org7d20702">j Miscelánea de ejemplos</h3>
<div class="outline-text-3" id="text-org7d20702">
</div>
<div id="outline-container-org97b7194" class="outline-5">
<h5 id="org97b7194">Observación 1.12 (Desarrollo de Taylor).</h5>
<div class="outline-text-5" id="text-org97b7194">
<p>
Para todo x &isin; (0, 1) vale que}
1
(1 − x)
{k+1}
=
X
n &ge; 0
</p>

<p>
n + k
k

x
n
. (17)
La identidad (17) se obtiene desarrollando la función h(x) = (1 − x)
−(k+1)
en serie de
Taylor alrededor del 0: observando que h
(n)
(0) = (k + 1)(k + 2)  &ctdot;  (k + n), se obtiene que
h
(n)
(0)
n{!}
=

n{+}k
k

.
</p>
</div>
</div>
<div id="outline-container-orga7b16c5" class="outline-5">
<h5 id="orga7b16c5">Ejemplo 1.13</h5>
<div class="outline-text-5" id="text-orga7b16c5">
<p>
(Variable compuesta). Sean N
1
; X<sub>1</sub>
, X<sub>2</sub>
, &hellip; una sucesión de variables aleato
rias independientes. Supongamos que N}
1
&sim; Geométrica(p}
1
) y que X
i
&sim; Bernoulli(p}
2
), i &ge; 1.
Entonces,
N
2
=
N
1
−{1}
X
{i=1}
X
i
&sim; Geométrica}
</p>

<p>
p
1
p
1
</p>
<ul class="org-ul">
<li>p</li>
</ul>
<p>
2
(1 − p}
1
)

− 1. (18)
Por definición N}
2
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{N}</td>
</tr>
</tbody>
</table>
<p>
1
= n &sim; Binomial(n − 1, p}
2
). Aplicando la fórmula de probabilidad total
obtenemos
\mathbb{P}(N
2
= k) =
X
n &ge; 1
\mathbb{P}(N
2
= k | N}
1
= n)\mathbb{P}(N}
1
= n)
=
X
n &ge; {k+1}
</p>

<p>
n − 1
k

p
k
2
(1 − p}
2
)
n{−{1}−}k
(1 − p}
1
)
n{−{1
p
1
=
X
m &ge; 0
</p>

<p>
m + k
k

p
k
2
(1 − p}
2
)
m
(1 − p}
1
)
m{+}k
p
1
= (p
2
(1 − p}
1
))
k
p
1
X
m &ge; 0
</p>

<p>
n + k
k

[(1 − p}
1
)(1 − p}
2
)]
m
. (19)
Usando (17) vemos que
X
m &ge; 0
</p>

<p>
m + k
k

[(1 − p}
1
)(1 − p}
2
)]
m
=
1
(1 − (1 − p}
1
)(1 − p}
2
))
{k+1}
=
1
(p
1
</p>
<ul class="org-ul">
<li>p</li>
</ul>
<p>
2
(1 − p}
1
))
{k+1}
. (20)
Combinando (19) y (20) obtenemos que
\mathbb{P}(N
2
= k) =
(p
2
(1 − p}
1
))
k
p
1
(p
1
</p>
<ul class="org-ul">
<li>p</li>
</ul>
<p>
2
(1 − p}
1
))
{k+1}
=
</p>

<p>
p
2
(1 − p}
1
)
p
1
</p>
<ul class="org-ul">
<li>p</li>
</ul>
<p>
2
(1 − p}
1
)

k
</p>

<p>
p
1
p
1
</p>
<ul class="org-ul">
<li>p</li>
</ul>
<p>
2
(1 − p}
1
)

. (21)
</p>
</div>
</div>
<div id="outline-container-org1735eed" class="outline-5">
<h5 id="org1735eed">Ejemplo 1.14</h5>
<div class="outline-text-5" id="text-org1735eed">
<p>
(Rachas). Para cada número entero m &gt; 1 sea Y}
m
la cantidad de ensayos
Bernoulli(p) que se deben realizar hasta obtener por primera vez una r acha de m éxitos segui- 
dos. En lo que sigue vamos a calcular E[Y
m
] mediante condicionales. Para ello introducimos
10
\hypertarget{pfb}
una variable aleatoria auxiliar N que cuenta la cantidad de ensayos que deben realizarse hasta
obtener por primera vez un fracaso y usaremos la identidad E[Y
m
] = E[E[Y
m
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{N]].}</td>
</tr>
</tbody>
</table>
<p>
Observando que
Y
m
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{N = n &sim;</td>
</tr>
</tbody>
</table>
<p>

n + Y
m
si n &le; m,}
m si n &gt; m,
obtenemos la expresión de la función de regresión
&varphi; ( n) = E[Y
m
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{N = n ] =</td>
</tr>
</tbody>
</table>
<p>

n + E[Y
m
] si n &le; m,}
m si n &gt; m.
En consecuencia, E[Y
m
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{N] = N{1} \{N &le; m\} + E[Y}</td>
</tr>
</tbody>
</table>
<p>
m
]1\{N &le; m\} + m{1{\}N &gt; m{\}, de donde se
deduce que E[Y
m
] = E[N{1{\}N &le; m{\] + E[Y
m
]\mathbb{P}(N &le; m) + m\mathbb{P}(N &gt; m). Equivalentemente,
E[Y}
m
] =
E[N{1{\}N &le; m{\]
\mathbb{P}(N &gt; m)
</p>
<ul class="org-ul">
<li>m. (22)</li>
</ul>
<p>
Debido a que N 1{\}N &le; m{\} = N − N{1{\}N &gt; m{\} el primer término del lado derecho de la
igualdad (22) se puede expresar de siguiente forma
E[N{1{\}N &le; m{\]
\mathbb{P}(N &gt; m)
=
E[N] − E[N 1{\}N &gt; m{\]
\mathbb{P}(N &gt; m)
=
E[N]
\mathbb{P}(N &gt; m)
− E[N |{N &gt; m]
=
E[N]
\mathbb{P}(N &gt; m)
− E[N ] − m. (23)}
La última igualdad se deduce de la propiedad de pérdida de memoria de la distribución
Geométrica. De N | N &gt; m &sim; m + N, resulta que E[N | N &gt; m] = m + E[N].
Combinando (22) y (23) obtenemos
E[Y}
m
] =
E[N]
\mathbb{P}(N &gt; m)
− E[N ] =
E[N]\mathbb{P}(N &le; m)
\mathbb{P}(N &gt; m)
=
1 − p}
m
(1 − p)p
m
. (24)
</p>
</div>
</div>
<div id="outline-container-org93c03ec" class="outline-5">
<h5 id="org93c03ec">Ejemplo 1.15</h5>
<div class="outline-text-5" id="text-org93c03ec">
<p>
(Coleccionista I). Sea M una variable aleatoria a valores 1, 2, &hellip; , m}. Sea}
(M}
n
</p>
<pre class="example">
n \in N ) una sucesión de variables aleatorias i ndependientes tal que M}

</pre>
<p>
n
&sim; M para
todo n &isin; N . Sea K = mín\{n &ge; m : \{M
1
, &hellip; , M
n
\} = \{1, 2, &hellip; , m}\}\} el tamaño de muestra}
mínimo que se necesita para /"coleccionar"/todos los valores 1, 2, &hellip; , m}. En lo que sigue vamos
a calcular E[K] mediante condicionales. Introducimos un elemento aleatorio C que indica el
orden en que se obtuvieron los valores 1, 2, &hellip; , m y usamos la identidad E[K] = E[E[K | C]].
Sea S(m) al conjunto de todas las permutaciones de los números 1, 2, &hellip; , m}. Para cada
permutación &sigma; = ( &sigma; 
1
, &sigma;
2
, &hellip; , &sigma;
m
) &isin; S}(m) vale que:
\mathbb{P}(C = &sigma;) =}
m{−{1
Y
{k=1}
\mathbb{P}(M = &sigma;
k
)
P
m
{i=k}
\mathbb{P}(M = &sigma;
i
)
.
Por otra parte
K | C = &sigma; &sim; 1 +
m{−{1
X
{k=1}
N ( &sigma;
i
</p>
<pre class="example">
1 \leq 1 \leq k), 

</pre>
<p>
11
\hypertarget{pfc}
donde N( &sigma; 
i
</p>
<pre class="example">
1 \leq i \leq k) \sim Geométrica

</pre>
<p>

P
m
{i=k+1}
\mathbb{P}(M = &sigma;
i
)

. Por lo tanto,
E[K] =}
X
{&sigma; &isin; S(m)}
E[K | C = &sigma;]\mathbb{P}(C = &sigma;)
=
X
{&sigma; &isin; S(m)}
1 +
m{−{1
X
{k=1}
1
P
m
{i=k+1}
\mathbb{P}(M = &sigma;
i
)
!
m{−{1
Y
{k=1}
\mathbb{P}(M = &sigma;
k
)
P
m
{i=k}
\mathbb{P}(M = &sigma;
i
)
. (25)
En el caso particular en que \mathbb{P}(M = i) = 1{/m para todo i &isin; \}1, 2, &hellip; , m{\} tenemos que
E[K] =}
X
{&sigma; &isin; S(m)}
1 +
m{−{1
X
{k=1}
1
P
m
{i=k+1}
1{/m}
!
m{−{1
Y
{k=1}
1{/m}
P
m
{i=k}
1{/m}
= m!
1 +
m{−{1
X
{k=1}
1
P
m
{i=k+1}
1{/m}
!
1
m{!}
=
m{−{1
X
{k=0}
1
P
m
{i=k+1}
1{/m}
= m
m
X
{i=1}
1
i
. (26)
</p>
</div>
</div>
<div id="outline-container-org5b41020" class="outline-5">
<h5 id="org5b41020">Ejemplo 1.16</h5>
<div class="outline-text-5" id="text-org5b41020">
<p>
(Coleccionista II). Sea X}
1
, X<sub>2</sub>
, &hellip; una sucesión de variables aleatorias inde
pendientes e idénticamente distribuidas a valores 1, 2, &hellip; , r}. Sea N}
r
= mín\{n &ge; 1 : X
n
= r{\} .
Para cada i = 1, &hellip; , r − 1 sea M}
i
=
P
N
r
−{1}
{n=1}
1\{X}
n
= i{\} . Queremos hallar la función de
probabilidad de M}
i
.
Por definición N}
r
&sim; Geométrica(p}
r
) y M}
i
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{N}</td>
</tr>
</tbody>
</table>
<p>
r
= n &sim; Binomial

n − 1}, p
i
(1 − p}
r
)
−{1}

. De
acuerdo con el Ejemplo 1.13 
tenemos que
M
i
&sim; Geométrica}
</p>

<p>
p
r
p
r
</p>
<ul class="org-ul">
<li>p</li>
</ul>
<p>
i
(1 − p}
r
)
−{1}
(1 − p}
r
)

− 1 = Geométrica}
</p>

<p>
p
r
p
r
</p>
<ul class="org-ul">
<li>p</li>
</ul>
<p>
i

− 1.
En particular, E[M}
i
] = p
i
/p
r
y V(M}
i
) = p
i
(p
r
</p>
<ul class="org-ul">
<li>p</li>
</ul>
<p>
i
)/p}
2
r
.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orge57f3b5" class="outline-2">
<h2 id="orge57f3b5">La distribución de Poisson</h2>
<div class="outline-text-2" id="text-orge57f3b5">
</div>
<div id="outline-container-org6e37399" class="outline-3">
<h3 id="org6e37399">Motivación: Aproximación de Poisson de la distribución binomial</h3>
<div class="outline-text-3" id="text-org6e37399">
<p>
En diversas aplicaciones tenemos que tratar con ensayos Bernoulli donde, para decirlo
de algún modo, n es grande y p es pequeño, mientras que el producto &lambda; = np es modera
do. En tales casos conviene usar una aproximación de las probabilidades \mathbb{P}(S<sub>n</sub>
= k), donde
S<sub>n</sub>
&sim;{Binomial(n, p) y p = &lambda;/n}. Para k = 0 tenemos}
\mathbb{P}(S<sub>n</sub>
= 0) = (1 − p)
n
=
</p>

<p>
1 −}
&lambda;
n

n
. (27)
Tomando logaritmos y usando el desarrollo de Taylor,
log(1 − t) = −t −}
1
2
t
2
−
1
3
t
3
−
1
4
t
4
−  &ctdot;  , 
se obtiene
log \mathbb{P}(S<sub>n</sub>
= 0) = n log
</p>

<p>
1 −}
&lambda;
n

= −{&lambda; −}
&lambda;
2
2n
−  &ctdot;  (28)
12
\hypertarget{pfd}
En consecuencia, para n grande se tiene que
\mathbb{P}(S<sub>n</sub>
= 0) &asymp; e}
− &lambda; 
, (29)
donde el signo &asymp; se usa para indicar una igualdad aproximada (en este caso de orden de
magnitud 1{/n). Más aún, usando la identidad (6) se puede ver que para cada k fijo y n
suficientemente grande
\mathbb{P}(S<sub>n</sub>
= k)
\mathbb{P}(S<sub>n</sub>
= k − 1)
=
(n − k + 1)p
k(1 − p ) 
&asymp;
&lambda;
k
. (30)
Recursivamente se concluye que
\mathbb{P}(S<sub>n</sub>
= 1) &asymp; &lambda; · \mathbb{P}(S<sub>n</sub>
= 0) &asymp; &lambda; e
− &lambda; 
,
\mathbb{P}(S<sub>n</sub>
= 2) &asymp;}
&lambda;
2
· \mathbb{P}(S 
n
= 1) &asymp;}
&lambda;
2
2
e
− &lambda; 
,
y en general
\mathbb{P}(S<sub>n</sub>
= k) &asymp;}
&lambda;
k
k{!}
e
− &lambda; 
. (31)
La igualdad aproximada (31) se llama la apro
ximación de Poisson de la distribución binomial.
0 2 4 6 8 10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
Figura 1: Comparación. Funciones de probabilidad de las distribuciones Binomial(10, 1 / 5)
(bolita negra) y Poisson(2) (cuadradillo vacío).
Otro modo de obtener el mismo resultado.
\mathbb{P}(S<sub>n</sub>
= k) =
</p>

<p>
n
k

p
k
(1 − p)
n{−}k
&sim;
1
k{!}
</p>

<p>
np
1 − p}

k
(1 − p)
n{−}k
&rarr;
&lambda;
k
k{!}
e
− &lambda; 
.
13
\hypertarget{pfe}
</p>
</div>
<div id="outline-container-org5b48839" class="outline-5">
<h5 id="org5b48839">Ejemplo 2.1 (Artículos defectuosos)</h5>
<div class="outline-text-5" id="text-org5b48839">
<p>
Una industria produce tornillos. Supongamos que la}
probabilidad de que un tornillo resulte defectuoso se a p = 0.015, entonces la probabilidad de
que una caja de 100 tornillos no contenga ninguno defectuoso es (0.985)
100
= 0.2206&#x2026; La
aproximación de Poisson es e
−{1.5}
= 0.2231&#x2026; y es suficientemente próx ima para la mayoría de
los propósitos prácticos. Si se pregunta: Cuántos tornillos debería contener la c aja para que la
probabilidad de encontrar al menos 100 tornillos sin defectos sea 0 .8 o mejor? Si 100 + x es el
número buscado, entonces x es un número pequeño. Para aplicar la aproximación de Poisson
para n = 100 + x ensayos debemos poner &lambda; = np, pero np es aproximadamente 100p = 1.5.
Buscamos el menor entero x para el cual
e
−{1.5}
</p>

<p>
1 +
1.5
1
</p>
<ul class="org-ul">
<li>&ctdot; }</li>
</ul>
<p>
(1.5)
x
x{!}

&ge; 0.8 (32)
Para x = 1 el valor del lado izquierdo de la inecuación (32) es aproximadamente 0.558, para
x = 2 es aproximadamente 0.809. Por lo tanto, l
a aproximación de Poisson permite concluir}
que se necesitan 102 tornillos. En realidad la probabilidad de encontrar al menos 100 tornillos
sin defectos en una caja de 102 es 0.8022 &hellip; .
</p>
</div>
</div>
</div>
<div id="outline-container-orgfb8348c" class="outline-3">
<h3 id="orgfb8348c">La distribución Poisson</h3>
<div class="outline-text-3" id="text-orgfb8348c">
<p>
Sea &lambda; &gt; 0. Una variable aleatoria N tiene distribución Poisson( &lambda; ) si sus posibles valores
son los enteros no negativos y si
\mathbb{P}(N = n) = e}
− &lambda; 
&lambda;
n
n{!}
, n = 0, 1, &hellip; (33)
Media y varianza. Usando el desarrollo de Taylor de la función exponencial e}
x
=
P
&infin;
{n=0}
x
n
n{!}
se demuestra que E[N] = &lambda; y V(N ) = &lambda;}.
Aditividad. El rasgo más importante de la distribución Poisson es su aditividad.
</p>
</div>
<div id="outline-container-org9666649" class="outline-5">
<h5 id="org9666649">Teorema 2.2 (Aditividad). Si N</h5>
<div class="outline-text-5" id="text-org9666649">
<p>
1
y N}
2
son variables aleatorias independientes con distribu
ción Poisson de medias &lambda;}
1
y &lambda;}
2
, respectivamente. Entonces,
N
1
</p>
<ul class="org-ul">
<li>N}</li>
</ul>
<p>
2
&sim; P oisson ( &lambda;}
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
).
</p>
</div>
</div>
<div id="outline-container-orgceb2f76" class="outline-5">
<h5 id="orgceb2f76">Demostración.</h5>
<div class="outline-text-5" id="text-orgceb2f76">
<p>
\mathbb{P}(N
1
</p>
<ul class="org-ul">
<li>N}</li>
</ul>
<p>
2
= n) =
n
X
{m=0}
\mathbb{P}(N
1
= m, N}
2
= n − m) =
n
X
{m=0}
\mathbb{P}(N
1
= m)\mathbb{P}(N}
2
= n − m)
=
n
X
{m=0}
e
− &lambda; 
1
&lambda;
m
1
m{!}
e
− &lambda; 
2
&lambda;
n{−}m
2
(n − m)!
=
e
−( &lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)
n{!}
n
X
{m=0}
</p>

<p>
n
m

&lambda;
m
1
&lambda;
n{−}m
2
= e
−( &lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)
( &lambda; 
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
)
n
n{!}
.
14
\hypertarget{pff}
</p>
</div>
</div>
<div id="outline-container-org6864272" class="outline-5">
<h5 id="org6864272">Nota Bene</h5>
<div class="outline-text-5" id="text-org6864272">
<p>
El resultado del Teorema 2.2 se extiende por inducción a la suma de una}
cantidad finita de variables aleatorias independientes con distribución Poisson.
</p>
</div>
</div>
<div id="outline-container-orgf70fb68" class="outline-5">
<h5 id="orgf70fb68">Teorema 2.3 (Competencia). Sean N</h5>
<div class="outline-text-5" id="text-orgf70fb68">
<p>
1
, N
2
, &hellip; , N
m
variables aleatorias independientes, cada
N
j
con distribución Poisson de media &lambda;}
j
, respectivamente. Sea S = N}
1
</p>
<ul class="org-ul">
<li>&ctdot;  + N}</li>
</ul>
<p>
m
. Entonces,
para cada n &ge; 1 vale que
(N}
1
, N
2
, &hellip; , N
m
)|{S = n &sim; Multinomial}
</p>

<p>
n,
&lambda;
1
&lambda;
,
&lambda;
2
&lambda;
, &hellip; ,
&lambda;
m
&lambda;

,
donde &lambda; =
P
j
&lambda;
j
. En particular,
\mathbb{P}(N
j
= 1{|{S = 1) =
&lambda;
j
&lambda;
.
</p>
</div>
</div>
<div id="outline-container-org46752d1" class="outline-5">
<h5 id="org46752d1">Demostración</h5>
<div class="outline-text-5" id="text-org46752d1">
<p>
La suma S = N
1
+{ &ctdot; }+{N}
m
tiene distribución Poisson de media &lambda; =
P
j
&lambda;
j
;
y entonces siempre que n
1
</p>
<ul class="org-ul">
<li>&ctdot;  + n</li>
</ul>
<p>
m
= n,
\mathbb{P}(N
1
= n
1
, &hellip; , N
m
= n
m
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{S = n) =</td>
</tr>
</tbody>
</table>
<p>
\mathbb{P}(N
1
= n
1
, &hellip; , N
m
= n
m
)
\mathbb{P}(S = n)
=
Y
j
e
− &lambda; 
j
&lambda;
n
j
j
n
j
!
!,
</p>

<p>
e
− &lambda; 
&lambda;
n
n{!}

=
n{!}
n
1
!n
2
!  &ctdot;  n}
m
!
Y
j
</p>

<p>
&lambda;
j
&lambda;

n
j
.
</p>
</div>
</div>
<div id="outline-container-orgf12cf1c" class="outline-5">
<h5 id="orgf12cf1c">Nota Bene</h5>
<div class="outline-text-5" id="text-orgf12cf1c">
<p>
En el caso particular n = 2, el resultado del Teorema 2.3 se reduce a que,}
si N}
1
y N}
2
son variables aleatorias independientes con distribución Poisson de medias &lambda;}
1
y
&lambda;
2
, respectivamente, entonces, dado que N}
1
</p>
<ul class="org-ul">
<li>N}</li>
</ul>
<p>
2
= n, la distribución condicional de N}
1
es
Binomial(n, p), donde p =
&lambda;
1
&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
.
</p>
</div>
</div>
<div id="outline-container-orgc672709" class="outline-5">
<h5 id="orgc672709">Teorema 2.4 (Adelgazamiento). Sea N una variable aleatoria Poisson de media &lambda;}. Sea M</h5>
<div class="outline-text-5" id="text-orgc672709">
<p>
una variable aleatoria tal que
M | N = n &sim; Binomial ( n, p ) .
Entonces, M y N − M son variables aleatorias independientes con distribución Poisson de
medias p&lambda; y (1 − p) &lambda; , res pectivamente.
</p>
</div>
</div>
<div id="outline-container-org886517c" class="outline-5">
<h5 id="org886517c">Demostración</h5>
<div class="outline-text-5" id="text-org886517c">
<p>
Sean m, k &ge; 0}
\mathbb{P}(M = m, N − M = k) = \mathbb{P}(M = m, N − M = k | N = m + k)\mathbb{P}(N = m + k)
= \mathbb{P}(M = m | N = m + k)\mathbb{P}(N = m + k)
=
</p>

<p>
m + k
m

p
m
(1 − p)
k

e
− &lambda; 
&lambda;
m{+}k
(m + k)!
=
</p>

<p>
e
−{p&lambda;}
(p&lambda;)
m
m{!}

e
−(1}−{p ) &lambda;}
((1 − p) &lambda; )
k
k{!}

.
15
</p>
</div>
</div>
<div id="outline-container-org548e566" class="outline-5">
<h5 id="org548e566">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-org548e566">
<ol class="org-ol">
<li>Sea N una variable aleatoria con distribución Poisson de media &lambda;}. Mostrar que}</li>
</ol>
<p>
\mathbb{P}(N = n) =}
&lambda;
n
\mathbb{P}(N = n − 1), n = 1, 2, &hellip;
Usar ese resultado para encontrar el valor de n para el cual \mathbb{P}(N = n) es maximal.
</p>
<ol class="org-ol">
<li></li>
</ol>
<p>

Se lanza una moneda una cantidad aleatoria N de veces, donde N tiene distribución
Poisson. Sean N}
1
y N}
2
la cantidad de total de caras y de cecas observadas, respectivamente.
Mostrar que las variables aleatorias N}
1
y N}
2
son independientes y que tienen distribución
Poisson.
</p>
<ol class="org-ol">
<li>Sea X}</li>
</ol>
<p>
1
, X<sub>2</sub>
, &hellip; una sucesión de variables aleatorias independientes, cada una con distribu
ción Bernoulli(p). Para cada n &ge; 1 se define S<sub>n</sub>
:=
P
n
{i=1}
X
i
. Por convención, S}
0
:= 0. Sea N}
una variable aleatoria con distribución Poisson( &lambda; ). Mostrar que S}
N
&sim; Poisson(p&lambda;).
</p>
</div>
</div>
</div>
<div id="outline-container-org8e85918" class="outline-3">
<h3 id="org8e85918">e La aproximación Poisson. (Técnica de acoplamiento)</h3>
<div class="outline-text-3" id="text-org8e85918">
<p>
En lo que sigue mostraremos que cuando se consideran una gran cantidad de eventos inde
pendientes y cada uno de ellos tiene una probabilidad muy pequeña de ocurrir, la cantidad de
tales eventos que realmente ocurre tiene una distribución /"cercana"/a la distribución Poisson.
0 0.5 1 1.5 2 2.5 3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figura 2: Comparación de las funciones de probabilidad de las distribuciones Bernoulli(1 / 4)
(bolita negra) y Poisson(1 / 4) (cuadradillo vacío)
16
Construcción conjunta de variables Bernoulli y Poisson (Acoplamiento).
Para cada p &isin; [0, 1] dividimos el intervalo [0, 1) en dos intervalos
I
0
(p) = [0, 1 − p), I}
1
(p) = [1 − p, 1) (34)
y en la sucesión de intervalos
J
0
(p) = [0, e}
−p
), J}
k
(p) =


k{−{1
X
{j=0}
e
−p
p
k
k{!}
,
k
X
{j=0}
e
−p
p
k
k{!}


, k = 1, 2, &hellip; . (35)
Consideramos una variable aleatoria U con distribución U[0, 1) y construimos dos variables
aleatorias V y W con distribuciones Bernoulli(p) y Poisson(p), respectivamente:
V := 1{\}U &isin; I
1
(p)\, W :=
&infin;
X
{k=0}
k{1{\}U &isin; J
k
(p)\. (36)
De la desigualdad 1 −p &le; e}
−p
resulta que I}
0
(p) &sub; J
0
(p) y que J}
1
(p) &sub; I
1
(p). En consecuencia,
V = W \iff U &isin; I
0
(p) &cup; J
1
(p). Por ende,
\mathbb{P}(V = W ) = \mathbb{P}(U &isin; I
0
(p) &cup; J
1
(p)) = 1 − p + e
−p
p, (37)
y en consecuencia,
\mathbb{P}(V &ne; W ) = p − e
−p
p = p(1 − e
−p
) &le; p}
2
. (38)
Usando la desigualdad (38) pueden obtenerse las siguientes cotas:
sup
k &ge; 0
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{\mathbb{P}(V = k) − \mathbb{P}(W = k)}</td>
<td class="org-left">&le; p</td>
</tr>
</tbody>
</table>
<p>
2
, (39)
X
k
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{\mathbb{P}(V = k) − \mathbb{P}(W = k)}</td>
<td class="org-left">&le; 2p}</td>
</tr>
</tbody>
</table>
<p>
2
. (40)
La cota (39) se deduce de observar que
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{\mathbb{P}(V = k) − \mathbb{P}(W = k)}</td>
<td class="org-left">=</td>
<td class="org-left">{E[1 \{V = k}\] − E[1 \{W = k}\]</td>
</tr>
</tbody>
</table>
<p>
= |{E[1\{V = k{\} − 1}\W = k{\] | 
&le; E[|{1}\{V = k\} − 1}\{W = k\}|]
&le; E[1 \{V &ne; W \]
= \mathbb{P}(V &ne; W ).
La cota (40) se deduce de observar que para todo k = 0, 1, &hellip;}
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{\mathbb{P}(V = k) − \mathbb{P}(W = k)}</td>
<td class="org-left">=</td>
<td class="org-left">{\mathbb{P}(V = k, W &ne; k) − \mathbb{P}(W = k, V &ne; k)</td>
</tr>
</tbody>
</table>
<p>
&le; \mathbb{P}(V = k, V &ne; W ) + \mathbb{P}(W = k, V &ne; W ), 
y luego sumar sobre los posibles val ores de k:
X
k
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{\mathbb{P}(V = k) − \mathbb{P}(W = k)}</td>
<td class="org-left">&le; 2\mathbb{P}(V &ne; W ).</td>
</tr>
</tbody>
</table>
<p>
17
</p>
</div>
<div id="outline-container-org233d677" class="outline-5">
<h5 id="org233d677">Nota Bene</h5>
<div class="outline-text-5" id="text-org233d677">
<p>
Esta técnica, denominada técnica de acoplamiento de variables aleatorias,}
permite probar (sin usar la fórmula de Stirling) que la distribución Binomial converge a la
distribución Poisson.
</p>
</div>
</div>
<div id="outline-container-org22458e5" class="outline-5">
<h5 id="org22458e5">Teorema 2.5 (Le Cam). Sean X}</h5>
<div class="outline-text-5" id="text-org22458e5">
<p>
1
, &hellip; , X
n
variables aleatorias inde pendientes con distribu
ción Bernoulli de parámet ros p
1
, &hellip; , p
n
, respectivamente y sea S =
P
n
{i=1}
X
i
. Entonces
X
k
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{\mathbb{P}(S = k) − \mathbb{P}(N = k)}</td>
<td class="org-left">&le; 2}</td>
</tr>
</tbody>
</table>
<p>
n
X
{i=1}
p
2
i
, (41)
donde N es una variable aleatoria con distribución Poisson de media &lambda; =
P
n
{i=1}
p
i
.
</p>
</div>
</div>
<div id="outline-container-org8165131" class="outline-5">
<h5 id="org8165131">Demostración</h5>
<div class="outline-text-5" id="text-org8165131">
<p>
Sean U
1
, &hellip; , U
n
variables aleatorias independientes con distribución común
U[0, 1). Construimos variables aleatorias acopladas V
i
&sim; Bernoulli(p}
i
) y W}
i
&sim;{Poisson(p}
i
),
i = 1, &hellip; , n{:}
V
i
:= 1\{U 
i
&isin; I}
1
(p
i
)\, W
i
:=
&infin;
X
{k=0}
k{1{\}U
i
&isin; J}
k
(p
i
)\, 
y las sumamos
S
∗
=
n
X
{i=1}
V
i
, N =}
n
X
{i=1}
W
i
.
Por construcción, las variables V}
1
, &hellip; , V
n
son independientes y con distribución Bernoulli(p
i
),
respectivamente, y entonces, la variable S}
∗
tiene la misma distribución que S}; las variables
W
1
, &hellip; , W
n
son independientes y tienen distribución Poisson(p
i
), respectivamente, y entonces,
la variable N tiene distribución Poisson de media &lambda; =
P
n
{i=1}
p
i
.
Observando que cada k
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{\mathbb{P}(S</td>
</tr>
</tbody>
</table>
<p>
∗
= k) − \mathbb{P}(N = k)| &le; \mathbb{P}(S}
∗
= k, N &ne; k) + \mathbb{P}(N = k, S}
∗
&ne; k).
se obtiene que
X
k
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{\mathbb{P}(S</td>
</tr>
</tbody>
</table>
<p>
∗
= k) − \mathbb{P}(N = k)| &le; 2\mathbb{P}(S}
∗
&ne; N).
Si S}
∗
&ne; N, entonces V
i
&ne; W
i
para algún i = 1, &hellip; , n}. En consecuencia,
\mathbb{P}(S
∗
&ne; N) &le;
n
X
{i=1}
\mathbb{P}(V
i
&ne; W
i
) &le;}
n
X
{i=1}
p
2
i
.
</p>
</div>
</div>
<div id="outline-container-org01f1e5e" class="outline-5">
<h5 id="org01f1e5e">Corolario 2.6 (Aproximación Poisson). Para cada k &ge; 0</h5>
<div class="outline-text-5" id="text-org01f1e5e">
<p>
lim<sub>n  &rarr; &infin;</sub>
</p>

<p>
n
k

1 −}
&lambda;
n

n{−}k
</p>

<p>
&lambda;
n

k
= e
− &lambda; 
&lambda;
k
k{!}
18
</p>
</div>
</div>
<div id="outline-container-org38b4623" class="outline-5">
<h5 id="org38b4623">Demostración</h5>
<div class="outline-text-5" id="text-org38b4623">
<p>
Sean U
1
, &hellip; , U
n
variables aleatorias independientes con distribución común
U[0, 1). Para cada i = 1, &hellip; , n definimos parejas de variables aleatorias (V
i
, W
i
) independientes
V
i
:= 1\{U 
i
&isin; I}
1
(p)\, W
i
:=
&infin;
X
{k=0}
k{1{\}U
i
&isin; J}
k
(p)\}.
Por construcción, V}
i
&sim; Bernoulli(p) y W
i
&sim; Poisson(p), en consecuencia las sumas}
S =}
n
X
{i=1}
V
i
, N =}
n
X
{i=1}
W
i
son variables aleatorias con distribuciones Binomial(n, p) y Poisson(np), respectivamente. De
acuerdo con la demostración del Teorema de Le Cam tenemos que





</p>

<p>
n
k

1 −}
&lambda;
n

n{−}k
</p>

<p>
&lambda;
n

k
− e
− &lambda; 
&lambda;
k
k{!}





= |\mathbb{P}(S = k) − \mathbb{P}(N = k)| &le; 2{np}
2
= 2
&lambda;
2
n
&rarr; 0.
</p>
</div>
</div>
<div id="outline-container-orgb4ea998" class="outline-5">
<h5 id="orgb4ea998">Teorema 2.7. Supongamos que para cada n, X}</h5>
<div class="outline-text-5" id="text-orgb4ea998">
<p>
n,{1}
, &hellip; , X
n,r
n
son variables aleatorias inde
pendientes con distribución Bernoulli(p}
n,k
). Si}
r
n
X
{k=1}
p
n,k
&rarr; &lambda; &ge; 0, máx}
1{\leqk\leqr}
n
p
n,k
&rarr; 0, (42)
entonces
P
r
n
X
{k=1}
X
n,k
= i
!
&rarr; e
− &lambda; 
&lambda;
i
i{!}
, i = 0, 1, 2, &hellip; . (43)
Si &lambda; = 0}, el límite (43) se interpreta como 1 para i = 0 y 0 para i &ge; 1 . En el caso r
n
= n
y p
n,k
= &lambda;/n, (43) es la aproximación Poisson a la binomial. Notar que si &lambda; &gt; 0}, entonces
(42) implica que r
n
&rarr; &infin;.
</p>
</div>
</div>
<div id="outline-container-org4ef5c0f" class="outline-5">
<h5 id="org4ef5c0f">Demostración</h5>
<div class="outline-text-5" id="text-org4ef5c0f">
<p>
Sea U
1
, U
2
, &hellip; una sucesión de variables aleatorias independientes, con}
distribución común U[0, 1). Definimos
V
n,k
:= 1\{U 
k
&isin; I}
1
(p
n,k
)\}.
Las variables V}
n,{1}
, &hellip; , V
n,r
n
son independientes y con distribución Bernoulli(p
n,k
). Puesto que
V
n,{1}
, &hellip; , V
n,r
n
tienen la misma distribución que X
n,{1}
, &hellip; , X
n,r
n
, (43) se obtiene mostrando
que V}
n
=
P
r
n
{k=1}
V
n,k
satisface
\mathbb{P}(V
n
= i) &rarr; e}
− &lambda; 
&lambda;
i
i{!}
. (44)
Ahora definimos
W
n,k
:=
&infin;
X
{i=0}
i{1{\}U
k
&isin; J}
i
(p
n,k
)\}
19
W
n,k
tiene distribución Poisson de media p
n,k
. Puesto que las W}
n,k
son independientes, W}
n
=
P
r
n
{k=1}
W
n.k
tiene distribución Poisson de media &lambda;}
n
=
P
r
n
{k=1}
p
n,k
. De la desigualdad 1{−p &le; e}
−p
,
se obtiene como consecuencia que
\mathbb{P}(V
n,k
&ne; W
n,k
) = \mathbb{P}(V}
n.k
= 1 &ne; W}
n,k
) = \mathbb{P}(U}
k
&isin; I}
1
(p
n,k
) − J
1
(p
n,k
))
= p
n,k
− e
−p
n,k
p
n,k
&le; p
2
n,k
,
y por (42)
\mathbb{P}(V
n
&ne; W
n
) &le;}
r
n
X
{k=1}
p
2
n,k
&le; &lambda;}
n
máx
1{\leqk\leqr}
n
p
n,k
&rarr; 0.
(44) y (43) se obtienen de observar que
\mathbb{P}(W
n
= i) = e
− &lambda; 
n
&lambda;
i
n
n{!}
&rarr; e
− &lambda; 
&lambda;
n
n{!}
.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org4a0f1a2" class="outline-2">
<h2 id="org4a0f1a2">Cuentas con exponenciales</h2>
<div class="outline-text-2" id="text-org4a0f1a2">
</div>
<div id="outline-container-org9949023" class="outline-3">
<h3 id="org9949023">Motivación: pasaje de lo discreto a lo continuo</h3>
<div class="outline-text-3" id="text-org9949023">
<p>
Para fijar ideas consideraremos una conversación telefónica y supondremos que su duración
es un número entero de segundos. La duración de la conversación será tratada como una
variable aleatoria T cuya distribución de probabilidades p
n
= \mathbb{P}(T = n) es conocida. La línea
telefónica representa un sistema físico con dos estados posibles /"ocupada"/(E}
0
) y /"libre"/(E}
1
).
Imaginemos que cada segundo se decide si la conversación continúa o no por medio de
una moneda cargada. En otras palabras, se realiza una sucesión de ensayos Bernoulli con
probabilidad de éxito p a una tasa de un ensayo por segundo y se continúa hasta el primer
éx ito. La conversación termina cuando ocurre el primer éxito. En este caso la duración total
de la conversación, el tiempo de espera, tiene distribución geométrica p
n
= (1 − p)
n{−{1
p. Si en}
un instante cualquiera la línea está ocupada, la probabilidad que permanezca ocupada por
más de un segundo es (1 − p), y la probabilidad de transición E}
0
&rarr; E}
1
en el siguiente paso
es p. En este caso esas probabilidades son independientes de cuánto tiemp o estuvo ocupada
la línea.
La descripción de los tiempos de espera mediante modelos discretos presupone la cuanti
zación del tiempo y que l os cambios solo pueden ocurrir en las épocas ε, 2{ε, &hellip; . El tiempo de
espera T más sencillo es el tiempo de espera hasta el primer éxito en una sucesión de ensayos
Bernoulli con probabilidad de éxito p(ε). En tal caso \mathbb{P}(T &gt; nε) = (1 − p(ε))
n
y el tiempo
medio de espera es E[T ] = ε/p(ε). Este modelo puede se puede refinar haciendo que ε sea
cada vez más chico pero manteniendo fija la esperanza ε/p(ε) = 1{/&lambda;}. Para un intervalo de
duración t corresponden aproximadamente n &asymp; t/ε ensayos, y entonces para ε pequeño
\mathbb{P}(T &gt; t) &asymp; (1 − \lambdaε})
t/ε
&asymp; e
−{&lambda; t}
. (45)
Este modelo considera el tiempo de esp era como una variable aleatoria discreta distribuida
geométricamente y (45) dice que <i>"en el límite"</i> se obtiene una distribución exponencial.
20
Si no discretizamos el tiempo tenemos que tratar con variables aleatorias continuas. El rol
de la distribución geométrica para los tiempos de espera lo ocupa la distribución exponencial.
Es la única variable continua dotada de una completa falta de memoria. En otras palabras, la
probabilidad de que una conversación que llegó hasta el tiempo t continúe más allá del tiempo
t + s es independiente de la duración pasada de la conversación si, y solo si, la probabilidad}
que la conversación dure por lo menos t unidades de tiempo está dada por una exponencial
e
−{&lambda; t}
.
</p>
</div>
<div id="outline-container-orgb7160e9" class="outline-5">
<h5 id="orgb7160e9">Nota Bene Si en un momento arbitrario t la línea está ocupada, entonces la probabilidad}</h5>
<div class="outline-text-5" id="text-orgb7160e9">
<p>
de un cambio de estado durante el próximo segundo depende de cuan larga ha sido la con
versación. En otras palabras, el pasado inﬂuye sobre el futuro. Esta circunstancia es la fuente
de muchas dificultades en problemas más complicados.
</p>
</div>
</div>
</div>
<div id="outline-container-org5d56cd6" class="outline-3">
<h3 id="org5d56cd6">Distribución exponencial</h3>
<div class="outline-text-3" id="text-org5d56cd6">
<p>
Se dice que la variable aleatoria T tiene distribución exponencial de intensidad &lambda; &gt; 0 y se 
denota T &sim; Exp( &lambda; ) si la función de distribución de T es de la forma
F
T
(t) := \mathbb{P}(T &le; t) =

1 − e}
−{&lambda; t}

1\{t &ge; 0}\. (46) 
En tal caso T admite la siguiente función densidad de probabilidades
f
T
(t) = &lambda; e}
−{&lambda; t}
1\{t &ge; 0}\. (47) 
Media y Varianza. Los valores de la esperanza y la varianza de T son, respectivamente,}
E[T ] = 1{<i>&lambda; y V(T ) = 1{</i>&lambda;
2
.
</p>
</div>
</div>
<div id="outline-container-orga7d36ad" class="outline-3">
<h3 id="orga7d36ad">Suma de exponenciales independientes de igual intensidad</h3>
<div class="outline-text-3" id="text-orga7d36ad">
</div>
<div id="outline-container-org962f7a5" class="outline-5">
<h5 id="org962f7a5">Teorema 3.1. Sean T<sub>1</sub></h5>
<div class="outline-text-5" id="text-org962f7a5">
<p>
, T<sub>2</sub>
, &hellip; , T<sub>n</sub>
variables aleatorias independientes, idénticamente dis
tribuidas, con distribución exponencial de intensidad &lambda; &gt; 0. La suma S<sub>n</sub>
= T}
1
</p>
<ul class="org-ul">
<li>&ctdot;  + T<sub>n</sub></li>
</ul>
<p>
admite una densidad de probabilidades de la forma
f
S<sub>n</sub>
(t) = &lambda; e}
−{&lambda; t}
(&lambda; t)
n{−{1
(n − 1)!
1\{t &gt; 0}\} (48)}
y su función de distribución es
F
S<sub>n</sub>
(t) =
1 − e}
−{&lambda; t}
n{−{1
X
{i=0}
(&lambda; t)
i
i{!}
!
1\{t &ge; 0}\. (49) 
En otras palabras, la suma de n variables aleatorias independientes exponenciales de intensi
dad &lambda; &gt; 0 tiene distribución Gamma de parámetros n y &lambda;}: &Gamma;(n, &lambda;).
21
</p>
</div>
</div>
<div id="outline-container-orgc2005ec" class="outline-5">
<h5 id="orgc2005ec">Demostración</h5>
<div class="outline-text-5" id="text-orgc2005ec">
<p>
Por inducción. Para n = 1 no hay nada que probar: S
1
= T}
1
&sim; Exp( &lambda; ).
Supongamos ahora que la suma S<sub>n</sub>
= T}
1
</p>
<ul class="org-ul">
<li>&ctdot;  + T<sub>n</sub></li>
</ul>
<p>
admite una densidad de la forma (48).
Debido a que las variables aleatorias S<sub>n</sub>
y T}
{n+1}
son independientes, la densidad de S}
{n+1}
=
S<sub>n</sub>
</p>
<ul class="org-ul">
<li>T}</li>
</ul>
<p>
{n+1}
se obtiene convolucionando las densidades de S<sub>n</sub>
y T}
{n+1}
</p>
<pre class="example">


</pre>
<p>
f
S
{n+1}
(t) = (f
S<sub>n</sub>
∗ f
T
{n+1}
)(t) =
Z
t<sub>0</sub>
f
S<sub>n</sub>
(t − x)f
T
{n+1}
(x)dx}
=
Z
t<sub>0</sub>
&lambda; e
−{&lambda; ( t}−{x ) 
( &lambda; (t − x))
n{−{1
(n − 1)!
&lambda; e
−{&lambda; x}
dx
= &lambda; e}
−{&lambda; t}
&lambda;
n
(n − 1)!
Z
t<sub>0</sub>
(t − x)
n{−{1
dx = &lambda; e
−{&lambda; t}
&lambda;
n
(n − 1)!
t<sub>n</sub>
n
= &lambda; e}
−{&lambda; t}
(&lambda; t)
n
n{!}
.
Las funciones de distribución (49) se obtienen integrando las densidades (48). Sea t &ge; 0,
integrando por partes puede verse que
F
S<sub>n</sub>
(t) =
Z
t<sub>0</sub>
f
S<sub>n</sub>
(s)ds =
Z
t<sub>0</sub>
(&lambda; s)
n{−{1
(n − 1)!
&lambda; e
−{&lambda; s}
ds
= −}
(&lambda; s)
n{−{1
(n − 1)!
e
−{&lambda; s}




t<sub>0</sub>
</p>
<ul class="org-ul">
<li></li>
</ul>
<p>
Z
t<sub>0</sub>
(&lambda; s)
n{−{2
(n − 2)!
&lambda; e
−{&lambda; t}
ds
= −}
(&lambda; t)
n{−{1
(n − 1)!
e
−{&lambda; t}
</p>
<ul class="org-ul">
<li>F}</li>
</ul>
<p>
S<sub>n</sub>{−{1
(t). (50)
Iterando (50) obtenemos (49).
</p>
</div>
</div>
<div id="outline-container-org569af45" class="outline-5">
<h5 id="org569af45">Nota Bene</h5>
<div class="outline-text-5" id="text-org569af45">
<p>
En la demostración anterior se utilizó el siguiente resultado: si T<sub>1</sub>
, &hellip; , T<sub>n</sub>
son
variables aleatorias indepe ndientes, entonces funciones (medibles) de familias disjuntas de las
T
i
también son independientes. (Para más detalles ver el Capítulo 1 de Durrett, R., (1996).
Probability Theory and Examples, Duxbury Press, New York.)
</p>
</div>
</div>
</div>
<div id="outline-container-org1b639a0" class="outline-3">
<h3 id="org1b639a0">Mínimos</h3>
<div class="outline-text-3" id="text-org1b639a0">
</div>
<div id="outline-container-orge30f029" class="outline-5">
<h5 id="orge30f029">Lema 3.2. Sean T<sub>1</sub></h5>
<div class="outline-text-5" id="text-orge30f029">
<p>
y T}
2
dos variables aleatorias independientes y exponenciales de intensi
dades &lambda;}
1
y &lambda;}
2
, respectivamente. Vale que
\mathbb{P}(T<sub>1</sub>
&lt; T<sub>2</sub>
) =
&lambda;
1
&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
. (51)
</p>
</div>
</div>
<div id="outline-container-org2deab18" class="outline-5">
<h5 id="org2deab18">Demostración</h5>
<div class="outline-text-5" id="text-org2deab18">
<p>
La probabilidad \mathbb{P}(T<sub>1</sub>
&lt; T<sub>2</sub>
) puede calcularse condicionando sobre T}
1
</p>
<pre class="example">


</pre>
<p>
\mathbb{P}(T<sub>1</sub>
&lt; T<sub>2</sub>
) =
Z
&infin;
0
\mathbb{P}(T<sub>1</sub>
&lt; T<sub>2</sub>
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{T}</td>
</tr>
</tbody>
</table>
<p>
1
= t)f
T<sub>1</sub>
(t)dt =
Z
&infin;
0
\mathbb{P}(t &lt; T<sub>2</sub>
) &lambda; 
1
e
− &lambda; 
1
t
dt
= &lambda;}
1
Z
&infin;
0
e
− &lambda; 
2
t
e
− &lambda; 
1
t
dt = &lambda;
1
Z
&infin;
0
e
−( &lambda; }
1
</p>
<ul class="org-ul">
<li>&lambda;</li>
</ul>
<p>
2
)t
dt =}
&lambda;
1
&lambda;
1
</p>
<ul class="org-ul">
<li>&lambda;}</li>
</ul>
<p>
2
.
22
</p>
</div>
</div>
<div id="outline-container-org7a02f91" class="outline-5">
<h5 id="org7a02f91">Teorema 3.3. Sean T<sub>1</sub></h5>
<div class="outline-text-5" id="text-org7a02f91">
<p>
, T<sub>2</sub>
, &hellip; , T<sub>n</sub>
variables aleatorias exponenciales independientes de in
tensidades &lambda;}
1
, &lambda;
2
, &hellip; , &lambda;
n
, respectivamente. Sean T y J las variables aleatorias definidas por
T := mín}
i
T
i
, J := índice que realiza T.}
Entonces, T tiene distribución exponencial de intensidad &lambda;}
1
</p>
<ul class="org-ul">
<li>&ctdot;  + &lambda;}</li>
</ul>
<p>
n
y
\mathbb{P}(J = j) =}
&lambda;
j
&lambda;
1
</p>
<ul class="org-ul">
<li>&ctdot;  + &lambda;}</li>
</ul>
<p>
n
.
Más aún, las variables T y J son independientes.
</p>
</div>
</div>
<div id="outline-container-org9f62e16" class="outline-5">
<h5 id="org9f62e16">Demostración</h5>
<div class="outline-text-5" id="text-org9f62e16">
<p>
En prime r lugar, hay que observar que T &gt; t si y solo si T
i
&gt; t para}
todo i = 1, &hellip; , n}. Como las variables T}
1
, T<sub>2</sub>
, &hellip; , T<sub>n</sub>
son exponenciales independientes de
intensidades &lambda;}
1
, &lambda;
2
, &hellip; &lambda;
n
tenemos que
\mathbb{P}(T &gt; t) =}
n
Y
{i=1}
\mathbb{P}(T
i
&gt; t) =}
n
Y
{i=1}
e
− &lambda; 
i
t
= e
−( &lambda; }
1
<del>{&ctdot;}</del> &lambda; 
n
)t
.
Por lo tanto, T tiene distribución exponencial de intensidad &lambda;}
1
</p>
<ul class="org-ul">
<li>&ctdot;  + &lambda;}</li>
</ul>
<p>
n
.
En segundo lugar hay que observar que J = j si y solo si T = T}
j
. Por lo tanto,
\mathbb{P}(J = j) = \mathbb{P}(T
j
= mín
i
T
i
) = \mathbb{P}(T}
j
&lt; mín}
i{&ne;} j
T
i
) =
&lambda;
j
&lambda;
1
</p>
<ul class="org-ul">
<li>&ctdot;  + &lambda;}</li>
</ul>
<p>
n
.
La última igualdad se obtiene utilizando el Lema 3.2 pues las variables T}
j
y mín
i{&ne;} j
T
i
son
independientes y exponenciales con intensidades &lambda;}
j
y
P
i{&ne;} j
&lambda;
i
, respectivamente.
Finalmente, si para cada j definimos U}
j
= mín
i{&ne;} j
T
i
, tenemos que
\mathbb{P}(J = j, T &ge; t) = \mathbb{P}(t &le; T
j
&lt; U
j
)
=
Z
&infin;
t
\mathbb{P}(T
j
&lt; U
j
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">{T}</td>
</tr>
</tbody>
</table>
<p>
j
= s) &lambda; 
j
e
− &lambda; 
j
s
ds
= &lambda;}
j
Z
&infin;
t
\mathbb{P}(U
j
&gt; s ) e
− &lambda; 
j
s
ds = &lambda;
j
Z
&infin;
t
e
− ( 
P
i{&ne;} j
&lambda;
i
)s
e
− &lambda; 
j
s
ds
=
&lambda;
j
&lambda;
1
</p>
<ul class="org-ul">
<li>&ctdot;  + &lambda;}</li>
</ul>
<p>
n
Z
&infin;
t
( &lambda; 
1
</p>
<ul class="org-ul">
<li>&ctdot;  + &lambda;}</li>
</ul>
<p>
n
)e
−( &lambda; }
1
<del>{&ctdot;}</del> &lambda; 
n
)s
ds
=
&lambda;
j
&lambda;
1
</p>
<ul class="org-ul">
<li>&ctdot;  + &lambda;}</li>
</ul>
<p>
n
e
−( &lambda; }
1
<del>{&ctdot;}</del> &lambda; 
n
)t
.
Lo que completa la demostración.
</p>
</div>
</div>
<div id="outline-container-org2fce921" class="outline-5">
<h5 id="org2fce921">Ejercicios adicionales</h5>
<div class="outline-text-5" id="text-org2fce921">
<ol class="org-ol">
<li>Sean T<sub>1</sub></li>
</ol>
<p>
y T}
2
variables aleatorias independientes exponenciales de intensidad 2. Sean
T
(1)
= mín(T}
1
, T<sub>2</sub>
) y T}
(2)
= máx(T}
1
, T<sub>2</sub>
). Hallar la esperanza y la varianza de T}
(1)
y de T}
(2)
.
23
</p>
<ol class="org-ol">
<li>Suma geométrica de exponenciales independientes. Sean T</li>
</ol>
<p>
1
, T<sub>2</sub>
, &hellip; variables aleatorias}
independientes idénticamente distribuidas con ley exponencial de intensidad &lambda;}. Se define
T =}
P
N
{i=1}
T
i
, donde N es una variable aleatoria con distribución geométrica de parámetro
p, independiente de las variables T<sub>1</sub>
, T<sub>2</sub>
, &hellip; . Hallar la distribución de T . (Sugerencia{: Utilizar 
la fórmula de probabilidad total condicionando a los posibles valores de N y el desarrollo en
serie de Taylor de la función exponencial.)
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-org8a0f1ee" class="outline-2">
<h2 id="org8a0f1ee">Bibliografía consultada</h2>
<div class="outline-text-2" id="text-org8a0f1ee">
<p>
Para redactar estas notas se consultaron los siguientes libros:
</p>
<ol class="org-ol">
<li>Billingsley, P.: Probability and measure. John Wiley &amp; Sons, New
York. (1986)</li>
<li>Durrett R.:Probability. Theory and Examples. Duxbury Press,
Belmont. (1996)</li>
<li>Feller, W.: An introduction to Probability Theory and Its
Applications. Vol. 1. John Wiley &amp; Sons, New York. (1957)</li>
<li>Feller, W.: An introduction to Probability Theory and Its
Applications. Vol. 2. John Wiley &amp; Sons, New York. (1971)</li>
<li>Grimmett, G. R., Stirzaker, D. R.: Probability and Random
Processes. Oxford University Press, New York. (2001)</li>
<li>Meester, R.: A Natural Introduction to Probability Theory. Birk
hauser, Berlin. (2008).</li>
<li>Meyer, P. L.: Introductory Probability and Statistical
Applications. Addison-Wesley, Massachusetts. (1972)</li>
<li>Ross, S. M: Introduction to Probability and Statistics for
Engineers and Scientists. Elsevier Academic Press, San
Diego. (2004)</li>
<li>Soong, T. T.: Fundamentals of Probability and Statistics for
Engineers. John Wiley &amp; Sons Ltd. (2004)</li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
Last update: 2019-03-18 00:16
</div>
</body>
</html>
