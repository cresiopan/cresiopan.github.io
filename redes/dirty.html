<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-09-27 Sun 13:58 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="generator" content="Org mode" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="/res/org.css"/>
<link rel="stylesheet" type="text/css" href="/home/mk/Documents/blogs/org.css"/>
<script type="text/javascript">
// @license magnet:?xt=urn:btih:1f739d935676111cfff4b4693e3816e664797050&amp;dn=gpl-3.0.txt GPL-v3-or-Later
<!--/*--><![CDATA[/*><!--*/
     function CodeHighlightOn(elem, id)
     {
       var target = document.getElementById(id);
       if(null != target) {
         elem.cacheClassElem = elem.className;
         elem.cacheClassTarget = target.className;
         target.className = "code-highlighted";
         elem.className   = "code-highlighted";
       }
     }
     function CodeHighlightOff(elem, id)
     {
       var target = document.getElementById(id);
       if(elem.cacheClassElem)
         elem.className = elem.cacheClassElem;
       if(elem.cacheClassTarget)
         target.className = elem.cacheClassTarget;
     }
    /*]]>*///-->
// @license-end
</script>
<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML' async></script>
</head>
<body>
<div id="content">
<div id="outline-container-orgc29b217" class="outline-2">
<h2 id="orgc29b217">Redes de Computadoras y la Internet</h2>
<div class="outline-text-2" id="text-orgc29b217">
<ul class="org-ul">
<li>components that make up a network</li>
<li>network edge
<ul class="org-ul">
<li>end systems and network applications running in the network</li>
</ul></li>
<li>network core
<ul class="org-ul">
<li>links and the switches that transport data</li>
<li>access networks and physical media that connect end systems to the network
core.</li>
</ul></li>
</ul>

<p>
the Internet is a network of networks, and we’ll learn how these networks
connect with each other.
</p>
</div>

<div id="outline-container-org58dec49" class="outline-3">
<h3 id="org58dec49">Que es la Internet?</h3>
<div class="outline-text-3" id="text-org58dec49">
<p>
dos descripciones
</p>
</div>

<div id="outline-container-orgdcad772" class="outline-4">
<h4 id="orgdcad772">Descripcion de Componentes</h4>
<div class="outline-text-4" id="text-orgdcad772">
<p>
The Internet es una red de computadores que interconecta millones de dispostivos alrededor del mundo.
</p>

<p>
Estos dispositivos son llamados <code>hosts</code> o <code>end systems</code>.
</p>

<p>
imagen 1.1
</p>

<p>
Los End-systems estan conectados a traves de una red de <code>enlaces de
comunicacion</code> (coaxial,cobre,fibra,radio) y <code>conmutadores de paquetes</code>.
</p>

<p>
Diferentes enlaces pueden transmitir datos a diferentes velocidades, con la
<code>velocidad de transmision</code> de un enlace medida en bits/segundo.
</p>

<p>
When one end system has data to send to another end system, the sending end
system segments the data and adds header bytes to each segment. The resulting
packages of information, known as <code>packets</code>, are then sent through the network
to the destination end system, where they are reassembled into the original
data.
</p>

<p>
A packet switch takes a packet arriving on one of its incoming communication
links and forwards that packet on one of its outgoing communication links.
</p>

<p>
The two most prominent types in today’s Internet are routers and link-layer
switches. Both types of switches forward packets toward their ultimate
destinations. Link-layer switches are typically used in access networks,
while routers are typically used in the network core.
</p>

<p>
The sequence of communication links and packet switches traversed by a
packet from the sending end system to the receiving end system is known as a
<code>route</code> or <code>path</code> through the network.
</p>

<p>
Thus, in many ways, packets are analogous to trucks, communication links are
analogous to highways and roads, packet switches are analogous to intersections,
and end systems are analogous to buildings. Just as a truck takes a path through
the transportation network, a packet takes a path through a computer network.
</p>

<p>
End systems access the Internet through <code>Internet Service Providers (ISPs)</code>,
including residential ISPs such as
</p>
<ul class="org-ul">
<li>local cable or telephone companies</li>
<li>corporate ISPs</li>
<li>university ISPs</li>
<li>ISPs that provide WiFi access in airports, hotels, coffee shops, and other public places</li>
<li>cellular data ISPs, providing mobile access to our smartphones and other devices.</li>
</ul>

<blockquote>
<p>
Each ISP is in itself a network of packet switches and communication links.
</p>

<p>
ISPs provide a variety of types of network access to the end systems,
including residential broadband access such as cable modem or DSL,
high-speed local area network access, and mobile wireless access.
</p>

<p>
ISPs also provide ­ Internet access to content providers, connecting Web
sites and video servers directly to the Internet.
</p>
</blockquote>

<p>
The Internet is all about connecting end systems to each other, so the ISPs
that provide access to end systems must also be interconnected.  These
lower-tier ISPs are interconnected through national and international
upper-tier ISPs such as Level 3 Communications, AT&amp;T, Sprint, and NTT. An
upper-tier ISP consists of high-speed routers interconnected with high-speed
fiber-optic links. Each ISP network, whether upper-tier or lower-tier,
ismanaged independently, runs the IP protocol (see below), and conforms to
certain naming and address conventions.
</p>

<p>
End systems, packet switches, and other pieces of the Internet run
<code>protocols</code> that control the sending and receiving of information within the
Internet.
</p>

<p>
The Transmission Control Protocol (TCP) and the Internet Protocol (IP) are
two of the most important protocols in the Internet. The IP protocol
specifies the format of the packets that are sent and received among routers
and end systems. The Internet’s principal protocols are collectively known
as TCP/IP.
</p>

<p>
Internet ­ standards are developed by the Internet Engineering Task Force
(IETF) [IETF 2016]. The IETF standards documents are called requests for
comments (RFCs) .
</p>
</div>
</div>

<div id="outline-container-orgfa0dd76" class="outline-4">
<h4 id="orgfa0dd76">Descripcion de Servicios</h4>
<div class="outline-text-4" id="text-orgfa0dd76">
<p>
La internet tambien se puede describir como una infraestructura que provee
servicios a aplicaciones.
</p>

<p>
In addition to traditional applications such as e-mail and Web surfing,
Internet applications include mobile smartphone and tablet applications,
including:
</p>
<ul class="org-ul">
<li>Internet messaging,</li>
<li>mapping with real-time road-traffic information,</li>
<li>music streaming from the cloud,</li>
<li>movie and television streaming,</li>
<li>online social networks,</li>
<li>video conferencing,</li>
<li>multi-person games,</li>
<li><p>
location-based recommendation systems.
</p>

<p>
The applications are said to be <code>distributed applications</code>, since they
involve multiple end systems that exchange data with each other.
</p>

<blockquote>
<p>
Internet applications run on end systems- they do not run in the packet
switches in the network core. Although packet switches facilitate the
exchange of data among end systems, they are not concerned with the
application that is the source or sink of data.
</p>
</blockquote></li>
</ul>


<p>
Let’s explore a little more what we mean by an infrastructure that provides ­
services to applications. How does one program running on one end system
instruct the Internet to deliver data to another program running on another
end system?
</p>

<p>
End systems attached to the Internet provide a socket interface that
specifies how a program running on one end system asks the Internet
infrastructure to deliver data to a specific destination program running on
another end system.
</p>

<p>
This Internet socket interface is a set of rules that the sending program
must follow so that the Internet can deliver the data to the destination
program.
</p>

<p>
Suppose Alice wants to send a letter to Bob using the postal service.  Alice,
of course, can’t just write the letter (the data) and drop the letter out her
window. Instead, the postal service requires that Alice put the letter in an
envelope; write Bob’s full name, address, and zip code in the center of the
envelope; seal the envelope; put a stamp in the upper-right-hand corner of
the envelope; and finally, drop the envelope into an official postal service
mailbox.
</p>

<p>
Thus, the postal service has its own “postal service interface,” or set of
rules, that Alice must follow to have the postal service deliver her letter
to Bob. In a similar manner, the Internet has a socket interface that the
program sending data must follow to have the Internet deliver the data to
the program that will receive the data.
</p>

<p>
The postal service, of course, provides more than one service to its
customers. It provides express delivery, reception confirmation, ordinary
use, and many more services. In a similar manner, the Internet provides
multiple services to its applications.
</p>
</div>
</div>

<div id="outline-container-orgfa8808f" class="outline-4">
<h4 id="orgfa8808f">Que es un protocolo?</h4>
<div class="outline-text-4" id="text-orgfa8808f">
<p>
serie de reglas/comportamientos bien definidos que llevan al cumplimiento de un
objetivo.
</p>

<p>
algoritmo
</p>

<p>
intercambio de mensajes que desencadenan eventos/comportamientos/otros
mensajes para lograr un objetivo
</p>

<p>
it takes two (or more) communicating entities running the same protocol in
order to accomplish a task.
</p>

<p>
imagen 1.2
</p>

<p>
transmision y recepcion de mensajes y un conjunto de acciones convencionales
tomadas cuando estos mensajes son enviados y recibidos
</p>

<p>
All activity in the Internet that involves two or more communicating remote
entities is governed by a protocol.
</p>

<p>
For example, hardware-implemented protocols in two physically connected
computers control the flow of bits on the “wire” between the two network
interface cards; congestion-control protocols in end systems control the
rate at which packets are transmitted between sender and receiver;
protocols in routers determine a packet’s path from source to destination.
</p>

<blockquote>
<p>
A protocol defines the format and the order of messages exchanged between
two or more communicating entities, as well as the actions taken on the
transmission and/or receipt of a message or other event.
</p>
</blockquote>
</div>
</div>
</div>

<div id="outline-container-orgc304bc8" class="outline-3">
<h3 id="orgc304bc8">Network Edge&#xa0;&#xa0;&#xa0;<span class="tag"><span class="networkedge">networkedge</span></span></h3>
<div class="outline-text-3" id="text-orgc304bc8">
<p>
the computers and other devices connected to the Internet are often referred
to as end systems. They are referred to as end systems because they sit at
the edge of the Internet.
</p>

<p>
imagen 1.3
</p>

<p>
End systems are also referred to as hosts because they host (ie, run)
application programs such as
</p>
<ul class="org-ul">
<li>a Web browser/server</li>
<li><p>
an e-mail client/server
</p>

<p>
host = end system
</p>

<p>
Hosts are sometimes further divided into two categories: <code>clients</code> and
<code>servers</code>. Informally, clients tend to be desktop and mobile PCs,
smartphones, and so on, whereas servers tend to be more powerful machines
that store and distribute Web pages, stream video, relay e-mail, and so on.
</p>

<p>
Today, most of the servers from which we receive search results, e-mail, Web
pages, and videos reside in large <code>data centers</code>.
</p></li>
</ul>
</div>

<div id="outline-container-org9a93aa0" class="outline-4">
<h4 id="org9a93aa0">Redes de acceso</h4>
<div class="outline-text-4" id="text-org9a93aa0">
<p>
the network that physically connects an end system to the first router (also
known as the “edge router”) on a path from the end system to any other
distant end system.
</p>

<p>
imagen 1.4
</p>
</div>

<div id="outline-container-org4bb9c2d" class="outline-5">
<h5 id="org4bb9c2d">Home Access: DSL, Cable, FTTH, Dial-Up, and Satellite</h5>
<div class="outline-text-5" id="text-org4bb9c2d">
<p>
let’s begin our overview of access networks by considering how homes
connect to the Internet.
</p>

<p>
the two most prevalent types of broadband residential access are digital
subscriber line (DSL) and cable.
</p>

<p>
A residence typically obtains DSL Internet access from the same local
telephone company (telco) that provides its wired local phone access. Thus,
when DSL is used, a customer’s telco is also its ISP.
</p>

<p>
each customer’s DSL modem uses the existing telephone line to exchange data
with a digital subscriber line access multiplexer (DSLAM) located in the
telco’s local central office (CO). The home’s DSL modem takes digital data
and translates it to high-frequency tones for transmission over telephone
wires to the CO; the analog signals from many such houses are translated
back into digital format at the DSLAM.
</p>

<p>
The residential telephone line carries both data and traditional telephone
signals simultaneously, which are encoded at different frequencies:
</p>
<ul class="org-ul">
<li>A high-speed downstream channel, in the 50 kHz to 1 MHz band</li>
<li>A medium-speed upstream channel, in the 4 kHz to 50 kHz band</li>
<li><p>
An ordinary two-way telephone channel, in the 0 to 4 kHz band
</p>

<p>
imagen 1.5
</p>

<p>
This approach makes the single DSL link appear as if there were three
separate links, so that a telephone call and an Internet connection can
share the DSL link at the same time.
</p>

<p>
On the customer side, a splitter separates the data and telephone signals
arriving to the home and forwards the data signal to the DSL modem. On the
telco side, in the CO, the DSLAM separates the data and phone signals and
sends the data into the Internet. Hundreds or even thousands of households
connect to a single DSLAM
</p>

<p>
While DSL makes use of the telco’s existing local telephone infrastructure,
cable Internet access makes use of the cable television company’s existing
cable television infrastructure. A residence obtains cable Internet access
from the same company that provides its cable television
</p>

<p>
fiber optics connect the cable head end to neighborhood-level junctions,
from which traditional coaxial cable is then used to reach individual
houses and apartments. Each neighborhood junction typically supports 500 to
5,000 homes. Because both fiber and coaxial cable are employed in this
system, it is often referred to as hybrid fiber coax (HFC).
</p>

<p>
imagen 1.6
</p>

<p>
Cable internet access requires special modems, called cable modems. As with
a DSL modem, the cable modem is typically an external device and connects to
the home PC through an Ethernet port.
</p>

<p>
At the cable head end, the cable modem termination system (CMTS) serves a
similar function as the DSL network’s DSLAM-turning the analog signal sent
from the cable modems in many downstream homes back into digital
format. Cable modems divide the HFC network into two channels, a downstream
and an upstream channel.
</p>

<p>
As with DSL, access is typically asymmetric, with the downstream channel
typically allocated a higher transmission rate than the upstream channel.
</p>

<p>
The [[DOCSIS] 2.0 standard defines downstream rates up to 42.8 Mbps and
upstream rates of up to 30.7 Mbps. As in the case of DSL networks, the
maximum achievable rate may not be realized due to lower contracted data
rates or media impairments.
</p>

<p>
One important characteristic of cable Internet access is that it is a
<code>shared broadcast medium</code>. In particular, every packet sent by the head end
travels downstream on every link to every home and every packet sent by a
home travels on the upstream channel to the head end. For this reason, if
several users are simultaneously downloading a video file on the downstream
channel, the actual rate at which each user receives its video file will be
significantly lower than the aggregate cable downstream rate. On the other
hand, if there are only a few active users and they are all Web surfing,
then each of the users may actually receive Web pages at the full cable
downstream rate, because the users will rarely request a Web page at
exactly the same time. Because the upstream channel is also shared, a
distributed multiple access protocol is needed to coordinate transmissions
and avoid collisions. Mas en capitulo 6.
</p>

<p>
An up-and-coming technology that provides even higher speeds is <code>fiber to
  the home (FTTH)</code>. FTTH provides an optical fiber path from the CO directly
to the home.
</p>

<p>
There are several competing technologies for optical distribution from the
CO to the homes.
</p>
<ul class="org-ul">
<li>The simplest optical distribution network is called direct fiber, with one
fiber leaving the CO for each home.</li>
<li><p>
More commonly, each fiber leaving the central office is actually shared by
many homes; it is not until the fiber gets relatively close to the homes
that it is split into individual customer-specific fibers.
</p>

<p>
There are two competing optical-distribution network architectures that
perform this splitting:
</p>
<ul class="org-ul">
<li>active optical networks (AONs) and</li>
<li>passive optical networks (PONs).</li>
</ul>

<p>
imagen 1.7
</p>

<p>
Each home has an optical network terminator (ONT), which is connected by
dedicated optical fiber to a neighborhood splitter. The splitter combines a
number of homes (typically less than 100) onto a single, shared optical
fiber, which connects to an optical line ­ terminator (OLT) in the telco’s
CO. The OLT, providing conversion between optical and electrical signals,
connects to the Internet via a telco router. In the home, users connect a
home router (typically a wireless router) to the ONT and access the ­
Internet via this home router. In the PON architecture, all packets sent
from OLT to the splitter are replicated at the splitter (similar to a cable
head end).
</p>

<p>
Two other access network technologies are also used to provide Internet
access to the home. In locations where DSL, cable, and FTTH are not
available (e.g., in some rural settings),
</p>
<ul class="org-ul">
<li>a satellite link can be used to connect a residence to the Internet at
speeds of more than 1 Mbps; StarBand and HughesNet are two such satellite
access providers.</li>
<li>Dial-up access over traditional phone lines is based on the same model as
DSL-a home modem connects over a phone line to a modem in the ISP.
Compared with DSL and other broadband access networks, dial-up access is
excruciatingly slow at 56 kbps.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgf58cf47" class="outline-5">
<h5 id="orgf58cf47">Access in the Enterprise (and the Home): Ethernet and WiFi</h5>
<div class="outline-text-5" id="text-orgf58cf47">
<p>
On corporate and university campuses, and increasingly in home settings, a
local area network (LAN) is used to connect an end system to the edge
router.
</p>

<p>
Although there are many types of LAN technologies, Ethernet is by far the
most prevalent access technology in corporate, university, and home
networks.
</p>

<p>
imagen 1.8
</p>

<p>
Ethernet users use twisted-pair copper wire to connect to an Ethernet
switch, a technology discussed in detail in Chapter 6.
</p>

<p>
The Ethernet switch, or a network of such interconnected switches, is then
in turn connected into the larger Internet. With Ethernet access, users
typically have 100 Mbps or 1 Gbps access to the Ethernet switch, whereas
servers may have 1 Gbps or even 10 Gbps access.
</p>

<p>
In a wireless LAN setting, wireless users transmit/receive packets to/from
an access point that is connected into the enterprise’s network (most
likely using wired Ethernet), which in turn is connected to the wired
Internet.
</p>


<p>
Las tecnologias corporativas (?) de acceso a la red se volvieron comunes en
redes hogareñas.
</p>

<p>
imagen 1.9
</p>

<p>
This home network consists of a roaming laptop as well as a wired PC; a
base station (the wireless access point), which communicates with the
wireless PC and other wireless devices in the home; a cable modem,
providing broadband access to the Internet; and a router, which
interconnects the base station and the stationary PC with the cable modem.
</p>
</div>
</div>

<div id="outline-container-orga21eb5c" class="outline-5">
<h5 id="orga21eb5c">Wide-Area Wireless Access: 3G and LTE</h5>
<div class="outline-text-5" id="text-orga21eb5c">
<p>
Increasingly, devices such as iPhones and Android devices are being used to
message, share photos in social networks, watch movies, and stream music
while on the run. These devices employ the same wireless infrastructure
used for cellular telephony to send/receive packets through a base station
that is operated by the cellular network provider. Unlike WiFi, a user need
only be within a few tens of kilometers (as opposed to a few tens of
meters) of the base station.
</p>

<p>
Telecommunications companies have made enormous investments in so-called
third-generation (3G) wireless, which provides packet-switched wide-area
wireless Internet access at speeds in excess of 1 Mbps. But even
higher-speed wide-area access technologies-a fourth-generation (4G) of
wide-area wireless networks-are already being deployed. LTE (for “Long-Term
Evolution”-a candidate for Bad Acronym of the Year Award) has its roots in
3G technology, and can achieve rates in excess of 10 Mbps. LTE downstream
rates of many tens of Mbps have been reported in commercial deployments.
</p>
</div>
</div>
</div>

<div id="outline-container-org6ebb954" class="outline-4">
<h4 id="org6ebb954">Medios Fiscos</h4>
<div class="outline-text-4" id="text-org6ebb954">
<p>
For each transmitter-receiver pair, the bit ((data)) is sent by propagating
electromagnetic waves or optical pulses across a <code>physical medium</code>. The
physical medium can take many shapes and forms and does not have to be of
the same type for each transmitter-receiver pair along the path.
</p>

<p>
Examples of physical media include:
</p>
<ul class="org-ul">
<li>twisted-pair</li>
<li>copper wire</li>
<li>coaxial cable</li>
<li>multimode fiber-optic cable</li>
<li>terrestrial radio spectrum</li>
<li><p>
satellite radio spectrum
</p>

<p>
Physical media fall into two categories: <code>guided media</code> and <code>unguided
  media</code>.
</p>

<p>
With guided media, the waves are guided along a solid medium, such as a
fiber-optic cable, a twisted-pair copper wire, or a coaxial cable.
</p>

<p>
With unguided media, the waves propagate in the atmosphere and in outer
space, such as in a wireless LAN or a digital satellite channel.
</p></li>
</ul>
</div>

<div id="outline-container-orgdc45821" class="outline-5">
<h5 id="orgdc45821">Twisted-Pair Copper Wire</h5>
<div class="outline-text-5" id="text-orgdc45821">
<p>
The wires are twisted together to reduce the electrical interference from
similar pairs close by.
</p>

<p>
A wire pair constitutes a single communication link. <code>Unshielded twisted
pair (UTP)</code> is commonly used for computer networks within a building, that
is, for LANs. Data rates for LANs using twisted pair today range from 10
Mbps to 10 Gbps. The data rates that can be achieved depend on the
thickness of the wire and the distance between transmitter and receiver.
</p>
</div>
</div>

<div id="outline-container-orgb4148c0" class="outline-5">
<h5 id="orgb4148c0">Coaxial Cable</h5>
<div class="outline-text-5" id="text-orgb4148c0">
<p>
coaxial cable consists of two copper conductors, but the two conductors are
concentric rather than parallel. With this construction and special
insulation and shielding, coaxial cable can achieve high data transmission
rates. Coaxial cable is quite common in cable television systems.
</p>

<p>
In cable television and cable Internet access, the transmitter shifts the
digital signal to a specific frequency band, and the resulting analog
signal is sent from the transmitter to one or more receivers.
</p>

<p>
Coaxial cable can be used as a guided <code>shared medium</code>. Specifically, a
number of end systems can be connected directly to the cable, with each of
the end systems receiving whatever is sent by the other end systems.
</p>
</div>
</div>


<div id="outline-container-org195ae65" class="outline-5">
<h5 id="org195ae65">Fiber Optics</h5>
<div class="outline-text-5" id="text-org195ae65">
<p>
An optical fiber is a thin, flexible medium that conducts pulses of light,
with each pulse representing a bit. A single optical fiber can support
tremendous bit rates, up to tens or even hundreds of gigabits per
second. They are immune to electromagnetic interference, have very low
signal attenuation up to 100 kilometers, and are very hard to tap.
</p>

<p>
preferred long-haul guided transmission media, particularly for overseas
links.
</p>

<p>
The Optical Carrier (OC) standard link speeds range from 51.8 Mbps to 39.8
Gbps
</p>
</div>
</div>

<div id="outline-container-orgc4cc62d" class="outline-5">
<h5 id="orgc4cc62d">Terrestrial Radio Channels</h5>
<div class="outline-text-5" id="text-orgc4cc62d">
<p>
Radio channels carry signals in the electromagnetic spectrum. They are an
attractive medium because they require no physical wire to be installed,
can penetrate walls, provide connectivity to a mobile user,and can
potentially carry a signal for long distances. The characteristics of a
radio channel depend significantly on the propagation environment and the
distance over which a signal is to be carried.
</p>

<p>
Environmental considerations determine path loss and shadow fading (which
decrease the signal strength as the signal travels over a distance and
around/through obstructing objects), multipath fading (due to signal
reflection off of interfering objects), and interference (due to other
transmissions and electromagnetic signals).
</p>

<p>
Terrestrial radio channels can be broadly classified into three groups:
those that operate over very short distance (e.g., with one or two meters);
those that operate in local areas, typically spanning from ten to a few
hundred meters; and those that operate in the wide area, spanning tens of
kilometers. Personal devices such as wireless headsets, keyboards, and
medical devices operate over short distances; the wireless LAN technologies
described in Section 1.2.1 use local-area radio channels; the cellular
access technologies use wide-area radio channels.
</p>
</div>
</div>

<div id="outline-container-org7487c34" class="outline-5">
<h5 id="org7487c34">Satellite Radio Channels</h5>
<div class="outline-text-5" id="text-org7487c34">
<p>
A communication satellite links two or more Earth-based microwave
transmitter/ receivers, known as ground stations. The satellite receives
transmissions on one frequency band, regenerates the signal using a
repeater (discussed below), and transmits the signal on another
frequency. Two types of satellites are used in communications:
geostationary satellites and low-earth orbiting (LEO) satellites.
</p>

<p>
Geostationary satellites permanently remain above the same spot on
Earth. This stationary presence is achieved by placing the satellite in
orbit at 36,000 kilometers above Earth’s surface. This huge distance from
ground station through satellite back to ground station introduces a
substantial signal propagation delay of 280 milliseconds. Nevertheless,
satellite links, which can operate at speeds of hundreds of Mbps, are often
used in areas without access to DSL or cable-based Internet access.
</p>

<p>
LEO satellites are placed much closer to Earth and do not remain
permanently above one spot on Earth.  They rotate around Earth (just as the
Moon does) and may communicate with each other, as well as with ground
stations. To provide continuous coverage to an area, many satellites need
to be placed in orbit. There are currently many low-altitude communication
systems in development.
</p>
</div>
</div>
</div>
</div>
<div id="outline-container-orgea821db" class="outline-3">
<h3 id="orgea821db">Network Core&#xa0;&#xa0;&#xa0;<span class="tag"><span class="networkcore">networkcore</span></span></h3>
<div class="outline-text-3" id="text-orgea821db">
<p>
the mesh of packet switches and links that interconnects the Internet’s end
systems.
</p>

<p>
imagen 1.10
</p>

<p>
There are two fundamental approaches to moving data through a network of
links and switches: circuit switching and packet switching.
</p>
</div>

<div id="outline-container-org017cd87" class="outline-4">
<h4 id="org017cd87">Packet Switching</h4>
<div class="outline-text-4" id="text-org017cd87">
<p>
In a network application, end systems exchange <code>messages</code> with each
other. Messages can contain anything the application designer
wants. Messages may perform a control function or can contain data, such as
an e-mail message, a JPEG image, or an MP3 audio file.
</p>

<p>
To send a message from a source end system to a destination end system, the
source breaks long messages into smaller chunks of data known as
<code>packets</code>. Between source and destination, each packet travels through
communication links and <code>packet switches</code> (for which there are two
predominant types, <code>routers</code> and <code>link-layer switches</code>).
</p>

<p>
Packets are transmitted over each communication link at a rate equal to the
full transmission rate of the link. So, if a source end system or a packet
switch is sending a packet of L bits over a link with transmission rate R
bits/sec, then the time to transmit the packet is L / R seconds.
</p>
</div>

<div id="outline-container-orgc8ff43b" class="outline-5">
<h5 id="orgc8ff43b">Transmision Store-and-Forward</h5>
<div class="outline-text-5" id="text-orgc8ff43b">
<p>
Store-and-forward transmission means that the packet switch must receive
the entire packet before it can begin to transmit the first bit of the
packet onto the outbound link.
</p>

<p>
imagen 1.11
</p>

<p>
A router will typically have many incident links, since its job is to
switch an incoming packet onto an outgoing link; in this simple example,
the router has the rather simple task of transferring a packet from one
(input) link to the only other attached link. In this example, the source
has three packets, each consisting of L bits, to send to the destination.
</p>

<p>
the router cannot transmit the bits it has received; instead it must first
buffer (i.e., “store”) the packet’s bits.
</p>

<p>
Only after the router has received all of the packet’s bits can it begin to
transmit (i.e., “forward”) the packet onto the outbound link.
</p>

<p>
Let’s now consider the general case of sending one packet from source to
destination over a path consisting of N links each of rate R (thus, there
are N-1 routers between source and destination).  Applying the same logic
as above, we see that the end-to-end delay is:
</p>

<p>
d<sub>end-to-end</sub> = N(\frac{L}{R})
</p>
</div>
</div>

<div id="outline-container-orge60a8a6" class="outline-5">
<h5 id="orge60a8a6">Queuing Delays and Packet Loss</h5>
<div class="outline-text-5" id="text-orge60a8a6">
<p>
Each packet switch has multiple links attached to it. For each attached
link, the packet switch has an output buffer (also called an output queue),
which stores packets that the router is about to send into that link. The
output buffers play a key role in packet switching. If an arriving packet
needs to be transmitted onto a link but finds the link busy with the
transmission of another packet, the arriving packet must wait in the output
buffer. Thus, in addition to the store-and-forward delays, packets suffer
output buffer <code>queuing delays</code>. These delays are variable and depend on the
level of congestion in the network.
</p>

<p>
an arriving packet may find that the buffer is completely full with other
packets waiting for transmission, lo que provoca <code>packet loss</code>
</p>

<p>
imagen 1.12
</p>

<p>
La imagen 1.12 muestra que puede ocurrir delay de encolado
</p>
</div>
</div>

<div id="outline-container-org73823b6" class="outline-5">
<h5 id="org73823b6">Forwarding Tables and Routing Protocols</h5>
<div class="outline-text-5" id="text-org73823b6">
<p>
a router takes a packet arriving on one of its attached communication links
and forwards that packet onto another one of its attached communication
links. But how does the router determine which link it should forward the
packet onto?
</p>

<p>
In the Internet, every end system has an address called an IP address. When
a source end system wants to send a packet to a destination end system, the
source includes the destination’s IP address in the packet’s header.
</p>

<p>
As with postal addresses, this address has a hierarchical structure. When a
packet arrives at a router in the network, the router examines a portion of
the packet’s destination address and forwards the packet to an adjacent
router. More specifically, each router has a <code>forwarding table</code> that maps
destination addresses (or portions of the destination addresses) to that
router’s outbound links.  When a packet arrives at a router, the router
examines the address and searches its forwarding table, using this
destination address, to find the appropriate outbound link. The router then
directs the packet to this outbound link.
</p>

<p>
The end-to-end routing process is analogous to a car driver who does not
use maps but instead prefers to ask for directions.
</p>

<p>
a router uses a packet’s destination address to index a forwarding table
and determine the appropriate outbound link. But this statement begs yet
another question: How do forwarding tables get set? Are they configured by
hand in each and every router, or does the Internet use a more automated
procedure? This issue will be studied in depth in Chapter 5.
</p>

<p>
the Internet has a number of special <code>routing protocols</code> that are used to
automatically set the forwarding tables. A routing protocol may, for
example, determine the shortest path from each router to each destination
and use the shortest path results to configure the forwarding tables in the
routers.
</p>
</div>
</div>
</div>

<div id="outline-container-org960bc36" class="outline-4">
<h4 id="org960bc36">Circuit Switching</h4>
<div class="outline-text-4" id="text-org960bc36">
<p>
In circuit-switched networks, the resources needed along a path (buffers,
link transmission rate) to provide for communication between the end systems
are reserved for the duration of the communication session between the end
systems.
</p>

<p>
In packet-switched networks, these resources are not reserved; a session’s
messages use the resources on demand and, as a consequence, may have to wait
(that is, queue) for access to a communication link.
</p>

<p>
Before the sender can send the information, the network must establish a
connection between the sender and the receiver. This is a bona fide
connection for which the switches on the path between the sender and
receiver maintain connection state for that connection. In the jargon of
telephony, this connection is called a <code>circuit</code>.
</p>

<p>
imagen 1.13 &#x2026; (no se si hace falta)
</p>


<p>
The Internet makes its best effort to deliver packets in a timely manner,
but it does not make any guarantees.
</p>
</div>

<div id="outline-container-orgab7aaec" class="outline-5">
<h5 id="orgab7aaec">Multiplexing in Circuit-Switched Networks</h5>
<div class="outline-text-5" id="text-orgab7aaec">
<p>
A circuit in a link is implemented with either frequency-division
multiplexing (FDM) or time-division multiplexing (TDM).
</p>

<p>
With FDM, the frequency spectrum of a link is divided up among the
connections established across the link. Specifically, the link dedicates a
frequency band to each connection for the duration of the connection. FM
radio stations also use FDM to share the frequency spectrum between 88 MHz
and 108 MHz, with each station being allocated a specific frequency band.
</p>

<p>
For a TDM link, time is divided into frames of fixed duration, and each
frame is divided into a fixed number of time slots. When the network
establishes a connection across a link, the network dedicates one time slot
in every frame to this connection. These slots are dedicated for the sole
use of that connection, with one time slot available for use (in every
frame) to transmit the connection’s data.
</p>

<p>
imagen 1.14
</p>

<p>
With FDM, each circuit continuously gets a fraction of the bandwidth. With
TDM, each circuit gets all of the bandwidth periodically during brief
intervals of time (that is, during slots)
</p>

<p>
Proponents of packet switching have always argued that circuit switching is
wasteful because the dedicated circuits are idle during <code>silent
periods</code>. For example, when one person in a telephone call stops talking,
the idle network resources (frequency bands or time slots in the links
along the connection’s route) cannot be used by other ongoing connections.
</p>

<p>
Proponents of packet switching also enjoy pointing out that establishing
end-to-end circuits and reserving end-to-end transmission capacity is
complicated and requires complex signaling software to coordinate the
operation of the switches along the end-to-end path.
</p>
</div>
</div>

<div id="outline-container-orge237a6e" class="outline-5">
<h5 id="orge237a6e">Packet Switching Versus Circuit Switching</h5>
<div class="outline-text-5" id="text-orge237a6e">
<p>
Critics of packet switching have often argued that packet switching is not
suitable for real-time services (for example, telephone calls and video
conference calls) because of its variable and unpredictable end-to-end
delays (due primarily to variable and unpredictable queuing delays).
</p>

<p>
Proponents of packet switching argue that (1) it offers better sharing of
transmission capacity than circuit switching and (2) it is simpler, more
efficient, and less costly to implement than circuit switching.
</p>



<p>
Circuit switching pre-allocates use of the transmission link regardless of
demand, with allocated but unneeded link time going unused.
</p>

<p>
Packet switching on the other hand allocates link use on demand. Link
transmission capacity will be shared on a packet-by-packet basis only among
those users who have packets that need to be transmitted over the link.
</p>
</div>
</div>
</div>


<div id="outline-container-org911797e" class="outline-4">
<h4 id="org911797e">A Network of Networks</h4>
<div class="outline-text-4" id="text-org911797e">
<p>
Recall that the overarching goal is to interconnect the access ISPs so that
all end systems can send packets to each other.
</p>

<p>
One naive approach would be to have each access ISP directly connect with
every other access ISP. Such a <code>mesh design</code> is, of course, much too costly
for the access ISPs, as it would require each access ISP to have a separate
communication link to each of the hundreds of thousands of other access ISPs
all over the world.
</p>

<dl class="org-dl">
<dt>Network Structure 1</dt><dd>interconnects all of the access ISPs with a single
global transit ISP. Our (imaginary) global transit ISP is a network of
routers and communication links that not only spans the globe, but also
has at least one router near each of the hundreds of thousands of
access ISPs. Of course, it would be very costly for the global ISP to
build such an extensive network. To be profitable, it would naturally
charge each of the access ISPs for connectivity, with the pricing
reflecting (but not necessarily directly proportional to) the amount of
traffic an access ISP exchanges with the global ISP. Since the access
ISP pays the global transit ISP, the access ISP is said to be a
customer and the global transit ISP is said to be a provider.</dd>

<dt>Network Structure 2</dt><dd><p>
which consists of the hundreds of thousands of
access ISPs and multiple global ­ transit ISPs. The access ISPs
certainly prefer Network Structure 2 over Network Structure 1 since
they can now choose among the competing global transit providers as a
function of their pricing and services. Note, however, that the global
transit ISPs themselves must interconnect: Otherwise access ISPs
connected to one of the global transit providers would not be able to
communicate with access ISPs connected to the other global transit
providers.
</p>

<p>
is a two-tier hierarchy with global transit providers residing at the
top tier and access ISPs at the bottom tier. This assumes that global
transit ISPs are not only capable of getting close to each and every
access ISP, but also find it economically desirable to do so. In
reality, although some ISPs do have impressive global coverage and do
directly connect with many access ISPs, no ISP has presence in each and
every city in the world. Instead, in any given region, there may be a
<code>regional ISP</code> to which the access ISPs in the region connect. Each
regional ISP then connects to tier-1 ISPs. Tier-1 ISPs are similar to
our (imaginary) global transit ISP; but tier-1 ISPs, which actually do
exist, do not have a presence in every city in the world.
</p></dd>

<dt>Network Structure 3</dt><dd><p>
not only are there multiple competing tier-1 ISPs,
there may be multiple competing regional ISPs in a region. In such a
hierarchy, each access ISP pays the regional ISP to which it connects,
and each regional ISP pays the tier-1 ISP to which it connects. (An
access ISP can also connect directly to a tier-1 ISP, in which case it
pays the tier-1 ISP). Thus, there is customer- provider relationship at
each level of the hierarchy. Note that the tier-1 ISPs do not pay
anyone as they are at the top of the hierarchy. To further complicate
matters, in some regions, there may be a larger regional ISP (possibly
spanning an entire country) to which the smaller regional ISPs in that
region connect; the larger regional ISP then connects to a tier-1
ISP. For example, in China, there are access ISPs in each city, which
connect to provincial ISPs, which in turn connect to national ISPs,
which finally connect to tier-1 ISPs.
</p>

<p>
multi-tier hierarchy
</p></dd>

<dt>Network Structure 4</dt><dd>Ecosystem consisting of access ISPs, regional ISPs,
tier-1 ISPs, PoPs, multi-homing, peering, and IXPs
<ul class="org-ul">
<li><code>Points of presence (PoPs)</code>: PoPs exist in all levels of the
hierarchy, except for the bottom (access ISP) level. A PoP is simply
a group of one or more routers (at the same location) in the
provider’s network where customer ISPs can connect into the provider
ISP. For a customer network to connect to a provider’s PoP, it can
lease a high-speed link from a third-party telecommunications
provider to directly connect one of its routers to a router at the
PoP.</li>
<li><code>Multi-home</code>: Any ISP (except for tier-1 ISPs) may choose to
multi-home, that is, to connect to two or more provider ISPs. So, for
example, an access ISP may multi-home with two regional ISPs, or it
may multi-home with two regional ISPs and also with a tier-1
ISP. Similarly, a regional ISP may multi-home with multiple tier-1
ISPs. When an ISP multi-homes, it can continue to send and receive
packets into the Internet even if one of its providers has a failure.</li>
<li><code>Peering</code>: The amount that a customer ISP pays a provider ISP
reflects the amount of traffic it exchanges with the provider. To
reduce these costs, a pair of nearby ISPs at the same level of the
hierarchy can peer, that is, they can directly connect their networks
together so that all the traffic between them passes over the direct
connection rather than through upstream intermediaries. When two ISPs
peer, it is typically settlement-free, that is, neither ISP pays the
other. As noted earlier, tier-1 ISPs also peer with one another,
settlement-free.</li>
<li><code>Internet Exchange Point (IXP)</code>: a third-party company can create an
Internet Exchange Point (IXP), which is a meeting point where
multiple ISPs can peer together. An IXP is typically in a stand-alone
building with its own switches</li>
</ul></dd>

<dt>Network Structure 5</dt><dd><p>
describes today’s Internet. builds on top of
Network Structure 4 by adding <code>content-provider networks</code> or <code>content
  delivery networks</code>.
</p>

<p>
Google is currently one of the leading examples of such a
content-provider network. As of this writing, it is estimated that
Google has 50–100 data centers distributed across North America,
Europe, Asia, South America, and Australia. Some of these data centers
house over one hundred thousand servers, while other data centers are
smaller, housing only hundreds of servers. The Google data centers are
all interconnected via Google’s private TCP/IP network, which spans the
entire globe but is nevertheless separate from the public
Internet. Importantly, the Google private network only carries traffic
to/from Google servers. As shown in Figure 1.15, the Google private
network attempts to “bypass” the upper tiers of the Internet by peering
(settlement free) with lower-tier ISPs, either by directly connecting
with them or by connecting with them at IXPs.
</p>

<p>
However, because many access ISPs can still only be reached by
transiting through tier-1 networks, the Google network also connects to
tier-1 ISPs, and pays those ISPs for the traffic it exchanges with
them. By creating its own network, a contentprovider not only reduces
its payments to upper-tier ISPs, but also has greater control of how
its services are ultimately delivered to end users.
</p>

<p>
imagen 1.15
</p></dd>
</dl>
</div>
</div>
</div>

<div id="outline-container-org3b20ba8" class="outline-3">
<h3 id="org3b20ba8">Delay, Perdida de Paquetes y Throughput en Redes de Conmutadores-de-Paquetes</h3>
<div class="outline-text-3" id="text-org3b20ba8">
<p>
computer networks:
</p>
<ul class="org-ul">
<li>constrain throughput (the amount of data per second that can be transferred)
between end systems</li>
<li>introduce delays between end systems</li>
<li>can lose packets</li>
</ul>
</div>

<div id="outline-container-org0f82d60" class="outline-4">
<h4 id="org0f82d60">Overview of Delay in Packet-Switched Networks</h4>
<div class="outline-text-4" id="text-org0f82d60">
<p>
A medida que un paquete es transmitido entre end-systems, este sufre de
varios tipos de delay en cada nodo a lo largo de una ruta.
</p>
<ul class="org-ul">
<li>delay de procesamiento del nodo <code>nodal processing delay</code></li>
<li>delay de encolado <code>queuing delay</code></li>
<li>delay de transmision <code>transmission delay</code></li>
<li><p>
delay de prograpagion <code>propagation delay</code>
</p>

<p>
la suma de todos los delays se llama <code>delay del nodo</code>.
</p></li>
</ul>
</div>

<div id="outline-container-org22b454c" class="outline-5">
<h5 id="org22b454c">Tipos de Delay</h5>
<div class="outline-text-5" id="text-org22b454c">
<p>
imagen 1.16
</p>

<p>
Our goal is to characterize the nodal delay at router A.
</p>

<p>
As part of its end-to-end route between source and destination, a packet is
sent from the upstream node through router A to router B.
</p>

<p>
Note that router A has an outbound link leading to router B.
</p>

<p>
This link is preceded by a queue (also known as a buffer).
</p>

<p>
When the packet arrives at router A from the upstream node, router A
examines the packet’s header to determine the appropriate outbound link for
the packet and then directs the packet to this link.
</p>

<p>
In this example, the outbound link for the packet is the one that leads to
router B.
</p>

<p>
A packet can be transmitted on a link only if there is no other packet
currently being transmitted on the link and if there are no other packets
preceding it in the queue; if the link is currently busy or if there are
other packets already queued for the link, the newly arriving packet will
then join the queue.
</p>
</div>

<div id="outline-container-org2d996a9" class="outline-6">
<h6 id="org2d996a9">Processing delay</h6>
<div class="outline-text-6" id="text-org2d996a9">
<p>
es el tiempo requerido para examinar el encabezado de un paquete y
determinar a donde redireccionar el packet.
</p>

<p>
incluye otros factores:
</p>
<ul class="org-ul">
<li><p>
tiempo de verificacion de errores a nivel de bits que ocurrieron durante el
arribo del paquete.
</p>

<p>
luego del procesado del paquete, se envia a la cola del enlace que lleva al
destino.
</p></li>
</ul>
</div>
</div>

<div id="outline-container-orgd4b37bc" class="outline-6">
<h6 id="orgd4b37bc">Queuing Delay</h6>
<div class="outline-text-6" id="text-orgd4b37bc">
<p>
tiempo de espera en la cola hasta que el paquete sea transmitido por el enlace.
</p>

<p>
depende de la cantidad de paquetes que arribaron antes a la cola y se encuentran
esperado.
</p>

<p>
si no hay otros paquetes, el delay es 0.
</p>

<p>
se encuentran en el orden de microsegundos a milisegundos.
</p>
</div>
</div>

<div id="outline-container-org5d33c6e" class="outline-6">
<h6 id="org5d33c6e">Transmission Delay</h6>
<div class="outline-text-6" id="text-org5d33c6e">
<p>
tiempo en que se tarda en enviar todo el paquete por el enlace.
</p>

<p>
depende del largo del paquete (L bits) y la velocidad de transmision del enlace
(R bits/seg)
</p>

<p>
el delay es \(L/R\).
</p>

<p>
se encuentran en el orden de microsegundos a milisegundos.
</p>
</div>
</div>

<div id="outline-container-org6c99887" class="outline-6">
<h6 id="org6c99887">Propagation Delay</h6>
<div class="outline-text-6" id="text-org6c99887">
<p>
es el tiempo de propagacion por el enlace entre los nodos.
</p>

<p>
depende del medio fisico del enlace:
</p>
<ul class="org-ul">
<li>fibra optica</li>
<li>aire</li>
<li>cobre</li>
<li><p>
etc
</p>

<p>
is in the range of 2⋅108 meters/sec to 3⋅108 meters/sec or a little less
than, the speed of light.
</p>

<p>
es la distancia entre nodos divido la velocidad de propagacion
</p>

<p>
In WANs, propagation delays are on the order of milliseconds.
</p></li>
</ul>
</div>
</div>

<div id="outline-container-orgd54bd0b" class="outline-6">
<h6 id="orgd54bd0b">delay de transmision vs delay de propagacion</h6>
<div class="outline-text-6" id="text-orgd54bd0b">
<dl class="org-dl">
<dt>The transmission delay</dt><dd>the amount of time required for the router to
push out the packet; it is a function of the packet’s length and the
transmission rate of the link, but has nothing to do with the distance
between the two routers.</dd>

<dt>The propagation delay</dt><dd><p>
the time it takes a bit to propagate from one
router to the next; it is a function of the distance between the two
routers, but has nothing to do with the packet’s length or the
transmission rate of the link.
</p>

<p>
imagen 1.17
</p></dd>
</dl>
</div>
</div>
</div>
</div>

<div id="outline-container-orge797e36" class="outline-4">
<h4 id="orge797e36">Queuing Delay and Packet Loss</h4>
<div class="outline-text-4" id="text-orge797e36">
<p>
el delay de encolado puede variar de paquete a paquete.
</p>

<p>
si 10 paquetes arriban a una cola vacia, el primero no sufre de latencia,
mientras que el ultimo debe esperar a los 9 paquetes anteriores sean
transmitidos por el enlace.
</p>

<p>
para caracterizar el tiempo de encolado se usan medidas estadisticas:
</p>
<ul class="org-ul">
<li>promedio</li>
<li>varianza</li>
<li><p>
probabilidad de que el delay exceda cierto umbral
</p>

<p>
When is the queuing delay large and when is it insignificant?  it depends
on:
</p>
<ul class="org-ul">
<li>the rate at which traffic arrives at the queue</li>
<li>the transmission rate of the link</li>
<li><p>
the nature of the arriving traffic, that is, whether the traffic arrives
periodically or arrives in bursts.
</p>

<p>
the average rate at which bits arrive at the queue is \(L_{a}\) bits/sec.
</p>

<p>
The ratio \(L_{a}/R\), called the <code>traffic intensity</code>, estimates the queuing
delay.
</p>

<p>
si L<sub>a</sub>/R &gt; 1, los paquetes llegan a la cola a una mayor a la que se pueden
transmitir, por lo que la cola crece y el delay tiende a infinito.
</p>

<p>
Therefore, one of the golden rules in traffic engineering is: <i>Design your
system so that the traffic intensity is no greater than 1</i>.
</p>

<p>
si \(L_{a}/R \leq 1\), el delay depende de la naturaleza de los arribos:
</p>
<ul class="org-ul">
<li>arribos periodicos cada L/R segundos, cada paquete arriba a una cola vacia</li>
<li>arribos periodicos pero por rafagas de paquetes, por ej: arriban N paquetes de
forma simultanea cada (L/R)N segundos, entonces el primer paquete no tiene
delay, mientras que el N-esimo paquete tiene delay de encolado de (N-1)L/R</li>
</ul>

<p>
the average queuing delay (creo que es) \(\frac{L}{R}\frac{n+1}{2}\)
</p>

<p>
Si la intensidad de trafico es cercana a 0, tambien los es el delay de
encolado
</p>

<p>
imagen 1.18
</p>

<p>
a medida que la intensidad de trafico se acerca a 1, el delay de encolado
promedio incrementa rapidamente.
</p></li>
</ul></li>
</ul>
</div>

<div id="outline-container-org74f97bf" class="outline-5">
<h5 id="org74f97bf">Packet Loss</h5>
<div class="outline-text-5" id="text-org74f97bf">
<p>
debido a que la capacidad de un buffer es finita, el delay de encolado no se
acerca a infinito a medida que la intensidad de trafico se acerca a 1.
</p>

<p>
cuando una cola esta llena, un paquete entrante no tiene lugar por lo que un
router <code>descarta</code> dicho paquete, es decir que se <code>pierde</code> el paquete.
</p>

<p>
desde el punto de vista del end-system, el paquete se envia a la red, pero este
nunca emerge de la red hacia el destino.
</p>

<p>
la cantidad de paquetes perdidos incrementa a medida que lo hace la intensidad
de trafico.
</p>

<p>
la performance de un nodo tambien se puede medir en terminos de probabilidad de
que haya perdida de paquetes.
</p>
</div>
</div>
</div>

<div id="outline-container-org39bd5a2" class="outline-4">
<h4 id="org39bd5a2">End-to-End Delay</h4>
<div class="outline-text-4" id="text-org39bd5a2">
<p>
Let’s now consider the total delay from source to destination. To get a
handle on this concept, suppose there are N−1 routers between the source
host and the destination host. Let’s also suppose for the moment that the
network is uncongested (so that queuing delays are negligible), the
processing delay at each router and at the source host is d proc , the
transmission rate out of each router and out of the source host is R
bits/sec, and the propagation on each link is d prop . The nodal delays
accumulate and give an end-to- end delay,
</p>

<p>
\[dend−end = N(dproc+dtrans+dprop)\]
</p>

<p>
where, once again, dtrans=L/R, where L is the packet size.
</p>
</div>
</div>

<div id="outline-container-orge287080" class="outline-4">
<h4 id="orge287080">Throughput</h4>
<div class="outline-text-4" id="text-orge287080">
<p>
To define throughput, consider transferring a large file from Host A to Host
B across a computer network. This transfer might be, for example, a large
video clip from one peer to another in a P2P file sharing system.
</p>

<p>
The <code>instantaneous throughput</code> at any instant of time is the rate (in
bits/sec) at which a Host is receiving a file.
</p>

<p>
If the file consists of F bits and the transfer takes T seconds for a Host to
receive all F bits, then the <code>average throughput</code> of the file transfer is F/T
bits/sec.
</p>

<p>
imagen 1.19
</p>

<p>
Figure 1.19(a) shows two end systems, a server and a client, connected by
two communication links and a router.
</p>

<p>
Consider the throughput for a file transfer from the server to the client.
</p>
<ul class="org-ul">
<li>Let \(R_{s}\) denote the rate of the link between the server and the router</li>
<li><p>
Let \(R_{c}\) denote the rate of the link between the router and the client
</p>

<p>
Suppose that the only bits being sent in the entire network are those from
the server to the client.  We now ask, in this ideal scenario, what is the
server-to-client throughput?
</p>

<p>
the server cannot pump bits through its link at a rate faster than
\(R_{s}\) bps; and the router cannot forward bits at a rate faster than
\(R_{c}\) bps.
</p>

<p>
If \(R_{s}\)&lt;\(R_{c}\), then the bits pumped by the server will “flow” right
through the router and arrive at the client at a rate of \(R_{s}\) bps, giving
a throughput of \(R_{s}\) bps.
</p>

<p>
If \(R_{c}\)&lt;\(R_{s}\), then the router will not be able to forward bits as
quickly as it receives them. In this case, bits will only leave the router at
rate \(R_{c}\) , giving an end-to-end throughput of \(R_{c}\) .
</p>

<p>
For this simple two-link network, the throughput is \(min\{R_{c}, R_{s} \}\),
that is, it is the transmission rate of the <code>bottleneck link</code>.
</p>

<p>
Figure 1.19(b) now shows a network with N links between the server and the
client, with the transmission rates of the N links being R1,R2,&#x2026;,
RN. Applying the same analysis as for the two-link network, we find that the
throughput for a file transfer from server to client is \(min \{R1,R2,...,
  RN\}\) , which is once again the transmission rate of the bottleneck link
along the path between server and client.
</p>

<p>
imagen 1.20
</p>

<p>
the constraining factor for throughput in today’s Internet is typically the
access network.
</p>

<blockquote>
<p>
when there is no other intervening traffic, the throughput can simply be
approximated as the minimum transmission rate along the path between source
and destination.
</p>
</blockquote>

<blockquote>
<p>
The example in Figure 1.20(b) shows that more generally the throughput
depends not only on the transmission rates of the links along the path, but
also on the intervening traffic.
</p>

<p>
In particular, a link with a high transmission rate may nonetheless be the
bottleneck link for a file transfer if many other data flows are also
passing through that link.
</p>
</blockquote></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org8c188ed" class="outline-3">
<h3 id="org8c188ed">Capas de protocolos y sus servicios</h3>
<div class="outline-text-3" id="text-org8c188ed">
</div>
<div id="outline-container-org74f8a80" class="outline-4">
<h4 id="org74f8a80">Arquitectura de capas</h4>
<div class="outline-text-4" id="text-org74f8a80">
<p>
Una actividad compleja puede dividirse entre capas, cada una implementando
una funcionalidad. Cada capa, combinada con las capas inferiores, provee mas
funcionalidades/servicios.
</p>

<p>
Each layer provides its service by
</p>
<ol class="org-ol">
<li>performing certain actions within that layer and by</li>
<li><p>
using the services of the layer directly below it
</p>

<p>
una arquitectura de capas permite especificar parte un sistema complejo y
grande. tambien permite la modularizacion facilitando el cambio de
implementacion de un servicio provisto por la capa. mientras que la capa
proporcione el mismo servicio a la capa superior y utilice los mismos
serivicios de las capas inferiores, el resto del sistema no se ve alterado.
</p>

<p>
organizacion de capas provee estructura para diseño
</p></li>
</ol>
</div>

<div id="outline-container-org7109060" class="outline-5">
<h5 id="org7109060">protocol layering</h5>
<div class="outline-text-5" id="text-org7109060">
<p>
los protocolos pertenecan a cada capa.
</p>

<dl class="org-dl">
<dt>modelo de servicios de una capa</dt><dd><p>
nos interesan los servicios que cada
capa ofrece a la capa superior.
</p>

<p>
un protocolo de capa puede ser implementado en HW o SW o una combinacion de
ambos
</p>

<p>
desventajas:
</p>
<dl class="org-dl">
<dt>posible duplicacion de funcionalidad</dt><dd>recuperacion de errores</dd>
<dt>violacion de separacion de capas</dt><dd><p>
puede ser que una capa requiera
informacion disponible en otra capa
</p>

<p>
imagen 1.23
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 1:</span> Five layer Internet protocol stack</caption>

<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Application</td>
</tr>

<tr>
<td class="org-left">Transport</td>
</tr>

<tr>
<td class="org-left">Network</td>
</tr>

<tr>
<td class="org-left">Link</td>
</tr>

<tr>
<td class="org-left">Physical</td>
</tr>
</tbody>
</table>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 2:</span> Seven layer ISO OSI reference model</caption>

<colgroup>
<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Application</td>
</tr>

<tr>
<td class="org-left">Presentation</td>
</tr>

<tr>
<td class="org-left">Session</td>
</tr>

<tr>
<td class="org-left">Transport</td>
</tr>

<tr>
<td class="org-left">Network</td>
</tr>

<tr>
<td class="org-left">Link</td>
</tr>

<tr>
<td class="org-left">Physical</td>
</tr>
</tbody>
</table>

<p>
When taken together, the protocols of the various layers are called the
<code>protocol stack</code>. The Internet protocol stack consists of five layers: the
physical, link, network, transport, and application layers
</p></dd>
</dl></dd>
</dl>
</div>


<div id="outline-container-org422590b" class="outline-6">
<h6 id="org422590b">Application Layer</h6>
<div class="outline-text-6" id="text-org422590b">
<p>
The application layer is where network applications and their
application-layer protocols reside.
</p>

<p>
The Internet’s application layer includes many protocols, such as the
</p>
<ul class="org-ul">
<li>HTTP protocol (which provides for Web document request and transfer),</li>
<li>SMTP (which provides for the transfer of e-mail messages), and</li>
<li>FTP (which provides for the transfer of files between two end systems).</li>
<li><p>
DNS (which translates human-friendly names for Internet end systems like
www.ietf.org to a 32-bit network address)
</p>

<p>
An application-layer protocol is distributed over multiple end systems,
with the application in one end system using the protocol to exchange
packets of information with the application in another end system. We’ll
refer to this packet of information at the application layer as a
<code>message</code>.
</p></li>
</ul>
</div>
</div>

<div id="outline-container-org92b5507" class="outline-6">
<h6 id="org92b5507">Transport Layer</h6>
<div class="outline-text-6" id="text-org92b5507">
<p>
The Internet’s transport layer transports application-layer messages
between application endpoints. In the Internet there are two transport
protocols, TCP and UDP, either of which can transport application- layer
messages.
</p>

<p>
TCP provides a ­ connection-oriented service to its applications. This
service includes guaranteed delivery of application-layer messages to the
destination and flow control (that is, sender/receiver speed matching). TCP
also breaks long messages into shorter ­ segments and provides a
congestion-control mechanism, so that a source throttles its transmission
rate when the network is congested.
</p>

<p>
The UDP protocol provides a connectionless service to its
applications. This is a no-frills service that provides no reliability, no
flow control, and no congestion control. In this book, we’ll refer to a
transport-layer packet as a <code>segment</code>.
</p>
</div>
</div>

<div id="outline-container-orgd4f9ef9" class="outline-6">
<h6 id="orgd4f9ef9">Network Layer</h6>
<div class="outline-text-6" id="text-orgd4f9ef9">
<p>
The network layer is responsible for moving network-layer packets known as
<code>datagrams</code> from one host to another. The Internet transport-layer protocol
(TCP or UDP) in a source host passes a transport-layer segment and a
destination address to the network layer, just as you would give the postal
service a letter with a destination address. The network layer then
provides the service of delivering the segment to the transport layer in
the destination host.
</p>

<p>
The Internet’s network layer includes the celebrated IP protocol, which
defines the fields in the datagram as well as how the end systems and
routers act on these fields. There is only one IP protocol, and all
Internet components that have a network layer must run the IP
protocol. The Internet’s network layer also contains routing protocols
that determine the routes that datagrams take between sources and
destinations.
</p>
</div>
</div>

<div id="outline-container-org5c7de7a" class="outline-6">
<h6 id="org5c7de7a">Link Layer</h6>
<div class="outline-text-6" id="text-org5c7de7a">
<p>
delivers the datagram to the next node along the route.
</p>

<p>
The services provided by the link layer depend on the specific link-layer
protocol that is employed over the link. For example, some link-layer
protocols provide reliable delivery, from transmitting node, over one
link, to receiving node. Note that this reliable delivery service is
different from the reliable delivery service of TCP, which provides
reliable delivery from one end system to another.
</p>

<p>
Examples of link-layer protocols include Ethernet, WiFi, and the cable
access network’s DOCSIS protocol.
</p>

<p>
As datagrams typically need to traverse several links to travel from
source to destination, a datagram may be handled by different link-layer
protocols at different links along its route.
</p>

<p>
link-layer packets are refered as <code>frames</code>.
</p>
</div>
</div>

<div id="outline-container-org6297301" class="outline-6">
<h6 id="org6297301">Physical Layer</h6>
<div class="outline-text-6" id="text-org6297301">
<p>
While the job of the link layer is to move entire frames from one network
element to an adjacent network element, the job of the physical layer is
to move the individual bits within the frame from one node to the next.
</p>

<p>
The protocols in this layer are again link dependent and further depend on
the actual transmission medium of the link (for example, twisted-pair
copper wire, single-mode fiber optics).
</p>

<p>
For example, Ethernet has many physical-layer protocols: one for
twisted-pair copper wire, another for coaxial cable, another for fiber,
and so on. In each case, a bit is moved across the link in a different
way.
</p>
</div>
</div>

<div id="outline-container-org09b9b61" class="outline-6">
<h6 id="org09b9b61">Open Systems Interconnection (OSI) Model</h6>
<div class="outline-text-6" id="text-org09b9b61">
<p>
propuesto por la International Organization for Standardization (ISO).
</p>

<p>
tiene 7 capas.
</p>

<p>
The functionality of five of these layers is roughly the same as their
similarly named Internet counterparts.
</p>

<p>
The role of the presentation layer is to provide services that allow
communicating applications to interpret the meaning of data
exchanged. These services include data compression and data encryption as
well as data description (which frees the applications from having to worry
about the internal format in which data are represented/stored-formats that
may differ from one computer to another).
</p>

<p>
The session layer provides for delimiting and synchronization of data
exchange, including the means to build a checkpointing and recovery scheme.
</p>

<p>
In the Internet model, these services are delegated to the Application
Layer.
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orgc5389ec" class="outline-4">
<h4 id="orgc5389ec">Encapsulado</h4>
<div class="outline-text-4" id="text-orgc5389ec">
<p>
Figure 1.24 shows the physical path that data takes down a sending end
system’s protocol stack, up and down the protocol stacks of an intervening
link-layer switch and router, and then up the protocol stack at the receiving
end system.
</p>

<p>
imagen 1.24
</p>

<ul class="org-ul">
<li>link-layer switches implement layers 1 and 2;</li>
<li><p>
routers implement layers 1 through 3.
</p>

<p>
Internet routers are capable of implementing the IP protocol (a layer 3
protocol), while link-layer switches are not.
</p>

<blockquote>
<p>
Note that hosts implement all five layers; this is consistent with the view
that the Internet architecture puts much of its complexity at the edges of
the network.
</p>
</blockquote></li>
</ul>


<p>
Figure 1.24 also illustrates the important concept of <code>encapsulation</code>. At the
sending host, an <code>application-layer message</code> is passed to the transport
layer. In the simplest case, the transport layer takes the message and
appends additional information that will be used by the receiver-side
transport layer. The application-layer message and the transport-layer header
information together constitute the <code>transport-layer segment</code>. The
transport-layer segment thus encapsulates the application-layer message.
</p>

<p>
The added information might include information allowing the receiver-side
transport layer to deliver the message up to the appropriate application, and
error-detection bits that allow the receiver to determine whether bits in the
message have been changed in route.
</p>

<p>
The transport layer then passes the segment to the network layer, which adds
network-layer header information such as source and destination end system
addresses, creating a <code>network-layer datagram</code>.
</p>

<p>
The datagram is then passed to the link layer, which will add its own
link-layer header information and create a <code>link-layer frame</code>.
</p>

<p>
at each layer, a packet has two types of fields: header fields and a <code>payload
field</code>. The payload is typically a packet from the layer above.
</p>

<p>
The process of encapsulation can be more complex than that described
above. For example, a large message may be divided into multiple
transport-layer segments (which might themselves each be divided into
multiple network-layer datagrams). At the receiving end, such a segment must
then be reconstructed from its constituent datagrams.
</p>
</div>
</div>
</div>

<div id="outline-container-org3dd02a0" class="outline-3">
<h3 id="org3dd02a0">Networks Under Attack</h3>
<div class="outline-text-3" id="text-org3dd02a0">
<p>
Viruses are malware that require some form of user interaction to infect the
user’s device. The classic example is an e-mail attachment containing
malicious executable code. If a user receives and opens such an attachment,
the user inadvertently runs the malware on the device.  Typically, such e-mail
viruses are self-replicating: once executed, the virus may send an identical
message with an identical malicious attachment to, for example, every
recipient in the user’s address book.
</p>

<p>
Worms are malware that can enter a device without any explicit user
interaction. For example, a user may be running a vulnerable network
application to which an attacker can send malware. In some cases, without any
user intervention, the application may accept the malware from the Internet
and run it, creating a worm. The worm in the newly infected device then scans
the Internet, searching for other hosts running the same vulnerable network
application. When it finds other vulnerable hosts, it sends a copy of itself
to those hosts.
</p>

<p>
Another broad class of security threats are known as <code>denial-of-service (DoS)</code>
attacks. As the name suggests, a DoS attack renders a network, host, or other
piece of infrastructure unusable by legitimate users. Web servers, e-mail
servers, DNS servers, and institutional networks can all be subject to DoS
attacks. Internet DoS attacks are extremely common, with thousands of DoS
ttacks occurring every year.
</p>

<p>
Most Internet DoS attacks fall into one of three categories:
</p>
<dl class="org-dl">
<dt>Vulnerability attack</dt><dd>This involves sending a few well-crafted messages to
a vulnerable application or operating system running on a targeted host. If
the right sequence of packets is sent to a vulnerable application or
operating system, the service can stop or, worse, the host can crash.</dd>
<dt>Bandwidth flooding</dt><dd>The attacker sends a deluge of packets to the targeted
host-so many packets that the target’s access link becomes clogged,
preventing legitimate packets from reaching the server.</dd>
<dt>Connection flooding</dt><dd>The attacker establishes a large number of half-open
or fully open TCP connections (TCP connections are discussed in Chapter 3)
at the target host. The host can become so bogged down with these bogus
connections that it stops accepting legitimate connections.</dd>
</dl>
</div>
</div>
</div>

<div id="outline-container-org1071425" class="outline-2">
<h2 id="org1071425">Capa de Aplicacion y HTTP</h2>
<div class="outline-text-2" id="text-org1071425">
</div>
<div id="outline-container-org05601d9" class="outline-3">
<h3 id="org05601d9">Principios de aplicaciones de red</h3>
<div class="outline-text-3" id="text-org05601d9">
<p>
At the core of network application development is writing programs that run on
different end systems and communicate with each other over the network.
</p>

<p>
For example, in the Web application there are two distinct programs that
communicate with each other: the browser program running in the user’s host
(desktop, laptop, tablet, smartphone, and so on); and the Web server program
running in the Web server host.
</p>

<p>
As another example, in a P2P file-sharing system there is a program in each
host that participates in the file-sharing community. In this case, the
programs in the various hosts may be similar or identical.
</p>

<p>
Thus, when developing your new application, you need to write software that
will run on multiple end systems.
</p>

<p>
network-core devices do not function at the application layer but instead
function at lower layers-specifically at the network layer and below.  This
basic design-namely, confining application software to the end systems-as
shown in Figure 2.1, has facilitated the rapid development and deployment of a
vast array of network applications.
</p>

<p>
imagen 2.1
</p>
</div>

<div id="outline-container-orgee0fe2d" class="outline-4">
<h4 id="orgee0fe2d">Arquitecturas de Aplicaciones de Red</h4>
<div class="outline-text-4" id="text-orgee0fe2d">
<p>
The <code>application architecture</code>, is designed by the application developer and
dictates how the application is structured over the various end systems.
</p>

<p>
In choosing the application architecture, an application developer will
likely draw on one of the two predominant architectural paradigms used in
modern network applications: the <code>client-server architecture</code> or the
<code>peer-to-peer (P2P) architecture</code>.
</p>

<p>
In a <code>client-server architecture</code> , there is an always-on host, called the
<code>server</code>, which services requests from many other hosts, called <code>clients</code>.
</p>

<ul class="org-ul">
<li>clients do not directly communicate with each other</li>
<li>the server has a fixed, well-known address, called an IP address</li>
<li><p>
Web, FTP, Telnet, and e-mail
</p>

<p>
imagen 2.2a
</p>

<p>
Often in a client-server application, a single-server host is incapable of
keeping up with all the requests from clients.
</p>

<p>
For this reason, a data center, housing a large number of hosts, is often
used to create a powerful virtual server.
</p>

<p>
A data center can have hundreds of thousands of servers, which must be
powered and maintained. Additionally, the service providers must pay
recurring interconnection and bandwidth costs for sending data from their
data centers.
</p>

<p>
In a <code>P2P architecture</code>, there is minimal (or no) reliance on dedicated
servers in data centers. Instead the application exploits direct
communication between pairs of intermittently connected hosts, called peers.
</p>

<p>
The peers are not owned by the service provider, but are instead desktops and
laptops controlled by users, with most of the peers residing in homes,
universities, and offices.
</p>

<p>
Because the peers communicate without passing through a dedicated server, the
architecture is called peer-to-peer.
</p>

<p>
One of the most compelling features of P2P architectures is their
self-scalability. For example, in a P2P file-sharing application, although
each peer generates workload by requesting files, each peer also adds service
capacity to the system by distributing files to other peers. P2P
architectures are also cost effective, since they normally don’t require
significant server infrastructure and server bandwidth (in contrast with
clients-server designs with datacenters). However, P2P applications face
challenges of security, performance, and reliability due to their highly
decentralized structure.
</p>

<p>
some applications have <code>hybrid architectures</code>, combining both client-server
and P2P elements. For example, for many instant messaging applications,
servers are used to track the IP addresses of users, but user-to-user
messages are sent directly between user hosts (without passing through
intermediate servers).
</p></li>
</ul>
</div>
</div>

<div id="outline-container-orge7aaa61" class="outline-4">
<h4 id="orge7aaa61">Procesos Comunicandose</h4>
<div class="outline-text-4" id="text-orge7aaa61">
<p>
In the jargon of operating systems, it is not actually programs but
<code>processes</code> that communicate. A process can be thought of as a program that
is running within an end system.
</p>

<p>
how processes running on different hosts (with potentially different
operating systems) communicate.
</p>

<p>
Processes on two different end systems communicate with each other by
exchanging <code>messages</code> across the computer network. A sending process creates
and sends messages into the network; a receiving process receives these
messages and possibly responds by sending messages back.
</p>

<p>
imagen 2.1
</p>
</div>

<div id="outline-container-orgeef059c" class="outline-5">
<h5 id="orgeef059c">Procesos Cliente-Servidor</h5>
<div class="outline-text-5" id="text-orgeef059c">
<p>
A network application consists of pairs of processes that send messages to
each other over a network.
</p>

<p>
For each pair of communicating processes, we typically label one of the two
processes as the client and the other process as the server. With the Web, a
browser is a client process and a Web server is a server process. With P2P
file sharing, the peer that is downloading the file is labeled as the
client, and the peer that is uploading the file is labeled as the server.
</p>

<blockquote>
<p>
In the context of a communication session between a pair of processes, the
process that initiates the communication (that is, initially contacts the
other process at the beginning of the session) is labeled as the client. The
process that waits to be contacted to begin the session is the server.
</p>
</blockquote>
</div>
</div>

<div id="outline-container-org2126705" class="outline-5">
<h5 id="org2126705">La interfaz entre el Proceso y la Red</h5>
<div class="outline-text-5" id="text-org2126705">
<p>
most applications consist of pairs of communicating processes, with the two
processes in each pair sending messages to each other. Any message sent from
one process to another must go through the underlying network. A process
sends messages into, and receives messages from, the network through a
software interface called a <code>socket</code>.
</p>

<p>
A process is analogous to a house and its socket is analogous to its
door. When a process wants to send a message to another process on another
host, it shoves the message out its door (socket). This sending process
assumes that there is a transportation infrastructure on the other side of
its door that will transport the message to the door of the destination
process. Once the message arrives at the destination host, the message
passes through the receiving process’s door (socket), and the receiving
process then acts on the message.
</p>

<p>
imagen 2.3
</p>

<p>
a socket is the interface between the application layer and the transport
layer within a host. It is also referred to as the Application Programming
Interface (API) between the application and the network, since the socket is
the programming interface with which network applications are built.
</p>


<p>
The application developer has control of everything on the application-
layer side of the socket but has little control of the transport-layer side
of the socket. The only control that the application developer has on the
transport-layer side is (1) the choice of transport protocol and (2) perhaps
the ability to fix a few transport-layer parameters such as maximum buffer
and maximum segment sizes
</p>

<p>
Once the application developer chooses a transport protocol (if a choice is
available), the application is built using the transport-layer services
provided by that protocol.
</p>
</div>
</div>

<div id="outline-container-org86ea244" class="outline-5">
<h5 id="org86ea244">Addressing Processes</h5>
<div class="outline-text-5" id="text-org86ea244">
<p>
in order for a process running on one host to send packets to a process
running on another host, the receiving process needs to have an address.
</p>

<p>
imagen 2.3
(otra vez)
</p>

<p>
To identify the receiving process, two pieces of information need to be
specified: (1) the address of the host and (2) an identifier that specifies
the receiving process in the destination host.
</p>

<p>
In the Internet, the host is identified by its <code>IP address</code>.
</p>

<p>
an IP address is a 32-bit quantity that we can think of as uniquely
identifying the host.
</p>

<p>
In addition to knowing the address of the host to which a message is
destined, the sending process must also identify the receiving process (more
specifically, the receiving socket) running in the host. This information is
needed because in general a host could be running many network
applications. A destination port number serves this purpose. Popular
applications have been assigned specific port numbers. For example, a Web
server is identified by port number 80. A mail server process (using the
SMTP protocol) is identified by port number 25.
</p>
</div>
</div>
</div>

<div id="outline-container-orgf3d0b42" class="outline-4">
<h4 id="orgf3d0b42">Transport Services Available to Applications</h4>
<div class="outline-text-4" id="text-orgf3d0b42">
<p>
Recall that a socket is the interface between the application process and the
transport-layer protocol.  The application at the sending side pushes
messages through the socket. At the other side of the socket, the
transport-layer protocol has the responsibility of getting the messages to
the socket of the receiving process.
</p>

<p>
Many networks, including the Internet, provide more than one transport-layer
protocol. When you develop an application, you must choose one of the
available transport-layer protocols. How do you make this choice? Most
likely, you would study the <b>services provided by the available
transport-layer protocols</b>, and then pick the protocol with the services that
best match your application’s needs.
</p>

<p>
Services:
</p>
<ul class="org-ul">
<li>reliable data transfer</li>
<li>throughput</li>
<li>timing</li>
<li>security</li>
</ul>
</div>

<div id="outline-container-orgd063f59" class="outline-5">
<h5 id="orgd063f59">Reliable Data Transfer</h5>
<div class="outline-text-5" id="text-orgd063f59">
<p>
packets can get lost within a computer network. For example, a packet can
overflow a buffer in a router, or can be discarded by a host or router after
having some of its bits corrupted.
</p>

<p>
For many applications data loss can have devastating consequences. Thus, to
support these applications, something has to be done to guarantee that the
data sent by one end of the application is delivered correctly and
completely to the other end of the application. If a protocol provides such
a guaranteed data delivery service, it is said to provide <code>reliable data
transfer</code>.
</p>

<p>
When a transport protocol provides this service, the sending process can
just pass its data into the socket and know with complete confidence that
the data will arrive without errors at the receiving process.
</p>
</div>
</div>

<div id="outline-container-org20c1bca" class="outline-5">
<h5 id="org20c1bca">Throughput</h5>
<div class="outline-text-5" id="text-org20c1bca">
<p>
in the context of a communication session between two processes along a
network path, is the rate at which the sending process can deliver bits to
the receiving process.
</p>

<p>
Because other sessions will be sharing the bandwidth along the network path,
and because these other sessions will be coming and going, the available
throughput can fluctuate with time. These observations lead to another
natural service that a transport- layer protocol could provide, namely,
guaranteed available throughput at some specified rate.
</p>

<p>
With such a service, the application could request a guaranteed throughput
of r bits/sec, and the transport protocol would then ensure that the
available throughput is always at least r bits/sec.
</p>

<p>
Such a guaranteed throughput service would appeal to many applications. For
example, if an Internet telephony application encodes voice at 32 kbps, it
needs to send data into the network and have data delivered to the receiving
application at this rate. If the transport protocol cannot provide this
throughput, the application would need to encode at a lower rate (and
receive enough throughput to sustain this lower coding rate) or may have to
give up, since receiving, say, half of the needed throughput is of little or
no use to this Internet telephony application.
</p>

<p>
Applications that have throughput requirements are said to be
<code>bandwidth-sensitive</code> applications.
</p>

<p>
While bandwidth-sensitive applications have specific throughput
requirements, <code>elastic applications</code> can make use of as much, or as little,
throughput as happens to be available.
</p>
</div>
</div>

<div id="outline-container-org71386e3" class="outline-5">
<h5 id="org71386e3">Timing</h5>
<div class="outline-text-5" id="text-org71386e3">
<p>
timing guarantees can come in many shapes and forms. An example guarantee
might be that every bit that the sender pumps into the socket arrives at the
receiver’s socket no more than 100 msec later. Such a service would be
appealing to interactive real-time applications
</p>
</div>
</div>

<div id="outline-container-org4da47e6" class="outline-5">
<h5 id="org4da47e6">Security</h5>
<div class="outline-text-5" id="text-org4da47e6">
<p>
Finally, a transport protocol can provide an application with one or more
security services. For example, in the sending host, a transport protocol
can encrypt all data transmitted by the sending process, and in the
receiving host, the transport-layer protocol can decrypt the data before
delivering the data to the receiving process. Such a service would provide
confidentiality between the two processes, even if the data is somehow
observed between sending and receiving processes. A transport protocol can
also provide other security services in addition to confidentiality,
including data integrity and end-point authentication
</p>
</div>
</div>
</div>

<div id="outline-container-org25a61c0" class="outline-4">
<h4 id="org25a61c0">Transport Services Provided by the Internet</h4>
<div class="outline-text-4" id="text-org25a61c0">
<p>
The Internet (and, more generally, TCP/IP networks) makes two transport
protocols available to applications, UDP and TCP. When you (as an application
developer) create a new network application for the Internet, one of the
first decisions you have to make is whether to use UDP or TCP. Each of these
protocols offers a different set of services to the invoking applications.
</p>

<p>
imagen 2.4
</p>
</div>

<div id="outline-container-org088367a" class="outline-5">
<h5 id="org088367a">TCP</h5>
<div class="outline-text-5" id="text-org088367a">
<p>
The TCP service model includes a connection-oriented service and a reliable
data transfer service.  When an application invokes TCP as its transport
protocol, the application receives both of these services from TCP
</p>

<dl class="org-dl">
<dt>Connection-oriented service</dt><dd>TCP has the client and server exchange
transport-layer control information with each other before the
application-level messages begin to flow. This so-called handshaking
procedure alerts the client and server, allowing them to prepare for an
onslaught of packets. After the handshaking phase, a TCP connection is
said to exist between the sockets of the two processes. The connection is
a full-duplex connection in that the two processes can send messages to
each other over the connection at the same time. When the application
finishes sending messages, it must tear down the connection.</dd>

<dt>Reliable data transfer service</dt><dd>The communicating processes can rely on
TCP to deliver all data sent without error and in the proper order. When
one side of the application passes a stream of bytes into a socket, it can
count on TCP to deliver the same stream of bytes to the receiving socket,
with no missing or duplicate bytes.</dd>
</dl>


<p>
TCP also includes a congestion-control mechanism, a service for the general
welfare of the Internet rather than for the direct benefit of the
communicating processes. The TCP congestion-control mechanism throttles a
sending process (client or server) when the network is congested between
sender and receiver.TCP congestion control also attempts to limit each TCP
connection to its fair share of network bandwidth.
</p>
</div>
</div>

<div id="outline-container-orgd09ba46" class="outline-5">
<h5 id="orgd09ba46">seguridad por tcp</h5>
<div class="outline-text-5" id="text-orgd09ba46">
<p>
Neither TCP nor UDP provides any encryption-the data that the sending
process passes into its socket is the same data that travels over the
network to the destination process. So, for example, if the sending process
sends a password in cleartext (i.e., unencrypted) into its socket, the
cleartext password will travel over all the links between sender and
receiver, potentially getting sniffed and discovered at any of the
intervening links.
</p>

<p>
Because privacy and other security issues have become critical for many
applications, the Internet community has developed an enhancement for TCP,
called <code>Secure Sockets Layer (SSL)</code>. TCP-enhanced-with-SSL not only does
everything that traditional TCP does but also provides critical
process-to-process security services, including encryption, data integrity,
and end-point authentication.
</p>

<p>
We emphasize that SSL is not a third Internet transport protocol, on the
same level as TCP and UDP, but instead is an enhancement of TCP, with the
<b>enhancements being implemented in the application layer</b>.
</p>

<p>
In particular, if an application wants to use the services of SSL, it needs
to include SSL code (existing, highly optimized libraries and classes) in
both the client and server sides of the application. SSL has its own socket
API that is similar to the traditional TCP socket API.
</p>

<p>
When an application uses SSL, the sending process passes cleartext data to
the SSL socket; SSL in the sending host then encrypts the data and passes
the encrypted data to the TCP socket. The encrypted data travels over the
Internet to the TCP socket in the receiving process. The receiving socket
passes the encrypted data to SSL, which decrypts the data. Finally, SSL
passes the cleartext data through its SSL socket to the receiving process.
</p>
</div>
</div>

<div id="outline-container-orgc0286fb" class="outline-5">
<h5 id="orgc0286fb">UDP Services</h5>
<div class="outline-text-5" id="text-orgc0286fb">
<p>
UDP is a no-frills, lightweight transport protocol, providing minimal
services. UDP is connectionless, so there is no handshaking before the two
processes start to communicate. UDP provides an unreliable data transfer
service-that is, when a process sends a message into a UDP socket, UDP
provides no guarantee that the message will ever reach the receiving
process. Furthermore, messages that do arrive at the receiving process may
arrive out of order.
</p>

<p>
UDP does not include a congestion-control mechanism, so the sending side of
UDP can pump data into the layer below (the network layer) at any rate it
pleases.
</p>
</div>
</div>

<div id="outline-container-org630ef4a" class="outline-5">
<h5 id="org630ef4a">Services Not Provided by Internet Transport Protocols</h5>
<div class="outline-text-5" id="text-org630ef4a">
<p>
today’s Internet can often provide satisfactory service to time-sensitive
applications, but it cannot provide any timing or throughput guarantees.
</p>

<p>
imagen 2.5
</p>
</div>
</div>
</div>

<div id="outline-container-org656613e" class="outline-4">
<h4 id="org656613e">Protocolos de Capa de Aplicacion</h4>
<div class="outline-text-4" id="text-org656613e">
<p>
But how are these messages structured? What are the meanings of the various
fields in the messages? When do the processes send the messages? These
questions bring us into the realm of application-layer protocols.
</p>

<p>
An application-layer protocol defines how an application’s processes, running
on different end systems, pass messages to each other. In particular, an
application-layer protocol defines:
</p>
<ul class="org-ul">
<li>The types of messages exchanged, for example, request messages and response
messages</li>
<li>The syntax of the various message types, such as the fields in the message
and how the fields are delineated</li>
<li>The semantics of the fields, that is, the meaning of the information in the
fields</li>
<li><p>
Rules for determining when and how a process sends messages and responds to
messages
</p>

<p>
Some application-layer protocols are specified in RFCs and are therefore in
the public domain. For example, the Web’s application-layer protocol, HTTP
(the HyperText Transfer Protocol [RFC 2616]), is available as an RFC. If a
browser developer follows the rules of the HTTP RFC, the browser will be able
to retrieve Web pages from any Web server that has also followed the rules of
the HTTP RFC.
</p></li>
</ul>


<p>
It is important to distinguish between network applications and
application-layer protocols. An application-layer protocol is only one piece
of a network application
</p>
</div>
</div>
</div>

<div id="outline-container-org721188d" class="outline-3">
<h3 id="org721188d">La Web y HTTP</h3>
<div class="outline-text-3" id="text-org721188d">
<p>
the Web operates on demand. Users receive what they want, when they want
it. This is unlike traditional broadcast radio and television, which force
users to tune in when the content provider makes the content available.
</p>

<p>
In addition to being available on demand, the Web has many other wonderful
features that people love and cherish. It is enormously easy for any
individual to make information available over the Web-everyone can become a
publisher at extremely low cost.
</p>
</div>

<div id="outline-container-orgc0ac7cd" class="outline-4">
<h4 id="orgc0ac7cd">Overview of HTTP</h4>
<div class="outline-text-4" id="text-orgc0ac7cd">
<p>
The <code>HyperText Transfer Protocol (HTTP)</code>, the Web’s application-layer
protocol, is at the heart of the Web. It is defined in <code>[RFC 1945]</code> and <code>[RFC
2616]</code>. HTTP is implemented in two programs: a client program and a server
program. The client program and server program, executing on different end
systems, talk to each other by exchanging HTTP messages. HTTP defines the
structure of these messages and how the client and server exchange the
messages.
</p>

<p>
A <code>Web page</code> (also called a document) consists of objects. An <code>object</code> is
simply a file-such as an HTML file, a JPEG image, a Java applet, or a video
clip-that is addressable by a single URL. Most Web pages consist of a <code>base
HTML file</code> and several referenced objects. For example, if a Web page
contains HTML text and five JPEG images, then the Web page has six objects:
the base HTML file plus the five images. The base HTML file references the
other objects in the page with the objects’ URLs.  Each URL has two
components: the hostname of the server that houses the object and the
object’s path name. For example, the URL
</p>

<blockquote>

<div class="figure">
<p><img src="http://www.someschool.edu/someDepartment/picture.gif" alt="picture.gif" />
</p>
</div>
</blockquote>

<p>
has <code>www.someSchool.edu</code> for a hostname and <code>/someDepartment/picture.gif</code> for
a path name. Web servers, which implement the server side of HTTP, house Web
objects, each addressable by a URL.
</p>

<p>
HTTP defines how Web clients request Web pages from Web servers and how
servers transfer Web pages to clients. When a user requests a Web page, the
browser sends HTTP request messages for the objects in the page to the
server. The server receives the requests and responds with HTTP response
messages that contain the objects.
</p>

<p>
HTTP uses TCP as its underlying transport protocol (rather than running on
top of UDP). The HTTP client first initiates a TCP connection with the
server. Once the connection is established, the browser and the server
processes access TCP through their socket interfaces.
</p>

<p>
imagen 2.6
</p>

<p>
Once the client sends a message into its socket interface, the message is out
of the client’s hands and is “in the hands” of TCP.
</p>

<p>
each HTTP request message sent by a client process eventually arrives intact
at the server; similarly, each HTTP response message sent by the server
process eventually arrives intact at the client. Here we see one of the great
advantages of a layered architecture-HTTP need not worry about lost data or
the details of how TCP recovers from loss or reordering of data within the
network. That is the job of TCP and the protocols in the lower layers of the
protocol stack.
</p>

<p>
It is important to note that the server sends requested files to clients
without storing any state information about the client. If a particular
client asks for the same object twice in a period of a few seconds, the
server does not respond by saying that it just served the object to the
client; instead, the server resends the object, as it has completely
forgotten what it did earlier. Because an HTTP server maintains no
information about the clients, HTTP is said to be a <code>stateless protocol</code>. We
also remark that the Web uses the client-server application architecture
</p>

<p>
A Web server is always on, with a fixed IP address, and it services requests
from potentially millions of different browsers.
</p>
</div>
</div>

<div id="outline-container-org4d18a55" class="outline-4">
<h4 id="org4d18a55">Non-Persistent and Persistent Connections</h4>
<div class="outline-text-4" id="text-org4d18a55">
<p>
In many Internet applications, the client and server communicate for an
extended period of time, with the client making a series of requests and the
server responding to each of the requests. Depending on the application and
on how the application is being used, the series of requests may be made
back-to-back, periodically at regular intervals, or intermittently. When this
client-server interaction is taking place over TCP, the application developer
needs to make an important decision-should each request/response pair be sent
over a separate TCP connection, or should all of the requests and their
corresponding responses be sent over the same TCP connection? In the former
approach, the application is said to use <code>non-persistent connections</code>; and in
the latter approach, <code>persistent connections</code>.
</p>
</div>

<div id="outline-container-org2698555" class="outline-5">
<h5 id="org2698555">HTTP with Non-Persistent Connections</h5>
<div class="outline-text-5" id="text-org2698555">
<p>
Let’s suppose the page consists of a base HTML file and 10 JPEG images, and
that all 11 of these objects reside on the same server. Further suppose the
URL for the base HTML file is
</p>

<blockquote>
<p>
<a href="http://www.someschool.edu/someDepartment/home.index">http://www.someschool.edu/someDepartment/home.index</a>
</p>
</blockquote>

<p>
Here is what happens:
</p>
<ol class="org-ol">
<li>The HTTP client process initiates a TCP connection to the server
www.someSchool.edu on port number 80, which is the default port number for
HTTP. Associated with the TCP connection, there will be a socket at the
client and a socket at the server.</li>
<li>The HTTP client sends an HTTP request message to the server via its
socket. The request message includes the path name /someDepartment/home
.index . (We will discuss HTTP messages in some detail below.)</li>
<li>The HTTP server process receives the request message via its socket,
retrieves the object /someDepartment/home.index from its storage (RAM or
disk), encapsulates the object in an HTTP response message, and sends the
response message to the client via its socket.</li>
<li>The HTTP server process tells TCP to close the TCP connection. (But TCP
doesn’t actually terminate the connection until it knows for sure that the
client has received the response message intact.)</li>
<li>The HTTP client receives the response message. The TCP connection
terminates. The message indicates that the encapsulated object is an HTML
file. The client extracts the file from the response message, examines the
HTML file, and finds references to the 10 JPEG objects.</li>
<li><p>
The first four steps are then repeated for each of the referenced JPEG
objects.
</p>

<p>
The steps above illustrate the use of non-persistent connections, where each
TCP connection is closed after the server sends the object-the connection
does not persist for other objects. Note that each TCP connection transports
exactly one request message and one response message. Thus, in this example,
when a user requests the Web page, 11 TCP connections are generated.
</p>

<p>
In the steps described above, we were intentionally vague about whether the
client obtains the 10 JPEGs over 10 serial TCP connections, or whether some
of the JPEGs are obtained over parallel TCP connections. Indeed, users can
configure modern browsers to control the degree of parallelism. In their
default modes, most browsers open 5 to 10 parallel TCP connections, and each
of these connections handles one request-response transaction. If the user
prefers, the maximum number of parallel connections can be set to one, in
which case the 10 connections are established serially. As we’ll see in the
next chapter, the use of parallel connections shortens the response time.
</p>

<p>
to estimate the amount of time that elapses from when a client requests the
base HTML file until the entire file is received by the client. To this end,
we define the <code>round-trip time (RTT)</code>, which is the time it takes for a
small packet to travel from client to server and then back to the
client. The RTT includes packet-propagation delays, packet- queuing delays
in intermediate routers and switches, and packet-processing delays.
</p>

<p>
Now consider what happens when a user clicks on a hyperlink. As shown in
Figure 2.7, this causes the browser to initiate a TCP connection between the
browser and the Web server; this involves a “three-way handshake”-the client
sends a small TCP segment to the server, the server acknowledges and
responds with a small TCP segment, and, finally, the client acknowledges
back to the server. The first two parts of the three-way handshake take one
RTT. After completing the first two parts of the handshake, the client sends
the HTTP request message combined with the third part of the three-way
handshake (the acknowledgment) into the TCP connection. Once the request
message arrives at the server, the server sends the HTML file into the TCP
connection. This HTTP request/response eats up another RTT. Thus, roughly,
the total response time is two RTTs plus the transmission time at the server
of the HTML file.
</p>

<p>
imagen 2.7
</p>

<p>
shortcomings. First, a brand-new connection must be established and
maintained for each requested object. For each of these connections, TCP
buffers must be allocated and TCP variables must be kept in both the client
and server. This can place a significant burden on the Web server, which may
be serving requests from hundreds of different clients simultaneously.
Second, as we just described, each object suffers a delivery delay of two
RTTs-one RTT to establish the TCP connection and one RTT to request and
receive an object.
</p></li>
</ol>
</div>
</div>

<div id="outline-container-orgf3f67f6" class="outline-5">
<h5 id="orgf3f67f6">HTTP with Persistent Connections</h5>
<div class="outline-text-5" id="text-orgf3f67f6">
<p>
With HTTP 1.1 persistent connections, the server leaves the TCP connection
open after sending a response. Subsequent requests and responses between the
same client and server can be sent over the same connection. In particular,
an entire Web page (in the example above, the base HTML file and the 10
images) can be sent over a single persistent TCP connection. Moreover,
multiple Web pages residing on the same server can be sent from the server
to the same client over a single persistent TCP connection. These requests
for objects can be made back-to-back, without waiting for replies to pending
requests (pipelining). Typically, the HTTP server closes a connection when
it isn’t used for a certain time (a configurable timeout interval). When the
server receives the back-to-back requests, it sends the objects
back-to-back. The default mode of HTTP uses persistent connections with
pipelining. Most recently, HTTP/2 [RFC 7540] builds on HTTP 1.1 by allowing
multiple requests and replies to be interleaved in the same connection, and
a mechanism for prioritizing HTTP message requests and replies within this
connection.
</p>
</div>
</div>
</div>

<div id="outline-container-org0297064" class="outline-4">
<h4 id="org0297064">HTTP Message Format</h4>
<div class="outline-text-4" id="text-org0297064">
<p>
The HTTP specifications [RFC 1945; RFC 2616; RFC 7540] include the
definitions of the HTTP message formats. There are two types of HTTP
messages, <b>request messages</b> and <b>response messages</b>
</p>
</div>

<div id="outline-container-org2eb6972" class="outline-5">
<h5 id="org2eb6972">HTTP Request Message</h5>
<div class="outline-text-5" id="text-org2eb6972">
<pre class="example">
GET /somedir/page.html HTTP/1.1
Host: www.someschool.edu
Connection: close
User-agent: Mozilla/5.0
Accept-language: fr
</pre>

<p>
First of all, we see that the message is written in ordinary ASCII text, so
that your ordinary computer-literate human being can read it.
</p>

<p>
Second, we see that the message consists of five lines, each followed by a
carriage return and a line feed. The last line is followed by an additional
carriage return and line feed.
</p>

<p>
Although this particular request message has five lines, a request message
can have many more lines or as few as one line.
</p>

<p>
The first line of an HTTP request message is called the <code>request line</code>; the
subsequent lines are called the <code>header lines</code>. The request line has three
fields: the method field, the URL field, and the HTTP version field.
</p>

<p>
The method field can take on several different values, including GET, POST,
HEAD, PUT, and DELETE . The great majority of HTTP request messages use the
GET method. The GET method is used when the browser requests an object, with
the requested object identified in the URL field.
</p>

<p>
In this example, the browser is requesting the object /somedir/page.html
. The version is self- explanatory; in this example, the browser implements
version HTTP/1.1.
</p>

<p>
The header line <code>Host: www.someschool.edu</code> specifies the host on which the
object resides. You might think that this header line is unnecessary, as
there is already a TCP connection in place to the host. But the information
provided by the host header line is required by Web proxy caches.
</p>

<p>
By including the <code>Connection: close</code> header line, the browser is telling the
server that it doesn’t want to bother with persistent connections; it wants
the server to close the connection after sending the requested object.
</p>

<p>
The <code>User- agent:</code> header line specifies the user agent, that is, the
browser type that is making the request to the server. Here the user agent
is Mozilla/5.0, a Firefox browser. This header line is useful because the
server can actually send different versions of the same object to different
types of user agents. (Each of the versions is addressed by the same URL.)
</p>

<p>
Finally, the <code>Accept-language:</code> header indicates that the user prefers to
receive a French version of the object, if such an object exists on the
server; otherwise, the server should send its default version.
</p>


<p>
general format of a request message
</p>

<p>
imagen 2.8
</p>

<p>
after the header lines (and the additional carriage return and line feed)
there is an “entity body.” The entity body is empty with the GET method, but
is used with the POST method. An HTTP client often uses the POST method when
the user fills out a form-for example, when a user provides search words to
a search engine. With a POST message, the user is still requesting a Web
page from the server, but the specific contents of the Web page depend on
what the user entered into the form fields. If the value of the method field
is POST , then the entity body contains what the user entered into the form
fields.
</p>
</div>
</div>

<div id="outline-container-org4d9d53e" class="outline-5">
<h5 id="org4d9d53e">HTTP Response Message</h5>
<div class="outline-text-5" id="text-org4d9d53e">
<p>
This response message could be the response to the example request message
just discussed.
</p>

<pre class="example">
HTTP/1.1 200 OK
Connection: close
Date: Tue, 18 Aug 2015 15:44:04 GMT
Server: Apache/2.2.3 (CentOS)
Last-Modified: Tue, 18 Aug 2015 15:11:03 GMT
Content-Length: 6821
Content-Type: text/html
(data data data data data ...)
</pre>

<p>
It has three sections: an initial <code>status line</code>, six <code>header lines</code>, and
then the <code>entity body</code>. The entity body is the meat of the message-it
contains the requested object itself (represented by data data data data
data &#x2026; ). The status line has three fields: the protocol version field, a
status code, and a corresponding status message. In this example, the status
line indicates that the server is using HTTP/1.1 and that everything is OK
(that is, the server has found, and is sending, the requested object).
</p>

<p>
The server uses the <code>Connection: close</code> header line to tell the client that
it is going to close the TCP connection after sending the message.
</p>

<p>
The <code>Date:</code> header line indicates the time and date when the HTTP response
was created and sent by the server. Note that this is the time when the
server retrieves the object from its file system, inserts the object into
the response message, and sends the response message.
</p>

<p>
The <code>Server:</code> header line indicates that the message was generated by an
Apache Web server; it is analogous to the <code>User-agent:</code> header line in the
HTTP request message.
</p>

<p>
The <code>Last-Modified:</code> header line indicates the time and date when the object
was created or last modified. The <code>Last-Modified:</code> header, which we will
soon cover in more detail, is critical for object caching, both in the local
client and in network cache servers (also known as proxy servers).
</p>

<p>
The <code>Content-Length:</code> header line indicates the number of bytes in the
object being sent. The <code>Content-Type:</code> header line indicates that the object
in the entity body is HTML text.
</p>


<p>
general format of a response message
</p>

<p>
imagen 2.9
</p>

<p>
The status code and associated phrase indicate the result of the
request. Some common status codes and associated phrases include:
</p>
<ul class="org-ul">
<li>200 OK: Request succeeded and the information is returned in the response.</li>
<li>301 Moved Permanently: Requested object has been permanently moved; the new
URL is specified in Location : header of the response message. The client
software will automatically retrieve the new URL.</li>
<li>400 Bad Request: This is a generic error code indicating that the request
could not be understood by the server.</li>
<li>404 Not Found: The requested document does not exist on this server.</li>
<li>505 HTTP Version Not Supported: The requested HTTP protocol version is not
supported by the server.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-org363b589" class="outline-4">
<h4 id="org363b589">User-Server Interaction: Cookies</h4>
<div class="outline-text-4" id="text-org363b589">
<p>
We mentioned above that an HTTP server is stateless. This simplifies server
design and has permitted engineers to develop high-performance Web servers
that can handle thousands of simultaneous TCP connections. However, it is
often desirable for a Web site to identify users, either because the server
wishes to restrict user access or because it wants to serve content as a
function of the user identity. For these purposes, HTTP uses cookies.
Cookies, defined in [RFC 6265], allow sites to keep track of users.
</p>

<p>
cookie technology has four components:
</p>
<ol class="org-ol">
<li>a cookie header line in the HTTP response message;</li>
<li>a cookie header line in the HTTP request message;</li>
<li>a cookie file kept on the user’s end system and managed by the user’s
browser;</li>
<li><p>
a back-end database at the Web site.
</p>

<p>
imagen 2.10
</p>

<p>
let’s walk through an example of how cookies work. Suppose Susan, who always
accesses the Web using Internet Explorer from her home PC, contacts
Amazon.com for the first time.  Let us suppose that in the past she has
already visited the eBay site. When the request comes into the Amazon Web
server, the server creates a unique identification number and creates an
entry in its back- end database that is indexed by the identification
number. The Amazon Web server then responds to Susan’s browser, including in
the HTTP response a <code>Set-cookie:</code> header, which contains the identification
number. For example, the header line might be: <code>Set-cookie: 1678</code>
</p>

<p>
When Susan’s browser receives the HTTP response message, it sees the
Set-cookie: header. The browser then appends a line to the special cookie
file that it manages. This line includes the hostname of the server and the
identification number in the Set-cookie: header. Note that the cookie file
already has an entry for eBay, since Susan has visited that site in the
past. As Susan continues to browse the Amazon site, each time she requests a
Web page, her browser consults her cookie file, extracts her identification
number for this site, and puts a cookie header line that includes the
identification number in the HTTP request. Specifically, each of her HTTP
requests to the Amazon server includes the header line: <code>Cookie: 1678</code>
</p>

<p>
In this manner, the Amazon server is able to track Susan’s activity at the
Amazon site. Although the Amazon Web site does not necessarily know Susan’s
name, it knows exactly which pages user 1678 visited, in which order, and at
what times! Amazon uses cookies to provide its shopping cart service- Amazon
can maintain a list of all of Susan’s intended purchases, so that she can pay
for them collectively at the end of the session.
</p>

<p>
If Susan returns to Amazon’s site, say, one week later, her browser will
continue to put the header line Cookie: 1678 in the request messages. Amazon
also recommends products to Susan based on Web pages she has visited at
Amazon in the past. If Susan also registers herself with Amazon- providing
full name, e-mail address, postal address, and credit card information-Amazon
can then include this information in its database, thereby associating
Susan’s name with her identification number (and all of the pages she has
visited at the site in the past!). This is how Amazon and other e-commerce
sites provide “one-click shopping”-when Susan chooses to purchase an item
during a subsequent visit, she doesn’t need to re-enter her name, credit card
number, or address.
</p>

<p>
From this discussion we see that cookies can be used to identify a user. The
first time a user visits a site, the user can provide a user identification
(possibly his or her name). During the subsequent sessions, the browser
passes a cookie header to the server, thereby identifying the user to the
server.  <b>Cookies can thus be used to create a user session layer on top of
stateless HTTP</b>. For example, when a user logs in to a Web-based e-mail
application (such as Hotmail), the browser sends cookie information to the
server, permitting the server to identify the user throughout the user’s
session with the application.
</p>

<p>
Although cookies often simplify the Internet shopping experience for the
user, they are controversial because they can also be considered as an
invasion of privacy. As we just saw, using a combination of cookies and
user-supplied account information, a Web site can learn a lot about a user
and potentially sell this information to a third party.
</p></li>
</ol>
</div>
</div>

<div id="outline-container-org5e3d70c" class="outline-4">
<h4 id="org5e3d70c">Web Cache</h4>
<div class="outline-text-4" id="text-org5e3d70c">
<p>
A <code>Web cache</code> -also called a <code>proxy server</code> -is a network entity that
satisfies HTTP requests on the behalf of an origin Web server. The Web cache
has its own disk storage and keeps copies of recently requested objects in
this storage.
</p>

<p>
a user’s browser can be configured so that all of the user’s HTTP requests
are first directed to the Web cache. Once a browser is configured, each
browser request for an object is first directed to the Web cache. As an
example, suppose a browser is requesting the object
<img src="http://www.someschool.edu/campus.gif" alt="campus.gif" /> . Here is what happens:
</p>

<ol class="org-ol">
<li>The browser establishes a TCP connection to the Web cache and sends an
HTTP request for the object to the Web cache.</li>
<li>The Web cache checks to see if it has a copy of the object stored
locally. If it does, the Web cache returns the object within an HTTP
response message to the client browser.</li>
<li>If the Web cache does not have the object, the Web cache opens a TCP
connection to the origin server, that is, to www.someschool.edu . The Web
cache then sends an HTTP request for the object into the cache-to-server
TCP connection. After receiving this request, the origin server sends the
object within an HTTP response to the Web cache.</li>
<li><p>
When the Web cache receives the object, it stores a copy in its local
storage and sends a copy, within an HTTP response message, to the client
browser (over the existing TCP connection between the client browser and
the Web cache).
</p>

<p>
imagen 2.11
</p>

<p>
Note that a cache is both a server and a client at the same time. When it
receives requests from and sends responses to a browser, it is a server. When
it sends requests to and receives responses from an origin server, it is a
client.
</p>

<p>
Web caching has seen deployment in the Internet for two reasons:
</p>
<ol class="org-ol">
<li>First, a Web cache can substantially reduce the response time for a client
request, particularly if the bottleneck bandwidth between the client and
the origin server is much less than the bottleneck bandwidth between the
client and the cache. If there is a high-speed connection between the
client and the cache, as there often is, and if the cache has the
requested object, then the cache will be able to deliver the object
rapidly to the client.</li>
<li>Second, as we will soon illustrate with an example, Web caches can
substantially reduce traffic on an institution’s access link to the
Internet. By reducing traffic, the institution (for example, a company or
a university) does not have to upgrade bandwidth as quickly, thereby
reducing costs.</li>
</ol>

<p>
Furthermore, Web caches can substantially reduce Web traffic in the Internet
as a whole, thereby improving performance for all applications.
</p>

<p>
Through the use of <code>Content Distribution Networks (CDNs)</code>, Web caches are
increasingly playing an important role in the Internet. A CDN company
installs many geographically distributed caches throughout the Internet,
thereby localizing much of the traffic. There are shared CDNs (such as Akamai
and Limelight) and dedicated CDNs (such as Google and Netflix).
</p></li>
</ol>
</div>

<div id="outline-container-orgbf6e8cb" class="outline-5">
<h5 id="orgbf6e8cb">The Conditional GET</h5>
<div class="outline-text-5" id="text-orgbf6e8cb">
<p>
Although caching can reduce user-perceived response times, it introduces a
new problem-the copy of an object residing in the cache may be stale. In
other words, the object housed in the Web server may have been modified
since the copy was cached at the client. Fortunately, HTTP has a mechanism
that allows a cache to verify that its objects are up to date. This
mechanism is called the <code>conditional GET</code>.
</p>

<p>
An HTTP request message is a so-called conditional GET message if
</p>
<ol class="org-ol">
<li>the request message uses the GET method and</li>
<li><p>
the request message includes an <code>If-Modified-Since:</code> header line.
</p>

<p>
example
</p>

<p>
First, on the behalf of a requesting browser, a proxy cache sends a request
message to a Web server:
</p>

<pre class="example">
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
</pre>

<p>
Second, the Web server sends a response message with the requested object to
the cache:
</p>

<pre class="example">
HTTP/1.1 200 OK
Date: Sat, 3 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)
Last-Modified: Wed, 9 Sep 2015 09:23:24
Content-Type: image/gif
(data data data data data ...)
</pre>

<p>
The cache forwards the object to the requesting browser but also caches the
object locally. Importantly, the cache also stores the last-modified date
along with the object. Third, one week later, another browser requests the
same object via the cache, and the object is still in the cache. Since this
object may have been modified at the Web server in the past week, the cache
performs an up-to-date check by issuing a conditional GET. Specifically, the
cache sends:
</p>

<pre class="example">
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
If-modified-since: Wed, 9 Sep 2015 09:23:24
</pre>

<p>
Note that the value of the If-modified-since: header line is exactly equal
to the value of the Last-Modified: header line that was sent by the server
one week ago. This conditional GET is telling the server to send the object
only if the object has been modified since the specified date.  Suppose the
object has not been modified since 9 Sep 2015 09:23:24. Then, fourth, the
Web server sends a response message to the cache:
</p>

<pre class="example">
HTTP/1.1 304 Not Modified
Date: Sat, 10 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)
(empty entity body)
</pre>

<p>
We see that in response to the conditional GET, the Web server still sends a
response message but does not include the requested object in the response
message. Including the requested object would only waste bandwidth and
increase user-perceived response time, particularly if the object is
large. Note that this last response message has 304 Not Modified in the
status line, which tells the cache that it can go ahead and forward its (the
proxy cache’s) cached copy of the object to the requesting browser.
</p></li>
</ol>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org733d30f" class="outline-2">
<h2 id="org733d30f">SMTP y DNS</h2>
</div>
<div id="outline-container-org5e7acbe" class="outline-2">
<h2 id="org5e7acbe">Capa de Transporte, UDP y Entrega confiable</h2>
<div class="outline-text-2" id="text-org5e7acbe">
<p>
La capa de transporte tiene el rol de proveer servicios-de-comunicacion
directamente a la aplicacion ejecutandose en otro host.
</p>

<p>
extiende el servicio-de-entrega entre dos <b>end-systems</b> de la capa de red, a un
servicio-de-entrega entre dos <b>procesos</b> de capa de aplicacion ejecutandose en
los end-systems.
</p>
</div>

<div id="outline-container-orgbf0030a" class="outline-3">
<h3 id="orgbf0030a">Introduccion y Servicios de Capa de Transporte</h3>
<div class="outline-text-3" id="text-orgbf0030a">
<p>
un protocolo de capa de transporte provee <code>comunicacion logica</code> entre procesos
ejecutandose en diferentes hosts. Desde el punto de vista del proceso, es como
si los hosts estuviesen conectados directamente, sin tener que preocuparse por
los detalles involucrados para enviar los mensajes.
</p>

<p>
imagen 3.1
</p>

<p>
los protocolos de capa de transporte estan implementados en los end-systems y no
en los conmutadores de paquetes. De el lado que envia, la capa de transporte
convierte mensajes de capa-de-aplicacion en paquetes de capa-de-transporte
llamados <code>segmentos</code>.
</p>
</div>

<div id="outline-container-orgab2e2b1" class="outline-4">
<h4 id="orgab2e2b1">Relacion entre las capas de transporte y capa de red</h4>
<div class="outline-text-4" id="text-orgab2e2b1">
<p>
protocolos de capa de red proveen una comunicaicon logica entre <b>hosts</b>, los
protocolos de capa de transporte proveen una comunicacion logica entre
<b>procesos</b>
</p>
</div>
</div>

<div id="outline-container-org51ad849" class="outline-4">
<h4 id="org51ad849">Panorama general de la capa de transporte en la internet</h4>
<div class="outline-text-4" id="text-org51ad849">
<p>
La internet tiene dos protocolos de capa de transporte distintos disponibles
para la capa de aplicacion:
</p>
<dl class="org-dl">
<dt><code>UDP (User Datagram Protocol)</code></dt><dd>que provee un servicio sin conexion y no
confiable</dd>
<dt><code>TCP (Transmission Control Protocol)</code></dt><dd><p>
que provee un servicio orientado a
conexion y confiable.
</p>

<p>
La responsabilidad fundamental de UDP y TCP es extender el servicio de IP entre
dos end-systems a un servicio entre dos procesos. Esto se llama <code>multiplexacion</code>
y <code>demultiplexacion</code>.
</p>

<p>
UDP y TCP tambien proveen verificacion de integridad al incluir campos en el
encabezado para la deteccion de errores.
</p>

<p>
UDP solamente provee:
</p>
<ul class="org-ul">
<li>entrega de datos proceso-a-proceso</li>
<li><p>
verificacion de errores
</p>

<p>
no garantiza que los mensajes enviado lleguen a su destino.
</p></li>
</ul></dd>
</dl>


<p>
TCP ofrece otros servicios adicionales:
</p>
<dl class="org-dl">
<dt><code>Transferencia de datos confiable</code></dt><dd>Al utilizar control de flujo, numeros de
secuencia, acuse de recibo, y timer, se garantiza que los datos enviados
lleguen a su (proceso) destino, sin errores y en orden.</dd>
<dt><code>Control de Congestion</code></dt><dd>evita que congestionar enlaces y routers con
demasiado trafico. Se logra regulando a la velocidad en que se transmiten
paquetes.</dd>
</dl>
</div>
</div>
</div>

<div id="outline-container-org0371170" class="outline-3">
<h3 id="org0371170">Multiplexacion y Demultiplexacion</h3>
<div class="outline-text-3" id="text-org0371170">
<p>
(extender servicio de delivery de host-a-host (de la capa inferior (red - IP)) a
proceso-a-proceso)
</p>

<p>
imagen 3.2
</p>

<p>
Recordemos que las aplicaciones se comunican a traves de la red mediante
sockets.
</p>

<p>
Un host que recibe segmentos de capa de transporte los redirecciona al socket
apropiado, utilizando campos en el encabezado del segmento. el proceso de
entregar el segmento de capa de transporte al socket correcto se conoce como
demultiplexacion.
</p>

<p>
El proceso de recolectar datos de distintos sockets en el origen, encapsulando
cada uno, creando segmentos y pasarlos a la capa de red, se conoce como
multiplexacion.
</p>

<p>
La multiplexacion de capa de transporte requiere:
</p>
<ol class="org-ol">
<li>que los sockets tengan identificadores unicos</li>
<li><p>
que cada segmento tenga campos especiales para indicar al socket al que deben
ser entregados. estos campos son <code>source port number field</code> y <code>destination
   port number field</code>.
</p>

<p>
imagen 3.3
</p>

<p>
cada numero de puerto es un numero de 16-bits, entre 0 y 65535.
</p>

<p>
Los numeros de puertos de 0 a 1023 son well-known y estan reservados para
aplicaciones de capa de aplicacion well-known (<a href="http:80">http:80</a>, <a href="ftp:21">ftp:21</a>).
</p>

<p>
RFC 1700
</p></li>
</ol>
</div>

<div id="outline-container-org0b5f064" class="outline-4">
<h4 id="org0b5f064">Multiplexacion y Demultiplexacion Sin Conexion</h4>
<div class="outline-text-4" id="text-org0b5f064">
<p>
En general, el lado cliente de la aplicacion permite que la capa de transporte
asigne automaticamente el numero de puerto, mientras que el lado servidor de la
aplicacion asigna un numero de puerto especifico.
</p>

<p>
With port numbers assigned to UDP sockets, we can now precisely describe UDP
multiplexing/demultiplexing.
</p>

<p>
Supongamos que un proceso en el host A, con puerto UDP 19157, quiere enviar
datos a un proceso con puerto UDP 46428 en el host B. La capa de transporte en
el host A crea un segmento de capa de transporte que incluye datos de la
aplicacion, source port number (19157), destination port number (46428), y otros
dos valores. El segmento luego se pasa a la capa de red.
</p>

<p>
La capa de red encapsula el segmento en un datagrama IP y hace el mejor intento
de enviar el datagrama a su destino.
</p>

<p>
Si el segmento llega al host B, la capa de transporte examina el numero de
puerto destino (46428) y entrega el segmento al socket identificado en el
numero 46428.
</p>

<p>
Un socket UDP es identificable por una tupla formada por la IP y puerto de
destino.  Como consecuencia, Si dos segmentos UDP tienen diferentes IP de origen
y/o puerto de origen, pero tienen la misma IP y puerto de destino, entonces
ambos segmentos son dirigidos al mismo proceso mediante el mismo socket.
</p>

<p>
El puerto de origen sirve para poder devolver segmentos al mismo proceso
</p>

<p>
imagen 3.4
</p>
</div>
</div>

<div id="outline-container-org4e2d769" class="outline-4">
<h4 id="org4e2d769">Multiplexacion y Demultiplexacion Orientado a Conexion</h4>
<div class="outline-text-4" id="text-org4e2d769">
<p>
Un socket TCP es identificable por una cuatro-upla
</p>
<blockquote>
<p>
(IP origen, puerto origen, IP destino, puerto destino)
</p>
</blockquote>

<p>
Cuando se recibe un segmento TCP, el host utiliza estos cuatro valores para
demultiplexar el segmento al socket apropiado.
</p>

<p>
Ejemplo:
</p>
<ul class="org-ul">
<li>El servidor TCP tiene un socket aceptador, que espera por conexiones de
clientes TCP en el puerto 12000.</li>
<li>El cliente crea un socket y envia un pedido de conexion al
puerto 12000. Tambien envia el puerto de origen del segmento elegido por el
cliente.</li>
<li>Cuando el server recibe el segmento de pedido de conexion, el OS ubica al
proceso que espera por la conexion en el puerto 12000.</li>
<li><p>
Una vez que se acepta la conexion, se anotan los cuatro campos mencionados y
se crea un socket nuevo asociado a estos campos. Los siguientes segmentos con
los mismos cuatro valores son demultiplexados a este socket.
</p>

<p>
imagen 3.5
</p></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orga9f106d" class="outline-3">
<h3 id="orga9f106d">Transporte sin conexion: UDP</h3>
<div class="outline-text-3" id="text-orga9f106d">
<p>
UDP, definido en [RFC 768], hace lo minimo posible que un protocolo de capa de
transporte puede hacer.
</p>

<p>
Ademas de multiplexar y demultiplexar segmentos y una simple verificacion de
errores, no agrega nada a IP.
</p>

<p>
A UDP se lo denomina como <code>sin conexion</code> porque no realiza <code>handshaking</code> entre
los hosts antes de enviar y recibir segmentos.
</p>

<p>
DNS es un ejemplo de un protocolo de capa de aplicacion que usualmente utiliza
UDP.
</p>

<p>
Algunas aplicaciones utilizan UDP porque:
</p>
<dl class="org-dl">
<dt>Mayor control de la aplicacion sobre los datos enviados y cuando</dt><dd>En UDP,
apenas el proceso envia los datos por el socket UDP, el segmento se pasa
directo a la capa de red. TCP por el otro lado, al tener el mecanismo de
control de congestion, el trafico es regulado si el enlace o routers en el
camino sufren de congestion. TCP tambien reenvia segmentos hasta que se
reciba un acuse de recibo, lo que es un inconveniente para aplicaciones de
tiempo real ya que requieren una velocidad de transmision minima y pueden
tolerar perdida de datos.</dd>
<dt>Sin establecimiento de conexion (handshake)</dt><dd>UDP simplemente envia los datos
hacia la siguiente capa, por lo que no tiene delay para establecer una
conexion.</dd>
<dt>Sin estado de conexion</dt><dd>TCP mantiene el estado de conexion en los
end-systems. Esto incluye buffers de envio y recepcion, parametros de control
de congestion, numeros de secuencia y acuse de recibo. UDP, no mantiene estado
de la conexion ni lleva registro de estos parametros (utilizados para
garantizar una comunicacion confiable).</dd>
<dt>Encabezado pqueño</dt><dd><p>
El encabezado de TCP tiene 20 bytes mientras que el de
UDP es solamente de 8 bytes.
</p>

<p>
imagen 3.6
</p></dd>
</dl>
</div>

<div id="outline-container-org0d32629" class="outline-4">
<h4 id="org0d32629">UDP Segment Structure</h4>
<div class="outline-text-4" id="text-org0d32629">
<p>
imagen 3.7
</p>

<p>
La estructura del segmento UDP esta definida en RFC 768.
</p>

<p>
Los datos de la aplicacion se encuentran en el campo de datos del segmento.
</p>

<p>
El encabezado UDP solo tienen cuatro campos, cada uno de dos bytes.
</p>

<p>
El campo length especifica el numero de bytes en el segmento UDP (encapsulado +
datos). Es necesario ya que el tamaño del campo de datos puede variar entre
segmentos UDP.
</p>

<p>
El checksum es utilizado por el host receptor para verificar si el segmento
tiene errores.
</p>
</div>
</div>

<div id="outline-container-org61c552a" class="outline-4">
<h4 id="org61c552a">UDP Checksum</h4>
<div class="outline-text-4" id="text-org61c552a">
<p>
Del lado que envia, se realiza el complemento a 1 de la suma de todos los words
de 16 bits en el segmento.
</p>
</div>
</div>
</div>

<div id="outline-container-orgd58c46a" class="outline-3">
<h3 id="orgd58c46a">Principios de Transferencia de Datos Confiable (TDC)</h3>
<div class="outline-text-3" id="text-orgd58c46a">
<p>
Con un canal de comunicacion confiable, los bits transferidos no se corrompen
ni se pierden, y son entregados en el orden en que fueron enviados.
</p>

<p>
imagen 3.8
</p>

<p>
rdt : reliable data transfer
</p>

<p>
En esta seccion se considera el caso de la transferencia de datos
unidireccional.
</p>

<p>
Ademas del intercambio de paquetes de datos, ambos extremos de la
comunicacion confiable tambien deben intercambiar paquetes de control. El
envio de paquetes se realiza mediante la funcion udt<sub>send</sub>() (donde udt es
unreliable data transfer)
</p>
</div>

<div id="outline-container-orgc0ebc95" class="outline-4">
<h4 id="orgc0ebc95">Armando un protocolo de TDC</h4>
<div class="outline-text-4" id="text-orgc0ebc95">
</div>
<div id="outline-container-org5ac4d38" class="outline-5">
<h5 id="org5ac4d38">RDT 1.0: TDC sobre un canal confiable</h5>
<div class="outline-text-5" id="text-org5ac4d38">
<p>
We first consider the simplest case, in which the underlying channel is
completely reliable.
</p>

<p>
dos <code>maquinas de estado finitas (FSM)</code> definen las operaciones de un
transmisor y emisor rdt1.0.
</p>

<p>
imagen 3.9
</p>

<p>
Cada FSM tiene un estado
</p>

<p>
el lado transmisor del rdt:
</p>
<ol class="org-ol">
<li>acepta datos de la capa superior via el evento <code>rdt_send(data)</code>,</li>
<li>crea un paquete con los datos con la accion <code>make_pkt(data)</code>) y</li>
<li><p>
envia el paquete al canal.
</p>

<p>
el lado receptor:
</p>
<ol class="org-ol">
<li>recibe un paquete de la capa inferior via el evento <code>rdt_rcv(packet)</code>,</li>
<li>quita los datos del paquete con la accion <code>extract (packet, data)</code> ) y</li>
<li><p>
pasa los datos a la capa superior <code>deliver_data(data)</code>.
</p>

<p>
en este protocolo simple, no hay distincion etnre una unidad de datos y un
paquete. tambien, todo el flujo de paquetes es del emisor al receptor, no
hay necesidad de que el receptor envie feedback. tambien se asume que el el
emisor es capaz de recibir datos a la misma velocidad en que el transmisor
los envia (por lo que no es necesario que el receptor indique al transmisor
a que reduzca la velocidad).
</p></li>
</ol></li>
</ol>
</div>
</div>

<div id="outline-container-orge051ffb" class="outline-5">
<h5 id="orge051ffb">RDT 2.0: TDC sobre un canal con errores</h5>
<div class="outline-text-5" id="text-orge051ffb">
<p>
Un modelo mas realista es donde ocurre corrupcion de bits en el canal de
comunicacion.
</p>
</div>

<div id="outline-container-orgd0dc9e9" class="outline-6">
<h6 id="orgd0dc9e9"><span class="todo TODO">TODO</span> Protocolos ARQ (Automatic Repeat reQuest)</h6>
<div class="outline-text-6" id="text-orgd0dc9e9">
<p>
Reliable data transfer protocols based on retransmissions depending on the
control messages sent.
</p>

<p>
The types of control messages are
</p>
<ul class="org-ul">
<li>positive acknowledgments</li>
<li><p>
negative acknowledgments
</p>

<p>
These control messages allow the <b>reciever</b> to let the sender know what has
been received correctly, and what has been received in error and thus
requires repeating.
</p>

<p>
three additional protocol capabilities are required in ARQ protocols to
handle the presence of bit errors:
</p>
<dl class="org-dl">
<dt>Error detection</dt><dd>a mechanism that allows the receiver to detect when
bit errors have occurred. Recall from the previous section that UDP uses
the Internet checksum field for exactly this purpose. For now, we need
only know that these techniques require that extra bits (beyond the bits
of original data to be transferred) be sent from the sender to the
receiver; these bits will be gathered into the packet checksum field of
the rdt2.0 data packet.</dd>
<dt>Receiver feedback</dt><dd>the only way for the sender to know whether or not a
packet was received correctly, is for the receiver to provide explicit
feedback to the sender. The positive (<code>ACK</code>) and negative (<code>NAK</code>)
acknowledgment replies in the message-dictation scenario are examples of
such feedback. Our rdt2.0 protocol will similarly send ACK and NAK
packets back from the receiver to the sender. In principle, these packets
need only be one bit long; for example, a 0 value could indicate a NAK
and a value of 1 could indicate an ACK.</dd>
<dt>Retransmission</dt><dd><p>
A packet that is received in error at the receiver will
be retransmitted by the sender.
</p>

<p>
imagen 3.10
</p>

<p>
The send side of rdt2.0 has two states.
</p>

<ul class="org-ul">
<li>In the leftmost state, the send-side protocol is waiting for data to be
passed down from the upper layer. When the <code>rdt_send(data)</code> event occurs,
the sender will create a packet ( <code>sndpkt</code> ) containing the data to be
sent, along with a packet checksum and then send the packet via the
<code>udt_send(sndpkt)</code> operation.</li>
<li><p>
In the rightmost state, the sender protocol is waiting for an ACK or a
NAK packet from the receiver.
</p>
<ul class="org-ul">
<li>If an ACK packet is received the sender knows that the most recently
transmitted packet has been received correctly and thus the protocol
returns to the state of waiting for data from the upper layer.</li>
<li>If a NAK is received, the protocol retransmits the last packet and
waits for an ACK or NAK to be returned by the receiver in response to
the retransmitted data packet.</li>
</ul>

<p>
when the sender is in the wait-for-ACK-or-NAK state, it cannot get more
data from the upper layer; that is, the <code>rdt_send()</code> event can not occur;
that will happen only after the sender receives an ACK and leaves this
state. Thus, the sender will not send a new piece of data until it is sure
that the receiver has correctly received the current packet. Because of
this behavior, protocols such as rdt2.0 are known as <code>stop-and-wait</code>
protocols.
</p>

<p>
The receiver-side FSM for rdt2.0 still has a single state. On packet
arrival, the receiver replies with either an ACK or a NAK, depending on
whether or not the received packet is corrupted.
</p>

<p>
we haven’t accounted for the possibility that the ACK or NAK packet could
be corrupted. Minimally, we will need to add checksum bits to ACK/NAK
packets in order to detect such errors. The more difficult question is how
the protocol should recover from errors in ACK or NAK packets. The
difficulty here is that if an ACK or NAK is corrupted, the sender has no
way of knowing whether or not the receiver has correctly received the last
piece of transmitted data.
</p>

<p>
Consider three possibilities for handling corrupted ACKs or NAKs:
</p>
<ul class="org-ul">
<li>For the first possibility, consider what a human might do in the
message-dictation scenario. If the speaker didn’t understand the “OK” or
“Please repeat that” reply from the receiver, the speaker would probably
ask, “What did you say?” (thus introducing a new type of
sender-to-receiver packet to our protocol).  The receiver would then
repeat the reply. But what if the speaker’s “What did you say?” is
corrupted? The receiver, having no idea whether the garbled sentence was
part of the dictation or a request to repeat the last reply, would
probably then respond with “What did you say?” And then, of course, that
response might be garbled. Clearly, we’re heading down a difficult path.</li>
<li>A second alternative is to add enough checksum bits to allow the sender
not only to detect, but also to recover from, bit errors. This solves the
immediate problem for a channel that can corrupt packets but not lose
them.</li>
<li>A third approach is for the sender simply to resend the current data
packet when it receives a garbled ACK or NAK packet. This approach,
however, introduces duplicate packets into the sender-to-receiver
channel. The fundamental difficulty with duplicate packets is that the
receiver doesn’t know whether the ACK or NAK it last sent was received
correctly at the sender.  Thus, it cannot know a priori whether an
arriving packet contains new data or is a retransmission!</li>
</ul>

<p>
A simple solution is to add a new field to the data packet and have the
sender number its data packets by putting a <code>sequence number</code> into this
field. The receiver then need only check this sequence number to determine
whether or not the received packet is a retransmission. For this simple
case of a stop-and-wait protocol, a 1-bit sequence number will suffice,
since it will allow the receiver to know whether the sender is resending
the previously transmitted packet (the sequence number of the received
packet has the same sequence number as the most recently received packet)
or a new packet.Since we are currently assuming a channel that does not
lose packets, ACK and NAK packets do not themselves need to indicate the
sequence number of the packet they are acknowledging. The sender knows that
a received ACK or NAK packet (whether garbled or not) was generated in
response to its most recently transmitted data packet.
</p>

<p>
imagen 3.11
imagen 3.12
</p>

<p>
The rdt2.1 sender and receiver FSMs each now have twice as many states as
before. This is because the protocol state must now reflect whether the
packet currently being sent (by the sender) or expected (at the receiver)
should have a sequence number of 0 or 1. Note that the actions in those
states where a 0- numbered packet is being sent or expected are mirror
images of those where a 1-numbered packet is being sent or expected; the
only differences have to do with the handling of the sequence number.
</p>

<p>
Protocol rdt2.1 uses both positive and negative acknowledgments from the
receiver to the sender. When an out-of-order packet is received, the
receiver sends a positive acknowledgment for the packet it has
received. When a corrupted packet is received, the receiver sends a
negative acknowledgment. We can accomplish the same effect as a NAK if,
instead of sending a NAK, we send an ACK for the last correctly received
packet.
</p>

<p>
A sender that receives two ACKs for the same packet (that is, receives
duplicate ACKs) knows that the receiver did not correctly receive the
packet following the packet that is being ACKed twice.
</p>

<p>
One subtle change between rtdt2.1 and rdt2.2 is that the receiver must now
include the sequence number of the packet being acknowledged by an ACK
message (this is done by including the ACK , 0 or ACK , 1 argument in
<code>make_pkt()</code> in the receiver FSM), and the sender must now check the
sequence number of the packet being acknowledged by a received ACK message
(this is done by including the 0 or 1 argument in <code>isACK()</code> in the sender
FSM).
</p></li>
</ul></dd>
</dl></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgaa31b5d" class="outline-5">
<h5 id="orgaa31b5d">RDT 3.0: TDC sobre un canal con perdidas y errores</h5>
<div class="outline-text-5" id="text-orgaa31b5d">
<p>
Ahora tambien se asume que el canal de comunicacion tiene perdida de paquetes.
</p>

<ol class="org-ol">
<li>como detectar perdida de paquetes?</li>
<li><p>
que hacer cuando ocurre?
</p>

<p>
el uso de checksums, numeros de secuencias, paquetes ACK, y retransmisiones
nos permiten responder la segunda pregunta.
</p>

<p>
supongamos que el emisor transmite un paquete y que este paquete, o su ACK,
se pierden. en cualquier caso el emisor no recibe una respuesta del
receptor.  si el emisor esta dispuesto a esperar lo suficiente para
asegurarse de que el paquete se perdio, puede luego retransmitir el paquete.
</p>

<p>
pero cuanto tiempo debe esperar para asegurarse de que un paquete se perdio?
</p>

<p>
But how long must the sender wait to be certain that something has been
lost?
</p>

<p>
claramente debe esperar al menos el tiempo rtt mas el tiempo requerido para
procesar el paquete en el receptor. esto es muy dificil de
determinar. ademas el protocolo deberia recuperarse de la perdida de paquete
lo mas rapido posible.
</p>

<p>
si un ACK no se recibe dentro de este tiempo, el paquete es
retransmitido. notar que si el paquete sufre de mucho delay, el emisor puede
retransmitir aunque no se haya perdido el paquete o su ACK. esto introduce
la posibilidad de tener paquetes de datos duplicados (solucionado en rdt
2.2 con numeros de secuencia).
</p>

<p>
imagen 3.14
</p>

<p>
el emisor no sabe si
</p>
<ul class="org-ul">
<li>el paquete se perdio</li>
<li>el ACK se perdio</li>
<li><p>
el paquete o el ACK tuvieron mucho delay.
</p>

<p>
en cualquiera de los casos, la accion el la misma, retransmitir.
</p>

<p>
implementar el mecanismo de retransmision basado en timers requiere de un
timer capaz de interrumpir al emisor.  el emisor debe luego:
</p>
<ol class="org-ol">
<li>empezar el timer cada vez que un paquete se envia (ya sea por primera vez
o una retransmision).</li>
<li>responder a la interrupcion del timer, tomando acciones apropiadas.</li>
<li>frenar el timer</li>
</ol>

<p>
imagen 3.15
</p>

<p>
imagen 3.16
</p></li>
</ul></li>
</ol>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-org844aeaa" class="outline-2">
<h2 id="org844aeaa">Ventana deslizante y TCP</h2>
<div class="outline-text-2" id="text-org844aeaa">
</div>
<div id="outline-container-org7da7716" class="outline-3">
<h3 id="org7da7716"><span class="todo TODO">TODO</span> Pipelined reliable Data Transfer Protocols</h3>
<div class="outline-text-3" id="text-org7da7716">
<p>
Protocol rdt3.0 is a functionally correct protocol, but its performance
problem is the fact that it is a stop-and-wait protocol.
</p>

<p>
imagen 3.16
</p>

<p>
This stop-and-wait protocol limits the sender <code>utilization</code> of the channel
(the amount of time the sender actually sends bits into the channel).
</p>

<p>
image 3.17
</p>


<p>
The solution to this particular performance problem is simple: Rather than
operate in a stop-and-wait manner, the sender is allowed to send multiple
packets without waiting for acknowledgments.
</p>

<p>
illustrated in Figure 3.17(b). Figure 3.18(b) shows that if the sender is
allowed to transmit three packets before having to wait for acknowledgments,
the utilization of the sender is essentially tripled.
</p>

<p>
this technique is known as <code>pipelining</code>. Pipelining has the following
consequences for reliable data transfer protocols:
</p>

<ul class="org-ul">
<li>The range of sequence numbers must be increased, since each in-transit
packet (not counting retransmissions) must have a unique sequence number and
there may be multiple, in-transit, unacknowledged packets.</li>
<li>The sender and receiver sides of the protocols may have to buffer more than
one packet. Minimally, the sender will have to buffer packets that have been
transmitted but not yet acknowledged. Buffering of correctly received
packets may also be needed at the receiver, as discussed below.</li>
<li><p>
The range of sequence numbers needed and the buffering requirements will
depend on the manner in which a data transfer protocol responds to lost,
corrupted, and overly delayed packets. Two basic approaches toward pipelined
error recovery can be identified: <code>Go-Back-N</code> and <code>selective repeat</code>.
</p>

<p>
imagen 3.18
</p></li>
</ul>
</div>

<div id="outline-container-org4fb9306" class="outline-4">
<h4 id="org4fb9306">Go-Back-N (GBN)</h4>
<div class="outline-text-4" id="text-org4fb9306">
<p>
In a Go-Back-N (GBN) protocol, the sender is allowed to transmit multiple
packets (when available) without waiting for an acknowledgment, but is
constrained to have no more than some maximum allowable number, N, of
unacknowledged packets in the pipeline.
</p>

<p>
imagen 3.19
</p>

<p>
Figure 3.19 shows the sender’s view of the range of sequence numbers in a GBN
protocol. If we define <code>base</code> to be the sequence number of the oldest
unacknowledged packet and <code>nextseqnum</code> to be the smallest unused sequence
number (that is, the sequence number of the next packet to be sent), then
four intervals in the range of sequence numbers can be identified.
</p>

<p>
Sequence numbers in the interval
</p>
<ul class="org-ul">
<li><code>[ 0, base-1 ]</code> correspond to packets that have already been transmitted
and acknowledged.</li>
<li><code>[base, nextseqnum-1]</code> corresponds to packets that have been sent but not
yet acknowledged. Sequence numbers in the interval</li>
<li><code>[nextseqnum, base+N-1]</code> can be used for packets that can be sent
immediately, should data arrive from the upper layer.</li>
<li><p>
<code>[base+N, ...]</code> cannot be used until an unacknowledged packet currently in
the pipeline (specifically, the packet with sequence number <code>base</code> ) has
been acknowledged.
</p>

<p>
the range of permissible sequence numbers for transmitted but not yet
acknowledged packets can be viewed as a window of size <code>N</code> over the range of
sequence numbers.
</p>

<p>
As the protocol operates, this window slides forward over the sequence number
space. For this reason, <code>N</code> is often referred to as the <code>window size</code> and the
GBN protocol itself as a <code>sliding-window protocol</code>.
</p>

<p>
You might be wondering <b>why we would even limit the number of outstanding,
unacknowledged packets to a value of N?*</b>. <b>Why not allow an unlimited number
of such packets?</b>
</p>

<p>
flow control is one reason to impose a limit on the sender. We’ll examine
another reason to do so when we study TCP congestion control.
</p>

<p>
In practice, a packet’s sequence number is carried in a fixed-length field in
the packet header. If k is the number of bits in the packet sequence number
field, the range of sequence numbers is thus
</p></li>
</ul>


<p>
Figures 3.20 and 3.21 give an extended FSM description of the sender and
receiver sides of an ACK-based, NAK-free, GBN protocol.
</p>

<p>
imagen 3.20
</p>

<p>
imagen 3.21
</p>

<p>
we have added variables (similar to programming-language variables) for base
and nextseqnum , and added operations on these variables and conditional
actions involving these variables.
</p>

<p>
The GBN sender must respond to three types of events:
</p>
<dl class="org-dl">
<dt>Invocation from above</dt><dd><p>
When rdt<sub>send</sub>() is called from above, the sender
first checks to see if the window is full, that is, whether there are N
outstanding, unacknowledged packets.
</p>
<ul class="org-ul">
<li>If the window is not full, a packet is created and sent, and variables
are appropriately updated.</li>
<li>If the window is full, the sender simply returns the data back to the
upper layer, an implicit indication that the window is full.</li>
</ul>
<p>
The upper layer would presumably then have to try again later. In a real
implementation, the sender would more likely have either buffered (but not
immediately sent) this data, or would have a synchronization mechanism (for
example, a semaphore or a flag) that would allow the upper layer to call
rdt<sub>send</sub>() only when the window is not full.
</p></dd>
<dt>Receipt of an ACK</dt><dd>In our GBN protocol, an acknowledgment for a packet
with sequence number n will be taken to be a <code>cumulative acknowledgment</code>,
indicating that all packets with a sequence number up to and including n
have been correctly received at the receiver.</dd>
<dt>A timeout event</dt><dd>The protocol’s name, “Go-Back-N,” is derived from the
sender’s behavior in the presence of lost or overly delayed packets. As in
the stop-and-wait protocol, a timer will again be used to recover from lost
data or acknowledgment packets. If a timeout occurs, the sender resends all
packets that have been previously sent but that have not yet been
acknowledged. Our sender in Figure 3.20 uses only a single timer, which can
be thought of as a timer for the oldest transmitted but not yet
acknowledged packet. If an ACK is received but there are still additional
transmitted but not yet acknowledged packets, the timer is restarted. If
there are no outstanding, unacknowledged packets, the timer is stopped.</dd>
</dl>


<p>
The receiver’s actions in GBN are :
</p>
<ul class="org-ul">
<li>If a packet with sequence number n is received correctly and is in order
(that is, the data last delivered to the upper layer came from a packet
with sequence number ), the receiver sends an ACK for packet n and delivers
the data portion of the packet to the upper layer.</li>
<li>In all other cases, the receiver discards the packet and resends an ACK for
the most recently received in-order packet. Note that since packets are
delivered one at a time to the upper layer, if packet k has been received
and delivered, then all packets with a sequence number lower than k have
also been delivered. Thus, the use of <code>cumulative acknowledgments</code> is a
natural choice for GBN.</li>
</ul>


<p>
<b>In our GBN protocol, the receiver discards out-of-order packets.</b>
</p>

<p>
Recall that the receiver must deliver data in order to the upper
layer. Suppose now that packet n is expected, but packet n+1 arrives. Because
data must be delivered in order, the receiver could buffer packet n+1 and
then deliver this packet to the upper layer after it had later received and
delivered packet n. However, if packet n is lost, both it and packet n+1 will
eventually be retransmitted as a result of the GBN retransmission rule at the
sender.
</p>

<p>
Thus, the receiver can simply discard packet.
</p>

<p>
The advantage is the simplicity of receiver buffering—the receiver need not
buffer any out-of-order packets. Thus, while the sender must maintain the
upper and lower bounds of its window and the position of <code>nextseqnum</code> within
this window, the only piece of information the receiver need maintain is the
sequence number of the next in-order packet held in the variable
<code>expectedseqnum</code>.
</p>

<p>
the disadvantage of throwing away a correctly received packet is that the
subsequent retransmission of that packet might be lost or garbled and thus
even more retransmissions would be required.
</p>

<p>
imagen 3.22
</p>

<p>
Figure 3.22 shows the operation of the GBN protocol for the case of a window
size of four packets. Because of this window size limitation, the sender
sends packets 0 through 3 but then must wait for one or more of these packets
to be acknowledged before proceeding. As each successive ACK (for example,
ACK0 and ACK1 ) is received, the window slides forward and the sender can
transmit one new packet (pkt4 and pkt5, respectively). On the receiver side,
packet 2 is lost and thus packets 3, 4, and 5 are found to be out of order
and are discarded.
</p>
</div>
</div>

<div id="outline-container-orge84fb6d" class="outline-4">
<h4 id="orge84fb6d">Selective Repeat (SR)</h4>
<div class="outline-text-4" id="text-orge84fb6d">
<p>
There are scenarios in which GBN itself suffers from performance problems. In
particular, when the window size and bandwidth-delay product are both large,
many packets can be in the pipeline. A single packet error can thus cause GBN
to retransmit a large number of packets.
</p>

<p>
As the name suggests, selective-repeat protocols avoid unnecessary
retransmissions by having the sender retransmit only those packets that it
suspects were received in error (that is, were lost or corrupted) at the
receiver. This individual, as-needed, retransmission will require that the
receiver individually acknowledge correctly received packets.
</p>

<p>
A window size of N will again be used to limit the number of outstanding,
unacknowledged packets in the pipeline. However, unlike GBN, the sender will
have already received ACKs for some of the packets in the window.
</p>

<p>
The SR receiver will acknowledge a correctly received packet whether or not
it is in order. Out-of-order packets are buffered until any missing packets
(that is, packets with lower sequence numbers) are received, at which point a
batch of packets can be delivered in order to the upper layer.
</p>

<p>
imagen 3.23
</p>

<p>
imagen 3.24
</p>

<p>
imagen 3.25
</p>

<p>
It is important to note that in Step 2 in Figure 3.25, the receiver
reacknowledges (rather than ignores) already received packets with certain
sequence numbers below the current window base.
</p>

<p>
imagen 3.26
</p>

<p>
Given the sender and receiver sequence number spaces in Figure 3.23, for
example, if there is no ACK for packet send<sub>base</sub> propagating from the
receiver to the sender, the sender will eventually retransmit packet
send<sub>base</sub> , even though it is clear (to us, not the sender!) that the
receiver has already received that packet. If the receiver were not to
acknowledge this packet, the sender’s window would never move forward! This
example illustrates an important aspect of SR protocols (and many other
protocols as well). The sender and receiver will not always have an identical
view of what has been received correctly and what has not. For SR protocols,
this means that the sender and receiver windows will not always coincide.
</p>


<p>
The lack of synchronization between sender and receiver windows has important
consequences when we are faced with the reality of a finite range of sequence
numbers.
</p>

<pre class="example">
   Consider what could happen, for example, with a finite range of four packet
   sequence numbers, 0, 1, 2, 3, and a window size of three.

   Suppose packets 0 through 2 are transmitted and correctly received and
   acknowledged at the receiver. At this point, the receiver’s window is over the
   fourth, fifth, and sixth packets, which have sequence numbers 3, 0, and 1,
   respectively. Now consider two scenarios.

   In the first scenario, shown in Figure 3.27(a), the ACKs for the first three
   packets are lost and the sender retransmits these packets. The receiver thus
   next receives a packet with sequence number 0—a copy of the first packet sent.

   In the second scenario, shown in Figure 3.27(b), the ACKs for the first three
   packets are all delivered correctly. The sender thus moves its window forward
   and sends the fourth, fifth, and sixth packets, with sequence numbers 3, 0, and
   1, respectively. The packet with sequence number 3 is lost, but the packet with
   sequence number 0 arrives—a packet containing new data.

   Now consider the receiver’s viewpoint in Figure 3.27, which has a figurative
   curtain between the sender and the receiver, since the receiver cannot “see” the
   actions taken by the sender. All the receiver observes is the sequence of
   messages it receives from the channel and sends into the channel. As far as it
   is concerned, the two scenarios in Figure 3.27 are identical. There is no way of
   distinguishing the retransmission of the first packet from an original
   transmission of the fifth packet. Clearly, a window size that is 1 less than the
   size of the sequence number space won’t work. But how small must the window size
   be? A problem at the end of the chapter asks you to show that the window size
   must be less than or equal to half the size of the sequence number space for SR
   protocols.
</pre>

<p>
imagen 3.27
</p>

<p>
Table 3.1 Summary of reliable data transfer mechanisms and their use
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Mechanism</td>
<td class="org-left">Use, Comments</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Checksum</td>
<td class="org-left">Used to detect bit errors in a transmitted packet.</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Timer</td>
<td class="org-left">Used to timeout/retransmit a packet, possibly because the packet (or its ACK) was lost within the channel. Because timeouts can occur when a packet is delayed but not lost (premature timeout), or when a packet has been received by the receiver but the receiver-to-sender ACK has been lost, duplicate copies of a packet may be received by a receiver</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Sequence number</td>
<td class="org-left">Used for sequential numbering of packets of data flowing from sender to receiver. Gaps in the sequence numbers of received packets allow the receiver to detect a lost packet. Packets with duplicate sequence numbers allow the receiver to detect duplicate copies of a packet.</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Acknowledgment</td>
<td class="org-left">Used by the receiver to tell the sender that a packet or set of packets has been received correctly. Acknowledgments will typically carry the sequence number of the packet or packets being acknowledged. Acknowledgments may be individual or cumulative, depending on the protocol.</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Negative acknowledgment</td>
<td class="org-left">Used by the receiver to tell the sender that a packet has not been received correctly. Negative acknowledgments will typically carry the sequence number of the packet that was not received correctly.</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Window, pipelining</td>
<td class="org-left">The sender may be restricted to sending only packets with sequence numbers that fall within a given range. By allowing multiple packets to be transmitted but not yet acknowledged, sender utilization can be increased over a stop-and-wait mode of operation. We’ll see shortly that the window size may be set on the basis of the receiver’s ability to receive and buffer messages, or the level of congestion in the network, or both.</td>
<td class="org-left">&#xa0;</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>

<div id="outline-container-org6a05b58" class="outline-3">
<h3 id="org6a05b58">Transporte Orientado a Conexion: TCP</h3>
<div class="outline-text-3" id="text-org6a05b58">
<p>
TCP esta definido en RFC 793, RFC 1122, RFC 1323, RFC 2018, RFC 2581.
</p>
</div>

<div id="outline-container-org38c9c63" class="outline-4">
<h4 id="org38c9c63">The TCP Connection</h4>
<div class="outline-text-4" id="text-org38c9c63">
<p>
Se dice que TCP es orientado a conexion porque antes que una aplicacion
comienze a tranferir datos, ambos procesos deben primero realizar un
handshake con el otro, es decir, deben enviarse segmentos preliminares entre
si para establecer parametros para la transferencia de datos.
</p>

<p>
esta connexion tcp es una conexion logica punto-a-punto (no se puede hacer
multicast). el estado de la conexion es mantenido por ambos extremos de la
misma y no por elementos intermedios de la red. se provee un servicio
full-duplex, es decir que ambos extremos pueden enviar y recibir datos al
mismo tiempo.
</p>

<p>
si un host quiere comunicarse con otro, primero envia un paquete especial al
otro host; el otro host responde con un segundo paquete especial y por ultimo
el primer host responde con un tercer paquete especial. Los primeros dos
paquetes no contienen payload (datos de capa de aplicacion), el tercero puede
no tener.
</p>

<p>
este proceso para establecer la conexion se lo conoce como <code>three-way
handshake</code>.
</p>

<p>
Una vez establecida la conexion, los procesos pueden enviar y recibir datos
entre si. Cuando un proceso envia datos a traves del socket a TCP, TCP los
redirige al <code>buffer de salida</code> de la conexion, que fue creado en el proceso
del handshake. Cada cierto tiempo, TCP toma pedazos del buffer y los pasa a
la capa de red.
</p>

<p>
la cantidad maxima de datos que se pueden enviar a la vez esta limitada por
el <code>maximum segment size (MSS)</code>. En general se determina a partir del frame
mas largo que puede ser enviado por el host que envia (<code>maximum transmission
unit, MTU</code>).
</p>

<p>
Un valor tipico del MSS es 1460B. Observar que el MSS es la cantidad maxima
de datos que una aplicacion puede enviar en el segmento y no el tamaño maximo
del segmento TCP.
</p>

<p>
TCP agrupa pares de datos-del-cliente con un encabezado TCP, formando
<code>segmentos TCP</code>, que son pasados a la capa de red.
</p>

<p>
Del otro lado de la conexion, cuando se recibe un segmento, este es colocado
en el buffer de recepcion de la conexion. La aplicacion lee datos de este
buffer. Cada lado de la conexion tiene su propio buffer de envio y buffer de
recepcion.
</p>

<p>
imagen 3.28
</p>

<p>
la conexion TCP consiste de buffers, variables y un socket asociado a un
proceso en un host y otro conjunto de buffers, variables y un socket asociado
a un proceso en otro host.
</p>
</div>
</div>

<div id="outline-container-org3d953cb" class="outline-4">
<h4 id="org3d953cb">Estructura del segmento TCP</h4>
<div class="outline-text-4" id="text-org3d953cb">
<p>
el segmento consiste de campos del encabezado y el campo de datos.
</p>

<p>
el campo de datos contiene datos de la aplicacion.  el mss limita el tamaño
de este campo.
</p>

<p>
los campos del encabezado incluyen
</p>
<ul class="org-ul">
<li>puertos de origen y destino (multiplexacion y demultiplexacion),</li>
<li>campo de checksum,</li>
<li>numero de secuencia y numero de ACK, ambos numeros de 32 bits.</li>
<li>el campo <code>receive window</code> de 16 bits, utilizado para control de
flujo. indica la cantidad de bytes que el receptor puede aceptar.</li>
<li>el campo <code>largo del encabezado</code> de 4 bits, especifica el largo del
encabezado TCP en words de 32 bits, ya que puede ser de tamaño variable. en
general el encabezado es de 20 bytes.</li>
<li>campo de <code>opciones</code>, opcional y de tamaño variable, utilizado para negociar
el mss o como factor de escala para el tamaño de la ventana.</li>
<li><p>
campo de <code>flags</code> de 6 bits. <code>Bit de ACK</code>, utilizado para indicar que el
valor en el <code>campo de ACK</code> es valido.  Los bits de <code>RST</code>, <code>SYN</code>, y <code>FIN</code>
son utilizados para establecer y abandonar la conexion.  Los bits <code>CWR</code> y
<code>ECE</code> son utilizados para notificar congestion. El bit <code>PSH</code> indica que el
receptor deberia enviar los datos a la capa de arriba inmediatamente. El
bit <code>URG</code> es utilizado para indicar que el transmisor marca a los datos
como urgentes.
</p>

<p>
imagen 3.29
</p></li>
</ul>
</div>

<div id="outline-container-org4f30d8a" class="outline-5">
<h5 id="org4f30d8a">Numeros de secuencia y ACK</h5>
<div class="outline-text-5" id="text-org4f30d8a">
<p>
los dos campos mas importantes de TCP son:
</p>
<ul class="org-ul">
<li>el campo de numero de secuencia</li>
<li><p>
el campo de numero de acuse de recibo
</p>

<p>
que son criticos para la TDC
</p>

<p>
imagen 3.30
</p>

<p>
tcp ve los datos como un stream de bytes, sin estructura pero ordenados. el
uso de numeros de secuencia refleja esta perspectiva porque los numeros de
secuencia son sobre los bytes transmitidos y no sobre los segmentos
transmitidos. el <code>numero de secuencia de un segmento</code> es entonces el numero
de byte del stream correspondiente al primer byte del segmento.
</p>

<p>
un proceso en el host a quiere enviar un stream de datos a un proceso en el
host b sobre tcp. tcp en el host a, numera a cada byte del stream de datos.
supongamos que:
</p>
<ul class="org-ul">
<li>el stream es de 500000 B</li>
<li>mss 1000 B</li>
<li><p>
el primer byte del stream tiene numero 0.
</p>

<p>
tcp contruye 500 segmentos a partir del stream. al primer segmento se le
asigna el numero de secuencia 0, al segundo el numero 1000, luego 2000, y
asi sucesivamente y se colocan en el campo de numero de secuencia del
segmento.
</p>

<p>
consideremos a los numeros de acuse de recibo. recordemos que tcp es full
duplex. el numero de ACK del segmento, es el proximo byte que el receptor
espera a recibir del emisor, es decir, es el numero de secuencia del ultimo
segmento recibido mas uno.
</p>

<p>
supongamos que el host a recibio todos los bytes numerados del 0 al 535 del
host b, y que el host a esta a punto de enviar un segmento a B. el host A
esta esperando a el byte 536 y el resto de los bytes del stram del host
</p>
<ol class="org-ol">
<li>el host A coloca 536 en el campo de numero de acuse de recibo en el</li>
</ol>
<p>
segmento que envia a B.
</p>

<p>
si host A recibe segmentos con bytes de 0 a 535 y 900 a 1000, en el proximo
segmento que envie el host A, tendra el numero de ack 536, para reconstruir
el stream de datos original.
</p>

<p>
al solamente hacer ack de bytes hasta el primer byte faltante, se dice que
tcp provee <code>cumulative acknowledgments</code>.
</p>

<p>
si tcp recibe segmentos fuera de orden (como el ejemplo anterior), tcp
puede:
</p>
<ul class="org-ul">
<li>descartar segmentos fuera de orden</li>
<li>mantener a los segmentos fuera de orden y esperar a que los bytes
faltantes llenen los espacios.</li>
</ul>
<p>
ya que en los rfc, no esta especificado este comportamiento.
</p></li>
</ul></li>
</ul>


<p>
ambos lados de la conexion eligen un numero de secuencia inicial aleatorio.
esto es para minimizar la probabilidad de que un segmento de una conexion
anterior, que este todavia presente en la red sea interpretado cono legitimo
para otra conexion entre los mismos hosts (y que tambien utilice a los
mismos puertos).
</p>
</div>
</div>
</div>

<div id="outline-container-org9e70118" class="outline-4">
<h4 id="org9e70118">Estimacion de Timeout y RTT (Round-Trip Time)</h4>
<div class="outline-text-4" id="text-org9e70118">
<p>
claramente, el tiempo de timeout deberia ser mas largo que el tiempo de rtt
(es tiempo que tarda un paquete desde que es enviado hasta que se reciba su
ack). si no, se haran retransmisiones innecesarias.
</p>
</div>

<div id="outline-container-orgdc9bb2e" class="outline-5">
<h5 id="orgdc9bb2e">Estimando el RTT</h5>
<div class="outline-text-5" id="text-orgdc9bb2e">
<p>
la mayoria de las implementaciones de tcp, toma una muestra del rtt a la
vez, es decir que se muestrea el rtt para uno de los segmentos transmitidos
y que todavia no fue confirmada su recepcion. esto quiere decir que se
muestrea un rtt cada rtt. nunca se calcula un rtt de un segmento que fue
retransmitido, solo segmentos que son transmitidos una vez.
</p>

<p>
claramente, los valores del muestreo fluctuan entre segmentos debido a
la congestion en la red. tcp mantiene una estimacion para mitigar
fluctuaciones entre muestras. la estimacion se actualiza con una nueva
muestra segun:
</p>

<p>
\[Estimated_{RTT} = (1-\alpha)Estimated_{RTT} + \alpha Sample_{RTT}\]
</p>

<p>
[RFC 6298] recomienda el valor &alpha; = 0.125
</p>

<p>
tambien es util estimar la variabilidad del rtt. [RFC 6298] define a la
variacion del rtt como:
</p>

<p>
\[Dev_{RTT} = (1-\beta)Dev_{RTT} + \beta |Sample_{RTT}-Estimated_{RTT}|\]
</p>

<p>
con \(\beta = 0.25\).
</p>
</div>
</div>
</div>

<div id="outline-container-orge5924e5" class="outline-4">
<h4 id="orge5924e5">TDC</h4>
<div class="outline-text-4" id="text-orge5924e5">
<p>
[RFC 6298] recomienda que se utilice un solo timer para retransmisiones,
incluso si hay multiples paquetes transmitidos pero todavia no confirmados.
</p>
</div>

<div id="outline-container-orgda94a45" class="outline-5">
<h5 id="orgda94a45"><span class="todo TODO">TODO</span> Escenarios</h5>
<div class="outline-text-5" id="text-orgda94a45">
<p>
imagen 3.34
</p>

<p>
Host A sends one segment to Host B. Suppose that this segment has sequence
number 92 and contains 8 bytes of data. After sending this segment, Host A
waits for a segment from B with acknowledgment number 100. Although the
segment from A is received at B, the acknowledgment from B to A gets
lost. In this case, the timeout event occurs, and Host A retransmits the
same segment. Of course, when Host B receives the retransmission, it
observes from the sequence number that the segment contains data that has
already been received. Thus, TCP in Host B will discard the bytes in the
retransmitted segment.
</p>

<p>
imagen 3.35
</p>

<p>
Host A sends two segments back to back. The first segment has sequence
number 92 and 8 bytes of data, and the second segment has sequence number
100 and 20 bytes of data. Suppose that both segments arrive intact at B, and
B sends two separate acknowledgments for each of these segments. The first
of these acknowledgments has acknowledgment number 100; the second has
acknowledgment number 120. Suppose now that neither of the acknowledgments
arrives at Host A before the timeout.  When the timeout event occurs, Host A
resends the first segment with sequence number 92 and restarts the timer. As
long as the ACK for the second segment arrives before the new timeout, the
second segment will not be retransmitted.
</p>

<p>
imagen 3.36
</p>

<p>
Host A sends the two segments, exactly as in the second example. The
acknowledgment of the first segment is lost in the network, but before the
timeout event, Host A receives an acknowledgment with acknowledgement
number 120. Host A therefore knows that Host B has received everything up
through byte 119; so Host A does not resend either of the two segments.
</p>
</div>
</div>

<div id="outline-container-org95721c5" class="outline-5">
<h5 id="org95721c5">Duplicando el intervalo de timeout</h5>
<div class="outline-text-5" id="text-org95721c5">
<p>
cuando ocurre un timeout, configura el proximo tiempo de timeout al doble
del anterior.
</p>

<p>
Esta moficicacion provee una forma limitada de control de congestion. La
expiracion del timer, probablemente es causada por congestion en la red.
</p>
</div>
</div>

<div id="outline-container-orgefc3eb4" class="outline-5">
<h5 id="orgefc3eb4"><span class="todo TODO">TODO</span> Fast retransmit</h5>
<div class="outline-text-5" id="text-orgefc3eb4">
<p>
timeout-triggered retransmissions can be relatively long. When a segment is
lost, this long timeout period forces the sender to delay resending the lost
packet, thereby increasing the end-to-end delay. Fortunately, the sender can
often detect packet loss well before the timeout event occurs by noting
so-called duplicate ACKs. A <code>duplicate ACK</code> is an ACK that reacknowledges a
segment for which the sender has already received an earlier acknowledgment.
</p>


<p>
To understand the sender’s response to a duplicate ACK, we must look at why
the receiver sends a duplicate ACK in the first place.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Event</td>
<td class="org-left">TCP Receiver Action</td>
</tr>

<tr>
<td class="org-left">Arrival of in-order segment with expected sequence number. All data up to expected sequence number already acknowledged.</td>
<td class="org-left">Delayed ACK. Wait up to 500 msec for arrival of another in-order segment. If next in-order segment does not arrive in this interval, send an ACK.</td>
</tr>

<tr>
<td class="org-left">Arrival of in-order segment with expected sequence number. One other in-order segment waiting for ACK transmission.</td>
<td class="org-left">One Immediately send single cumulative ACK, ACKing both in-order segments.</td>
</tr>

<tr>
<td class="org-left">Arrival of out-of-order segment with higher-than-expected sequence number. Gap detected.</td>
<td class="org-left">Immediately send duplicate ACK, indicating sequence number of next expected byte (which is the lower end of the gap).</td>
</tr>

<tr>
<td class="org-left">Arrival of segment that partially or completely fills in gap in received data.</td>
<td class="org-left">Immediately send ACK, provided that segment starts at the lower end of gap.</td>
</tr>
</tbody>
</table>

<p>
When a TCP receiver receives a segment with a sequence number that is larger
than the next, expected, in-order sequence number, it detects a gap in the
data stream—that is, a missing segment. This gap could be the result of lost
or reordered segments within the network. Since TCP does not use negative
acknowledgments, the receiver cannot send an explicit negative
acknowledgment back to the sender. Instead, it simply reacknowledges (that
is, generates a duplicate ACK for) the last in-order byte of data it has
received.
</p>

<p>
If the TCP sender receives three duplicate ACKs for the same data, it takes
this as an indication that the segment following the segment that has been
ACKed three times has been lost. In the case that three duplicate ACKs are
received, the TCP sender performs a <code>fast retransmit</code> [RFC 5681],
retransmitting the missing segment before that segment’s timer expires. This
is shown in Figure 3.37, where the second segment is lost, then
retransmitted before its timer expires.
</p>

<p>
imagen 3.37
</p>
</div>
</div>

<div id="outline-container-org2acdf95" class="outline-5">
<h5 id="org2acdf95">Go-Back-N o Selective Repeat?</h5>
<div class="outline-text-5" id="text-org2acdf95">
<p>
TCP es un protocolo Go-Back-N o Selective Repeat?
</p>

<p>
recordemos que los acks de tcp son cumulativos y que los segmentos
correctamente recibidos pero fuera de orden no son confirmados por el
receptor.  el emisor tcp solo mantiene el menor numero de secuencia
transmitido pero no confirmado (SendBase) y el numero de secuencia a
transmitir (NextSeqNum). en este sentido, tcp es un protocolo gbn.
</p>

<p>
aunque, hay diferencias entre gbn y tcp. muchas implementaciones de tcp, si
guardan segmentos recibidos fuera de orden. tambien consideremos el caso en
que se transmiten y reciben los paquetes de 1 a N, pero se pierde el ack
para el paquete n&lt;N, pero el resto de los N-1 acuses de recibo llegan al
emisor antes de que expire el timer. en este caso gbn, retransmitiria el
paquete n y el resto n+1,n+2,&#x2026;,N. tcp por el otro lado, solo retransmite
el segmento n en caso de que el timer de n expire antes de llegar el acuse
de n+1.
</p>

<p>
una modificacion propuesta en [RFC 2018] para TCP, permite a tcp confirmar
paquetes recibidos fuera de orden de forma selectiva en vez de confirmar de
forma cumulativa al utlimo paquete recibido correctamente en orden. esto
hace que se parezca al protocolo de selective repeat.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-orgf490c03" class="outline-2">
<h2 id="orgf490c03">Control de Congestion</h2>
<div class="outline-text-2" id="text-orgf490c03">
</div>
<div id="outline-container-orge799199" class="outline-3">
<h3 id="orge799199">Transporte Orientado a Conexion</h3>
<div class="outline-text-3" id="text-orge799199">
</div>
<div id="outline-container-orge5fd4a4" class="outline-4">
<h4 id="orge5fd4a4">Control de flujo</h4>
<div class="outline-text-4" id="text-orge5fd4a4">
<p>
recordemos que cada extremo de la conexion tcp, reserva un buffer para la
recepcion de datos. los datos se colocan en el buffer si son correctos y estan
en orden. la aplicacion puede tardar un tiempo en leer los datos y por lo tanto,
el emisor puede desbordar el buffer receptor.
</p>

<p>
tcp proveee un servicio de control de flujo para evitar que esto suceda. el
control de flujo es entonces un servicio para equilibrar las velocidades de
transmision del emisor y lectura de la aplicacion receptora.
</p>

<p>
(esto es diferente de control de congestion, en el sentido de que cc es debido a
congestion en la red. cf es debido a la sincronizacion entre emisor y receptor)
</p>

<p>
we suppose throughout this section that the TCP implementation is such that the
TCP receiver discards out-of-order segments.
</p>

<p>
se provee cf al hacer que el emisor mantenga una variable llamada <code>receive
window</code>. le da al emisor una idea de cuanto espacio libre hay disponible en el
buffer receptor.
</p>

<p>
Suppose that Host A is sending a large file to Host B over a TCP
connection. Host B allocates a receive buffer to this connection; denote its
size by RcvBuffer. From time to time, the application process in Host B reads
from the buffer.
</p>

<p>
Define the following variables:
</p>
<ul class="org-ul">
<li>LastByteRead: numero del ultimo byte leido en el stream de datos en el buffer
por el proceso en el host B</li>
<li>LastByteRcvd: numero del tulimo byte del stream que llego de la red y fue
colocado en el buffer en B.</li>
</ul>

<p>
como no se puede desbordar el buffer alocado:
</p>

<p>
\[LastByteRcvd−LastByteRead \leq RcvBuffer\]
</p>

<p>
La ventana de recepcion (\(rwnd\)) se configura para que sea la cantidad de
espacio libre en el buffer:
</p>

<p>
\[rwnd=RcvBuffer−[LastByteRcvd−LastByteRead]\]
</p>

<p>
imagen 3.38
</p>

<p>
el host b notifica al host a cuanto espacio libre tiene en su buffer al
colocar el valor actual de rwnd en el campo recieve window del header tcp.
</p>

<p>
por el otro lado, el host A mantiene dos variables:
</p>
<ul class="org-ul">
<li>LastByteSent</li>
<li>LastByteAcked</li>
</ul>

<p>
La diferencia \(LastByteSent - LastByteAcked\) es la cantidad de datos no
confirmados que A envio a la conexion.  si esta diferencia es menor a rwnd,
entonces el host A se asegura de que no esta desbordando el buffer en el host
</p>
<ol class="org-ol">
<li></li>
</ol>

<p>
\[LastByteSent-LastByteAcked \leq rwnd\]
</p>

<p>
Que pasa si el receptor notifica que rwnd=0 y no tiene mensajes para enviar?
El emisor nunca podria saber cuando el receptor tiene espacio el buffer para
seguir almacenando datos.  por este motivo, la especificacion de TCP requiere
que el host A continue enviado segmentos de un byte de datos cuando el rwnd
de B es 0. estos segmentos son confirmados por el receptor. Eventualmente el
buffer del receptor se liberara y el rwnd aumentara.
</p>

<p>
udp no realiza control de flujo.
</p>
</div>
</div>

<div id="outline-container-org391f4f4" class="outline-4">
<h4 id="org391f4f4">Administracion de conexion de TCP</h4>
<div class="outline-text-4" id="text-org391f4f4">
<p>
Como se establece y se termina una conexion TCP.
</p>

<p>
por que?
</p>
<ul class="org-ul">
<li>puede incrementar delay percivido</li>
<li>entender explotacion de vulnerabilidades</li>
</ul>

<p>
supongamos que un cliente quiere iniciar una connexion con un server:
</p>
<ol class="org-ol">
<li>TCP del lado cliente envia un segmento TCP especial al lado servidor. No
contiene datos de la capa de aplicacion, pero un bit del campo de flags
del encabezado del segmento, bit SYN, se setea a 1. Segmento SYN. Se elige
un numero de secuencia inicial de forma aleatoria (client<sub>isn</sub>). El
segmento se encapsula en un datagrama IP y se envia al servidor.</li>
<li>una vez que llega el datagrama al servidor, se extrae el segmento TCP, se
reserva espacio para los buffers y variables de la conexion, y se envia un
segmento de conexion-aprobada al cliente. Este segmento tampoco contiene
datos de capa de aplicacion, pero el encabezado TCP si contiene el bit SYN
en 1, el campo de ACK en client<sub>isn</sub>+1, y el numero-de-secuencia, inicial,
aleatorio, del servidor (server<sub>isn</sub>). Al este segmento se lo llama SYNACK</li>
<li>cuando el cliente recibe este segmento SYNACK, este reserva buffers y
variables para la conexion, y se envia al servidor un ultimo segmento para
indicar al servidor que se recibio el segmento de conexion-aprobada. Este
segmento tiene el bit de SYN en 0, numero de ACK server<sub>isn</sub>+1 y puede
tener datos de capa de aplicacion en el payload del segmento.</li>
</ol>

<p>
una vez realizados estos pasos, el cliente y servidor pueden enviarse
segmentos con datos entre si. (SYN = 0).
</p>

<p>
imagen 3.39
</p>

<p>
a este proceso (de establecimiento de conexion) se lo llama <code>three way
handshake</code>.
</p>

<p>
una vez que se quiera finalizar la conexion, los recursos reservados para
esta, deben liberarse.
</p>

<p>
imagen 3.40
</p>

<p>
supongamos que el cliente desea cerrar la conexion. este envia un comando
close. TCP del lado del cliente envia un segmento especial con el bit FIN
en 1.  Cuando el servidor recibe este segmento, responde con un ACK. Luego el
servidor envia otro segmento, esta vez con el bit FIN en 1, a lo que el
cliente responde con un ACK. En este punto es cuando se pueden liberar los
recursos.
</p>

<p>
imagen 3.41
</p>

<p>
se muestra en la imagen los posibles estados de TCP en el lado del cliente.
</p>

<p>
imagen 3.42
</p>

<p>
se muestra en la imagen los posibles estados de TCP en el lado del servidor.
</p>


<p>
Que pasa si el host receptor no tiene ningun proceso escuchando en un puerto,
al que otro host quiere conecarse?
</p>

<p>
El host receptor envia un segmento con el bit RST en 1 de vuelta a la fuente,
indicando que no hay un socket para el segmento recibido.
</p>
</div>

<div id="outline-container-org8a6af12" class="outline-5">
<h5 id="org8a6af12">SYN FLOOD attack</h5>
<div class="outline-text-5" id="text-org8a6af12">
<p>
Al establecer la conexion TCP, un servidor reserva recursos para la misma y
luego envia un segmento SYNACK. Una forma de atacar al servidor es un Denial
of Service (DoS) mediante un SYN flood.
</p>

<p>
Consiste de enviar una gran cantidad de segmentos SYN, sin intencion de
completar el tercer paso del handshake. El servidor reserva recursos para
cada conexion falsa, lo que denega el servicio a clientes legitimos.
</p>

<p>
para contrarrestar esto se utilizan SYN cookies [RFC 4987] desplegadas en la
mayoria de los sistemas operativos
</p>

<ul class="org-ul">
<li>Cuando el servidor recibe un segmento SYN, el servidor crea un numero de
secuencia inicial a partir de un hash de: las ip de origen y destino;
numeros de puertos de origen y destino; y un numero secreto que solo
conoce el servidor. Este numero especial se llama cookie. El servidor
envia un SYNACK con el cookie como numero de secuencia. El servidor no
guarda informacion del estado de esta conexion (ni recursos ni cookie,
nada).</li>
<li>Un cliente legitimo devuleve el ACK. Cuando el servidor recive el
segmento, recalcula el hash y verifica que el numero del ACK sea el cookie
(recalculado) mas 1. Si es el caso, el servidor crea una conexion
completa.</li>
<li>Por el otro lado, si el cliente no responde con ACK, entonces el SYN
original no hace daño al servidor, ya que el servidor no reservo recursos.</li>
</ul>
</div>
</div>
</div>
</div>

<div id="outline-container-org8ff1097" class="outline-3">
<h3 id="org8ff1097"><span class="todo TODO">TODO</span> Principios de Control de Congestion</h3>
<div class="outline-text-3" id="text-org8ff1097">
<p>
Analizamos Control de congestion en un contexto generico:
</p>

<p>
congestion:
</p>
<ul class="org-ul">
<li>por que es malo</li>
<li>como se manifiesta (como es percivida en capas superiores)</li>
<li>como evitarla</li>
</ul>
</div>

<div id="outline-container-orga9ee02c" class="outline-4">
<h4 id="orga9ee02c">Causas y costos de la congestion</h4>
<div class="outline-text-4" id="text-orga9ee02c">
</div>
<div id="outline-container-orgfd16b39" class="outline-5">
<h5 id="orgfd16b39">escenario 1 - dos emisores, un router con buffers infinitos</h5>
<div class="outline-text-5" id="text-orgfd16b39">
<p>
imagen 3.43
</p>

<p>
el host a transmite datos con un promedio de &lambda;<sub>in</sub> bytes/seg. estos datos
son originales en el sentido en que cada unidad se envia al socket solo una
vez.
</p>

<p>
el protocolo de capa de transporte es simple, solo encapsula y envia. sin
recuperacion de errores, sin control de flujo o control de congestion.
</p>

<p>
ignorando el delay de otras capas, la velocidad de transmision el &lambda;<sub>in</sub>
</p>

<p>
el host b opera de forma similar. asumimos tambien que transmite a una velocidad
de &lambda;<sub>in</sub>.
</p>

<p>
a y b comparten un enlace de capacidad R al router. el router tiene buffers en
dicho enlace para que no halla overflow.
</p>

<p>
imagen 3.44
</p>

<p>
la imagen muestra el throughput en funcion de la velocidad de transmision. si la
velocidad de transmision va entre 0 a R/2, todo va bien, todo lo que se
transmite, se recibe. Cuando la velocidad de transmision pasa R/2, el throughput
se limita a R/2. el enlace no puede entregar paquetes que exceden la capacidad
del enlace. maximizar el throughput puede parecer bueno, porque se aprovecha la
capacidad del enlace, pero el otro grafico de la imagen 3.44, muestra las
consecuencias de operar al limite de la capacidad del enlace. cuando se excede
la transmision de R/2, la cantidad de paquetes encolados en el router incrementa
sin limite, por lo que el delay tambien incrementa.
</p>

<p>
se tienen delays de encolado a medida que la velocidad de arrivos de paquetes
(al router) se acerca a la capacidad del enlace.
</p>
</div>
</div>

<div id="outline-container-orgf2553d7" class="outline-5">
<h5 id="orgf2553d7">escenario 2 - dos emisores, un router con buffers finitos</h5>
<div class="outline-text-5" id="text-orgf2553d7">
<p>
en este escenario, puede ocurrir perdida de paquetes.
la conexion es confiable. si ocurre una perdida, el protocolo de
capa-de-transporte se encarga de retransmitir.
</p>

<p>
ahora hay una distincion entre transmision de datos-originales y transmision de
datos-originales-y-retransmitidos, &lambda;<sub>in</sub> y &lambda;<sub>in</sub><sup>&rsquo;</sup>
respectivamente.
</p>

<p>
a &lambda;<sub>in</sub><sup>&rsquo;</sup> tambien se lo llama <code>offered load</code> a la red.
</p>

<p>
&#x2026;
</p>

<p>
el transmisor debe retransmitir para compensar por perdida de paquetes debido a
buffer overflow.
</p>


<p>
retransmisiones innecesarias ante largos delays pueden causar que un router
utilice el bandwidth del enlace para enviar copias de paquetes inncesarias.
</p>
</div>
</div>

<div id="outline-container-org69d8d6f" class="outline-5">
<h5 id="org69d8d6f">escenario 3 - cuatro emisores, routers con buffers finitos, multiples saltos</h5>
<div class="outline-text-5" id="text-org69d8d6f">
<p>
imagen 3.47
</p>

<p>
&#x2026;
</p>

<p>
cuando un paquete se pierde en la ruta, la capacidad de transmision que fue
utilizada para enviar el paquete hasta el punto en el que se perdio, fue
desperdiciada.
</p>

<p>
&#x2026;
</p>
</div>
</div>
</div>

<div id="outline-container-org2d506bb" class="outline-4">
<h4 id="org2d506bb">Abordando el control de congestion</h4>
<div class="outline-text-4" id="text-org2d506bb">
<p>
la capa de red puede asistir o no, a la capa de transporte para el control de
congestion:
</p>

<dl class="org-dl">
<dt>End-to-end congestion control</dt><dd>la capa-de-red no provee apoyo explicito a la
capa-de-transporte para el control de congestion. Los end-systems deben
inferir la presencia de congestion basados solamente en obsevaciones de la
red. La perdida de paquetes en TCP (por timeout o 3 ACK consecutivos) se toma
como congestion de la red.</dd>
<dt>Network-assisted congestion control</dt><dd><p>
los routers proveen feedback explicito
al emisor sobre el estado de la red. puede consistir de un solo bit indicando
la congestion en el enlace. IBM SNA, DEC DECnet, ATM.  El control de
congestion puede ser mas sofisticado, por ejemplo el router informa al emisor
sobre la capacidad maxima de transmision que tiene en un enlace saliente
(visto en ATM Available Bite Rate (ABR))
</p>

<p>
otra forma: cuando un paquete pasa por un router con congestion, el router marca
al encabezado indicando esto. cuando el receptor responde al emisor, lo hace con
la misma marca en el encabezado, de esta forma indicando al emisor de la
congestion. (toma un RTT)
</p></dd>
</dl>

<p>
imagen 3.49
</p>
</div>
</div>
</div>

<div id="outline-container-org82e263c" class="outline-3">
<h3 id="org82e263c">Control de Congestion de TCP</h3>
<div class="outline-text-3" id="text-org82e263c">
<p>
tcp debe usar control de congestion end-to-end ya que ip no provee asistencia.
</p>

<p>
en tcp cada transmisor limita la velocidad a la cual transmite trafico a la
conexion en funcion de la congestion percivida en la red.
</p>

<ul class="org-ul">
<li>como limita la velocidad de transmision?</li>
<li>como percive la congestion en la red?</li>
<li>que algoritmo deberia utilizar para cambiar la velocidad de transmision?</li>
</ul>

<p>
el emisor en tcp mantiene:
</p>
<ul class="org-ul">
<li>buffer de entrada y salida</li>
<li>LastByteRead</li>
<li>rwnd</li>
<li><code>congestion windows (cwnd)</code> : limita a la velocidad de transmision. La
cantidad de datos sin ACK en el emisor no puede superar al minimo entre cwnd y
rwnd
\[LastByteSent - LastByteAcked \leq \min\{cwnd,rwnd\}\]</li>
</ul>

<p>
se considera a un paquete perdido cuando ocurre un timeout o cuando el emisor
recibe tres ACK duplicados
</p>

<p>
por el otro lado, suponiendo que no hay perdidas de paquetes, el arrivo de
paquetes indica al emisor que todo anda bien y que se puede incrementar la
ventana de congestion. si los ACKs llegan despacio, la ventana de congestion
incrementa despacio. por este uso de este mecanismo, se dice que TCP es
<code>self-clocking</code>.
</p>

<ul class="org-ul">
<li>un segmento perdido implica congestion y por lo tanto, el emisor debe bajar la
velocidad de transmision</li>
<li>un segmento confirmado indica que la red entrega los segmentos al receptor y
por lo tanto el emisor puede incrementar la velocidad de transmision cuando
llegue el ACK de un paquete enviado.</li>
</ul>

<p>
El algoritmo de control de congestion definido en [RFC 5681] tiene 3
componentes:
</p>
<ol class="org-ol">
<li>slow start</li>
<li>congestion avoidance</li>
<li>fast recovery (opcional)</li>
</ol>
</div>

<div id="outline-container-org994e216" class="outline-4">
<h4 id="org994e216">Slow Start (SS)</h4>
<div class="outline-text-4" id="text-org994e216">
<p>
cuando la conexion TCP empieza , en general cwnd = 1MSS
[RFC 3390]
</p>

<p>
por lo que la velocidad de transmision es MSS/RTT aprox. Se incrementa en 1 MSS
cada vez que se recibe un ACK.
</p>

<p>
imagen 3.50
</p>

<p>
crecimiento exponencial.
</p>

<p>
como termina ss?
</p>
<ul class="org-ul">
<li>si hay una perdida por timeout, cwnd=1, y se mantiene en modo SS
se establece una variable ssthresh (slow start threshold) = cwnd/2 , cuando se
detecta congestion</li>
<li>si cwnd=ssthresh, se ingresa en modo (CA congestion avoidance)</li>
<li>si se reciben 3 ACKs duplicados (4 ACKs iguales), se realiza fast retransmit y
se ingresa en modo fast recovery</li>
</ul>

<p>
imagen 3.51
</p>
</div>
</div>

<div id="outline-container-orge87bcab" class="outline-4">
<h4 id="orge87bcab"><span class="todo TODO">TODO</span> Congestion Avoidance (CA)</h4>
<div class="outline-text-4" id="text-orge87bcab">
<p>
[RFC 5681]
</p>

<p>
se setea a cwnd = cwnd/2 cuando se detecta congestion
</p>

<p>
por cada RTT, se incrementa cwnd en 1 MSS.  en realidad se incrementa por
cada ACK recibido, pero al enviarse cwnd/MSS (cantidad de MSSs/segmentos de
tamaño MSS) y cada ACK incrementa cwnd en MSS/cwnd; si se reciben todos los
ACKs, se termina incrementando cwnd en 1 MSS = cwnd/MSS (ACKs recibidos) *
MSS/cwnd (incremento por ACK)
</p>


<p>
como termina CA?
se entra en modo FR
</p>
</div>
</div>

<div id="outline-container-org1d0ac70" class="outline-4">
<h4 id="org1d0ac70"><span class="todo TODO">TODO</span> Fast Recovery (FR)</h4>
<div class="outline-text-4" id="text-org1d0ac70">
<p>
[RFC 5681]
</p>

<p>
el valor de cwnd se incrementa en 1 MSS por cada ACK duplicado recibido para el segmento que causo que TCP entrara en modo FR.
</p>

<p>
utilizado en TCP Reno
</p>
</div>
</div>

<div id="outline-container-orga8b03cb" class="outline-4">
<h4 id="orga8b03cb">en retrospectiva</h4>
<div class="outline-text-4" id="text-orga8b03cb">
<p>
asumiendo que las perdidas ocurren por ACKs duplicados y no por timeout, el control de congestion de TCP consiste de incrementos aditivos y decrementos multiplicativos de la ventana de congestion (cwnd).
</p>

<p>
imagen 3.53
</p>
</div>
</div>

<div id="outline-container-org4cabfea" class="outline-4">
<h4 id="org4cabfea"><span class="todo TODO">TODO</span> TCP Splitting</h4>
</div>
</div>

<div id="outline-container-org5318797" class="outline-3">
<h3 id="org5318797">Resumen</h3>
<div class="outline-text-3" id="text-org5318797">
<p>
servicios que un protocolo de capa de transporte provee a aplicaciones de red.
</p>

<ul class="org-ul">
<li>mux/demux</li>
<li>entrega de datos confiable</li>
<li>garantia de delay</li>
<li>garantia de ancho de banda</li>
</ul>

<p>
servicios que se proveen estan restringidos por el protocolo de capa de red
(que esta abajo) si no se proveen garantias de delay y ancho de banda a
segmentos de capa de transporte, no se puede proveer estas garantias a
mensajes entre aplicaciones
</p>

<p>
se puede proveer TDC a traves de acuse de recibo, timers , retransmisiones y
numeros de secuencia.
</p>

<p>
TCP provee:
</p>
<ul class="org-ul">
<li>administracion de conexion</li>
<li>control de flujo</li>
<li>estimacion de tiempo de round-trip</li>
<li>TDC</li>
<li>control de congestion end-to-end que incrementa la velocidad de transmision de forma aditica y la decrementa de forma multiplicativa cuando se detecta perdida de paquetes.</li>
</ul>

<p>
La complejidad de TCP esta oculta para la aplicacion de red.
</p>

<p>
The Datagram Congestion Control Protocol (DCCP) [RFC 4340] provides a
low-overhead, message-oriented, UDP-like unreliable service, but with an
application-selected form of congestion control that iscompatible with TCP. If
reliable or semi-reliable data transfer is needed by an application, then
thiswould be performed within the application itself, perhaps using the
mechanisms we have studied inSection 3.4. DCCP is envisioned for use in
applications such as streaming media thatcan exploit the tradeoff between
timeliness and reliability of data delivery, but that want to be responsiveto
network congestion.
</p>

<p>
Google’s QUIC (Quick UDP Internet Connections) protocol [Iyengar
2016], implemented in Google’sChromium browser, provides reliability via
retransmission as well as error correction, fast-connectionsetup, and a
rate-based congestion control algorithm that aims to be TCP friendly—all
implemented asan application-level protocol on top of UDP. o
</p>

<p>
DCTCP (Data Center TCP) [Alizadeh 2010] is a version of TCP
designed specifically for data centernetworks, and uses ECN to better support
the mix of short- and long-lived flows that characterize datacenter
workloads.
</p>

<p>
The Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286]
is a reliable, message-oriented protocol that allows several different
application-level “streams” to be multiplexed through asingle SCTP connection
(an approach known as “multi-streaming”).
From a reliability standpoint,
thedifferent streams within the connection are handled separately, so that
packet loss in one stream doesnot affect the delivery of data in other
streams.
</p>

<p>
QUIC provides similar multi-stream semantics.
</p>

<p>
SCTP also allows data to
be transferred over two outgoing paths when a host is connected to two or
morenetworks, optional delivery of out-of-order data, and a number of other
features.
</p>

<p>
SCTP’s flow- andcongestion-control algorithms are essentially the same
as in TCP.
</p>

<p>
The TCP-Friendly Rate Control (TFRC) protocol [RFC 5348] is a
congestion-control protocol rather thana full-fledged transport-layer
protocol. It specifies a congestion-control mechanism that could be used
inanother transport protocol such as DCCP (indeed one of the two
application-selectable protocolsavailable in DCCP is TFRC). The goal of TFRC
is to smooth out the “saw tooth” behavior (see Fig­ure3.53) in TCP congestion
control, while maintaining a long-term sending rate that is “reasonably” close
tothat of TCP. With a smoother sending rate than TCP, TFRC is well-suited for
multimedia applicationssuch as IP telephony or streaming media where such a
smooth rate is important. TFRC is an “equation-based” protocol that uses the
measured packet loss rate as input to an equation [Padhye 2000] thatestimates
what TCP’s throughput would be if a TCP session experiences that loss rate.
This rate is thentaken as TFRC’s target sending rate.
</p>
</div>
</div>
</div>
<div id="outline-container-orgc23d169" class="outline-2">
<h2 id="orgc23d169">Arquitectura de routers, IP y fragmentacion</h2>
<div class="outline-text-2" id="text-orgc23d169">
<p>
la capa de red se puede descomponer en dos partes que interactuan entre si, el
<code>data plane</code> y <code>control plane</code>.
</p>

<p>
las funciones del <code>data plane</code> de la capa de red son las funciones por-enrutador
que determinan cómo un datagrama (paquete de capa de red) que llega a uno de los
enlaces de entrada de un enrutador se reenvía a uno de los enlaces de salida.
</p>

<p>
las funciones del <code>control plane</code> de la capa de red es la lógica de toda la red
que controla cómo se enruta un datagrama entre enrutadores a lo largo de una
ruta de extremo a extremo desde el host de origen al host de destino.
</p>

<p>
Tradicionalmente, estos protocolos de enrutamiento del plano de control y el
forwarding se han implementado juntas, monolíticamente, dentro de un enrutador.
</p>

<p>
en <code>Software-defined networking (SDN)</code>, se separan explícitamente el <code>data
plane</code> y <code>control plane</code> mediante la implementación de las funciones del
<code>control plane</code> como un servicio separado, normalmente en un &ldquo;controlador&rdquo;
remoto.
</p>
</div>

<div id="outline-container-org8ec13a8" class="outline-3">
<h3 id="org8ec13a8">Overview of the Network Layer</h3>
<div class="outline-text-3" id="text-org8ec13a8">
<p>
Figure 4.1 shows a simple network with two hosts, H1 and H2, and several routers
on the path between H1 and H2. Let’s suppose that H1 is sending information to
H2, and consider the role of the network layer in these hosts and in the
intervening routers. The network layer in H1 takes segments from the transport
layer in H1, encapsulates each segment into a datagram, and then sends the
datagrams to its nearby router, R1. At the receiving host, H2, the network layer
receives the datagrams from its nearby router R2, extracts the transport-layer
segments, and delivers the segments up to the transport layer at H2.
</p>


<p>
la funcion principal del data plane de cada router es enviar datagramas desde
los enlaces de entrada a los de salida
</p>

<p>
la funcion principal del control plane es coordinar las acciones de envio
por-router para que los datagramas lleguen a su destino.
</p>

<p>
los routers no ejecutan protocoloes de capa de aplicacion o capa de transporte
</p>

<p>
imagen 4.1
</p>
</div>

<div id="outline-container-org3e2ad53" class="outline-4">
<h4 id="org3e2ad53">Forwarding y Routing: The Data and Control Planes</h4>
<div class="outline-text-4" id="text-org3e2ad53">
<p>
el rol principal de la capa de red es mover paquetes del host emisor al host
receptor.
</p>
</div>

<div id="outline-container-org5ea1c03" class="outline-5">
<h5 id="org5ea1c03">Forwarding</h5>
<div class="outline-text-5" id="text-org5ea1c03">
<p>
cuando un paquete llega al router por el enlace de entrada, el router debe
moverlo al enlace de salida apropiado.
</p>

<p>
For example, a packet arriving from Host H1 to Router R1 in Figure 4.1 must be
forwarded to the next router on a path to H2.
</p>


<p>
forwarding es solo una funcion implementada en el data plane.
</p>

<p>
en otro caso, un paquete puede tener la salida bloqueada de un router (por ej,
si el paquete se origina de host malicioso conocido o si tiene un host destino
prohibido), o puede ser duplicado y enviado por multiples enlaces de salida.
</p>

<p>
para lograr el forwarding, un router tiene una <code>forwarding table</code>. en router
envia un paquete al examinar los valores de algunos campos del paquete, y los
consulta en la tabla que termina indicando el enlace de salida apropiado para
dicho paquete.
</p>
</div>
</div>

<div id="outline-container-org663abb7" class="outline-5">
<h5 id="org663abb7">Routing</h5>
<div class="outline-text-5" id="text-org663abb7">
<p>
la capa de red debe determinar la ruta o el camino que deben seguir los paquetes
a medida que fluyen desde el origen a su destino. Los algortimos de enrutamiento
calculan estos caminos. El enrutamiento es implementado en el control plane.
</p>

<p>
imagen 4.2
</p>
</div>
</div>

<div id="outline-container-org44cf957" class="outline-5">
<h5 id="org44cf957">Control Plane: The Traditional Approach</h5>
<div class="outline-text-5" id="text-org44cf957">
<p>
como se configuran las forwading tables?
</p>

<p>
el algoritmo de enrutamiento determina el contenido de las tablas. este se
ejecuta en cada router y ambas funciones de forwarding y ruteo estan contenidas
en el router.
</p>

<p>
el algoritmo de ruteo de un router se comunica con otro de otro router para
calcular los valores para la forwarding table.
</p>
</div>
</div>

<div id="outline-container-org6da7ac0" class="outline-5">
<h5 id="org6da7ac0">Control Plane: The SDN Approach</h5>
<div class="outline-text-5" id="text-org6da7ac0">
<p>
imagen 4.3
</p>

<p>
un metodo alternativo consiste de un controlador remoto fisicamente separado,
computa y distribuye las forwarding tables a cada router.
</p>

<p>
el enrutamiento es separado fisicamente del router, solamente realiza
forwarding.
</p>

<p>
el controlador remoto puede estar implementado en un data center, con alta
redundancia y confiabilidad
</p>
</div>
</div>
</div>

<div id="outline-container-org3a74f0d" class="outline-4">
<h4 id="org3a74f0d">Network Service Model</h4>
<div class="outline-text-4" id="text-org3a74f0d">
<p>
El <code>network service model</code> define las caracteristicas de la entrega de paquetes
end-to-end.
</p>

<p>
Algunos posibles servicios que puede ofrecer la capa de red son:
</p>
<dl class="org-dl">
<dt>Entrega garantizada</dt><dd>Se garantiza que un paquete enviado de un host
eventualmente llegara a su destino</dd>
<dt>Entrega garantizada con demora limitada</dt><dd>Ademas de garantizar la entrega del
paquete, tambien se garantiza sera completada dentro de una cantidad de
tiempo.</dd>
<dt>entrega de paquete en order</dt><dd>se garantiza que los paquetes arrivan al
destino en el orden en que se enviaron.</dd>
<dt>Ancho de banda minimo garantizado</dt><dd>se emula el comportamiento de un enlace
de transmision de una velocidad garantizada. ?</dd>
<dt>Seguridad</dt><dd>encriptacion de los datagramas en el origen y decriptacion en el
destino.</dd>
</dl>

<p>
la capa de red de la internet provee un solo servicio, <code>best-effort service</code> en
donde no hay garantias de entrega, entrega eventual u orden de los paquetes
</p>
</div>
</div>
</div>

<div id="outline-container-orge970485" class="outline-3">
<h3 id="orge970485">Qué hay adentro de un router?</h3>
<div class="outline-text-3" id="text-orge970485">
<p>
imagen 4.4
</p>

<p>
A high-level view of a generic router architecture is shown in Figure 4.4.
</p>

<p>
Se pueden identificar cuatro componentes de un router:
</p>
<dl class="org-dl">
<dt>Puertos de entrada</dt><dd>se realiza una funcion de lookup en el puerto de
entrada. La tabla de forwarding es consultada para determinar el puerto de
salida. Los paquetes de control (por ej paquetes con informacion de protocolos
de ruteo) son enviados del puerto de entrada al procesador.</dd>
<dt>Switching fabric</dt><dd>Conecta a los puertos de entrada con los de salida.</dd>
<dt>Puertos de salida</dt><dd>almacena paquetes recibidos de la switching fabric y los
transmite al enlace saliente.</dd>
<dt>Procesador</dt><dd>realiza las funciones del control plane. en routers
tradicionales, ejecuta los protocolos de ruteo, administra las tablas de ruteo
y el estado de los enlaces, calcula la forwarding table. En routers SDN, el
procesador se comunica con el controlador remoto para recibir entradas de la
forwarding table.</dd>
</dl>
</div>

<div id="outline-container-orgadf4c56" class="outline-4">
<h4 id="orgadf4c56">Input Port Processing and Destination-Based Forwarding</h4>
<div class="outline-text-4" id="text-orgadf4c56">
<p>
imagen 4.5
</p>
<p>
La forwarding table es copiada del procesador a las line cards sobre un bus
aparte. Con una copia en cada puerto, las decisiones de forwarding pueden
hacerse de forma local sin invocar al procesador por cada paquete entrante y asi
evitar cuellos de botella.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 3:</span> forwarding table con 4 interfaces</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Destination Address Range</th>
<th scope="col" class="org-right">Link Interface</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">11001000 00010111 00010000 00000000</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">through</td>
<td class="org-right">&#xa0;</td>
</tr>

<tr>
<td class="org-left">11001000 00010111 00010111 11111111</td>
<td class="org-right">&#xa0;</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">11001000 00010111 00011000 00000000</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">through</td>
<td class="org-right">&#xa0;</td>
</tr>

<tr>
<td class="org-left">11001000 00010111 00011000 11111111</td>
<td class="org-right">&#xa0;</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">11001000 00010111 00011001 00000000</td>
<td class="org-right">2</td>
</tr>

<tr>
<td class="org-left">through</td>
<td class="org-right">&#xa0;</td>
</tr>

<tr>
<td class="org-left">11001000 00010111 00011111 11111111</td>
<td class="org-right">&#xa0;</td>
</tr>
</tbody>
<tbody>
<tr>
<td class="org-left">Otherwise</td>
<td class="org-right">3</td>
</tr>
</tbody>
</table>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">
<caption class="t-above"><span class="table-number">Table 4:</span> forwarding table utilizando prefijos</caption>

<colgroup>
<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Prefix</th>
<th scope="col" class="org-right">Link Interface</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">11001000 00010111 00010</td>
<td class="org-right">0</td>
</tr>

<tr>
<td class="org-left">11001000 00010111 00011000</td>
<td class="org-right">1</td>
</tr>

<tr>
<td class="org-left">11001000 00010111 00011</td>
<td class="org-right">2</td>
</tr>

<tr>
<td class="org-left">Otherwise</td>
<td class="org-right">3</td>
</tr>
</tbody>
</table>

<p>
Con el ultimo estile de la tabla de forwarding, el router encuentra un prefijo
de la direccion de destino entre las entradas de la table; si hay alguna
conincidencia, el router envia el paquete al enlace asociado.
</p>

<p>
si hay multiples conincidencias, el router utiliza el <code>longest prefix matching
rule</code>; esto es, encuentra el prefijo mas largo que coincide y envia el paquete a
la interfaz asociada.
</p>

<p>
el paquete es enviado al puerto de salida mediante la switching fabric.
</p>

<p>
otras acciones ocurren en la etapa de procesamiento en el puerto de entrada
</p>
<ol class="org-ol">
<li>procesamiento de capa fisica y capa de enlace</li>
<li>los campos de version de numero del paquete, checksum y time-to-live son verificados y los ultimos dos campos deben ser reescritos</li>
<li>se actualizan los contadores utilizados para la administracion de la red (ej la cantidad de datagramas IP recibidos)</li>
</ol>
</div>
</div>

<div id="outline-container-orge790095" class="outline-4">
<h4 id="orge790095">Switching</h4>
<div class="outline-text-4" id="text-orge790095">
<p>
los paquetes son enviados de los puertos de entrada a los de salida mediante la switching fabric
</p>

<dl class="org-dl">
<dt>Switching via memoria</dt><dd>los primeros routers eran computadoras tradicionales. El switcheo de paquetes lo realizaba la CPU. Los puertos de entrada y salida funcionaban como dispostivos de I\O tradicionales en un sistema operativo. Se indicaba el arribo de un paquete con una interrupcion, se copiaba el paquete en memoria, se procesaba y copiaba al puerto de salida correspondiente.</dd>
<dt>Switching via bus</dt><dd>un puerto de entrada tranfiere el paquete directamente al puerto de salida por sobre un bus compartido, sin intervencion del procesador. esto se logra haciendo que el puerto de entrada anteponga un encabezado al paquete indicando el puerto de salida y enviandolo por el bus. todos los puertos de salida reciben el paquete pero solo el que coincida con el encabezado lo guarda y le quita el encabezado. solo un paquete se puede transferir por el bus a la vez. el ancho de banda esta limitado por la velocidad del bus.</dd>
<dt>Switching via red de interconexiones</dt><dd>un crossbar switch es una red de interconexion que consiste de 2N buses que conectan N puertos de entrada a N puertos de salida. Cada bus vertical intersecta con uno horizontal formando una malla que puede ser controlada. Multiples paquetes pueden ser transferidos en simultaneo mientras no se dirijan al mismo puerto. Un crossbar switch es no bloqueante: un paquete enviado a un puerto de salida no sera bloqueado siempre y cuando no haya otro paquete siendo enviado al puerto de salida.</dd>
</dl>

<p>
imagen 4.6
</p>
</div>
</div>

<div id="outline-container-org617e5c4" class="outline-4">
<h4 id="org617e5c4">Procesamiento del puerto de salida</h4>
<div class="outline-text-4" id="text-org617e5c4">
<p>
se toman paquetes que fueron almacenados en la memoria del puerto y se transmiten por el enlace. esto incluye seleccion, desencolado de paquetes y la funciones de capa fisica y capa de enlace necesarias.
</p>

<p>
imagen 4.7
</p>
</div>
</div>

<div id="outline-container-org4f69e05" class="outline-4">
<h4 id="org4f69e05">Where Does Queuing Occur?</h4>
<div class="outline-text-4" id="text-org4f69e05">
<p>
Las colas de paquetes se pueden formar en los puertos de entrada y de salida. La
ubicacion y tamaño del encolado va a depender de la carga de trafico, la
velocidad de la switching fabric y la velocidad de line card.
</p>
</div>

<div id="outline-container-org78049a5" class="outline-5">
<h5 id="org78049a5">Encolado en la entrada</h5>
<div class="outline-text-5" id="text-org78049a5">
<p>
Si la velocidad de switcheo es mas rapida que la de transmision, el encolado sera despreciable.
</p>

<p>
<code>head-of-the-line (HOL) blocking</code> un paquete en la cola de entrada debe esperar
a ser transferido por la switching fabric porque el primer paquete de la misma
cola (head of line) esta bloqueado.
</p>

<p>
imagen 4.8
</p>

<p>
cuando no hay suficiente memoria para guardar el paquete entrante, se debe
decidir entre descartar el paquete (politica <code>drop-tail</code>) o remover paquetes
encolados. En algunos casos puede ser conveniente descartar (o marcar) un
paquete antes de que se llene la cola y luego enviar una señal al amisor
(algortimos <code>active queue management</code>).
</p>
</div>
</div>

<div id="outline-container-orga136f05" class="outline-5">
<h5 id="orga136f05">Encolado en la salida</h5>
<div class="outline-text-5" id="text-orga136f05">
<p>
Se pueden formar colas en los puertos de salida cuando la switching fabric es
mas rapida que la velocidad de transmision.
</p>

<p>
imagen 4.9
</p>

<p>
how much buffering is required? [RFC 3439] indica que la cantidad de buffering
(B) deberia ser igual a el (RTT) promedio multiplicado por la capacidad del
enlace (C).
</p>

<p>
\[ B = RTT * C\]
</p>
</div>
</div>
</div>

<div id="outline-container-org9b45c1e" class="outline-4">
<h4 id="org9b45c1e">Packet Scheduling</h4>
<div class="outline-text-4" id="text-org9b45c1e">
<p>
Decide el puerto de entrada a atender.
</p>
</div>

<div id="outline-container-org80dc03b" class="outline-5">
<h5 id="org80dc03b">FIFO</h5>
<div class="outline-text-5" id="text-org80dc03b">
<p>
Esta politica selecciona paquetes para su transmision segun su orden de llagada a la cola del puerto de salida
</p>

<p>
imagen 4.10
</p>

<p>
imagen 4.11
</p>
</div>
</div>

<div id="outline-container-orgbf84f20" class="outline-5">
<h5 id="orgbf84f20">Cola de prioridad</h5>
<div class="outline-text-5" id="text-orgbf84f20">
<p>
Los paquetes entrantes al puerto de salida son clasificados en clases de prioridades apenas llegan a la cola.
</p>

<p>
En la practica, se configura la cola para que los paquetes con informacion para la administracion de la red tengan prioridad sobre trafico de usuarios.
</p>

<p>
imagen 4.12
</p>

<p>
Cada clase tiene su propia cola. Se deben transmitir todos los paquetes de una cola de mayor prioridad antes continuar con las demas.
</p>

<p>
imagen 4.13
</p>
</div>
</div>

<div id="outline-container-org47d2be3" class="outline-5">
<h5 id="org47d2be3">Round Robin (RR) y Weighted Fair Queuing (WFQ)</h5>
<div class="outline-text-5" id="text-org47d2be3">
<p>
En Round Robin, los paquetes son ordenados en clases como en la politica de
cola-de-prioridad. La diferencia es que el servicio a las colas se alterna sin
prioridad.
</p>

<p>
La politica <code>work-conserving queuing</code> (encolado de conservacion de trabajo) nunca permite que el enlace permanezca desocupado cuando haya paquetes (de cualquier clase) encolados para ser transmitidos. Una politica
work-conserving round robin que busca paquetes de una clase pero no encuentra ninguno, pasa a la siguiente clase.
</p>

<p>
imagen 4.14
</p>

<p>
En la politica <code>weighted fair queuing (WFQ)</code>, los paquetes entrantes son clasificados y encolados en al area de espera de la clase apropiada. Como en RR, las clases se van sirviendo en orden; cuando no hay mas paquetes en una, se pasa a la siguiente. La diferencia entre WFQ y RR es que cada clase puede recibir una cantidad de servicio mas que otra clase. A cada clase \(i\) se le asigna un peso, \(w_{i}\). Durante cualquier intervalo de tiempo donde hay que enviar clase<sub>i</sub> paquetes, la clase i tiene garantizada recibir (\(w_{i}/\sum_{j}w_{j}\)) de servicio, donde la suma en el denominador es tomada sobre todas las clases que tambien tienen paquetes para transmitir. En el peor caso, la clase i tiene garantizada una fraccion del ancho de banda. Para un enlace con velocidad de transmision R, la clase i siempre tendra un throughput de al menos \(R*w_{i}/\sum_{j}w_{j}\).
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-orge2dcbcd" class="outline-3">
<h3 id="orge2dcbcd">IP</h3>
<div class="outline-text-3" id="text-orge2dcbcd">
<p>
Existen dos versiones de IP en  uso hoy en dia.
</p>
</div>

<div id="outline-container-org170b651" class="outline-4">
<h4 id="org170b651">IPv4 [RFC 791]</h4>
<div class="outline-text-4" id="text-org170b651">
<p>
imagen 4.16
</p>
</div>

<div id="outline-container-orga840b6a" class="outline-5">
<h5 id="orga840b6a">Formato de Datagrama</h5>
<div class="outline-text-5" id="text-orga840b6a">
<p>
campos clave del datagrama IPv4:
</p>

<dl class="org-dl">
<dt>Version number</dt><dd>4 bits la version del protocolo del datagrama. El router lo puede utilizar para determinar como interpretar el resto del datagrama.</dd>
<dt>Header length</dt><dd>un datagrama IPv4 puede contener una cantidad variable de opciones. Cuatro bits determinan donde en el datagrama comienza el payload. La mayoria de los datagramas no contiene opciones por lo que tienen un encabezado de 20 Bytes.</dd>
<dt>Type of service</dt><dd>Los bits de <code>TOS</code> fueron incluidos para permitir que
diferentes tipos de datagramas sean distinguibles unos de otros. Por ej,
datagramas de tiempo real y de no tiempo real. Es configurado por el
administrador de red del router.</dd>
<dt>Datagram length</dt><dd>tamaño total del datagrama (encabezado + datos) en bytes. Numero de 16 bits, por lo que el maximo teorico es 65535 bytes. Rara vez son mayores a 1500 bytes.</dd>
<dt>Identifier, flags, fragmentation offset</dt><dd>Tienen que ver con la framentacion IP.</dd>
<dt>Time-to-live</dt><dd>El campo <code>TTL</code> se incluye para asegurarse de que los datagramas no circulen para siempre por la red. Se decrementa en uno cada vez que es procesado por un router. Si llega a cero, es descartado.</dd>
<dt>Protocol</dt><dd>Tipicamente utilizado cuando el datagrama llega a su destino. El valor indica el protocolo especifico de capa-de-transporte (IANA Protocol Numbers 2016). El numero de protocolo es el pegamento que une a la capa-de-red con la capa-de-transporte. (como los puertos unen transpote con aplicacion)</dd>
<dt>Header checksum</dt><dd>Asiste al router para detectar errores de bits en el datagrama. Si el checksum no coincide con el computado, se descarta. Debe ser reescrito ya que cambian algunos campos del encabezado (TTL, opciones).</dd>
<dt>Source and destination IP addresses</dt><dd>Cuando el host origen crea un
datagrama, inserta su propia direccion IP y la del host destino en estos
campos.</dd>
<dt>Options</dt><dd>permiten extender el encabezado IP.</dd>
<dt>Data (payload)</dt><dd>contiene el segmento de capa de transporte para ser entregado.</dd>
</dl>

<p>
Asumiendo que no se utiliza el campo de opciones, el encabezado es de 20 Bytes.
</p>

<p>
Si el segmento que se transporta es de TCP, el datagrama tiene 40 bytes de encabezado (20 bytes de TCP)
</p>
</div>
</div>

<div id="outline-container-org4c0d258" class="outline-5">
<h5 id="org4c0d258">Fragmentacion de Datagramas</h5>
<div class="outline-text-5" id="text-org4c0d258">
<p>
La cantidad maxima de datos que un frame de capa de enlace puede llevar es llamado
<code>maximum transmission unit (MTU)</code>.
Debido a que cada datagrama es encapsulado dentro de un frame, el MTU limita el largo de un datagrama IP. La limitacion no es un problema en si, lo que lo es, es que cada enlace de la ruta entre emisor y receptor pueden utilizar distintos protocolos de capa de enlace, y que estos protocolos tengan diferentes MTUs.
</p>

<p>
Ante el problema de &ldquo;apretar&rdquo; un datagrama IP en el campo de payload de un frame de capa de enlace, la solucion es fragmentar el payload en el datagrama IP en dos o mas datagramas pequeños, encapsulando cada uno de estos en un frame separado; y enviar estos frames por el enlace de salida. A cada uno de estos datagramas se los llama <code>fragmentos</code>.
</p>

<p>
Los fragmentos deben ser reensamblados antes llegar a la <b>capa-de-transporte</b> en el destino. Los protocolos de capa-de-transporte esperan recibir un datagrama completo de la capa-de-red. El reensamblado ocurre en el destino.
</p>

<p>
Para permitir al host destino realizar la tarea de reensamblado, se hace uso de los campos de identificacion, flag, y <code>framentation offset</code> en el encabezado del datagrama IP.
</p>

<p>
Tipicamente, el host emisor incrementa el numero de identificacion para datagrama que envia. Cuando un router debe fragmentar un datagrama, cada framento es marcado con las ip de origen y destino y el numero de indentificacion del datagrama original.
</p>

<p>
Para que el host destino se asegure de haber recibido todos los fragmentos, el ultimo fragmento es marcado con un bit en 0, mientras que el resto de los fragmentos tienen un 1. El campo de <code>offset</code> tambien es utilizado para verificar si falta algun fragmento.
</p>

<p>
imagen 4.17
</p>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-org68064e3" class="outline-2">
<h2 id="org68064e3">Data Plane - Direccionamiento e IPv6</h2>
<div class="outline-text-2" id="text-org68064e3">
</div>
<div id="outline-container-org5f6e592" class="outline-3">
<h3 id="org5f6e592">Direccionamiento IPv4</h3>
<div class="outline-text-3" id="text-org5f6e592">
<p>
A host typically has only a single link into the network; when IP in the host
wants to send a datagram, it does so over this link. The boundary between the
host and the physical link is called an <code>interface</code>.
</p>

<p>
Now consider a router and its interfaces. Because a router’s job is to receive a
datagram on one link and forward the datagram on some other link, a router
necessarily has two or more links to which it is connected. The boundary between
the router and any one of its links is also called an interface. A router thus
has multiple interfaces, one for each of its links.
</p>

<p>
Because every host and router is capable of sending and receiving IP datagrams,
IP requires each host and router interface to have its own IP address. Thus, an
IP address is technically associated with an interface, rather than with the
host or router containing that interface.
</p>

<p>
Each IP address is 32 bits long (4 bytes), and there are thus a total of
\(2^{32}\) (or approximately 4 billion) possible IP addresses.
</p>

<p>
Each interface on every host and router in the global Internet must have an IP
address that is globally unique. These addresses cannot be chosen in a
willy-nilly manner, however. A portion of an interface’s IP address will be
determined by the subnet to which it is connected.
</p>

<p>
&#x2014; A group of interfaces interconnected to each other by a network that
contains no routers forms a <code>subnet</code>. [RFC 950] Interfaces in the same subnet
share a number of bits in their IP address. This is called <code>subnet mask</code>, which
are the (N) leftmost bits of the address. It is &#x2014;
</p>

<p>
imagen 4.18
</p>


<p>
imagen 4.19
</p>

<p>
For a general interconnected system of routers and hosts, we can use the
following recipe to define the subnets in the system:
</p>

<blockquote>
<p>
To determine the subnets, detach each interface from its host or router,
creating islands of isolated networks, with interfaces terminating the end
points of the isolated networks. Each of these isolated networks is called a
subnet.
</p>
</blockquote>

<p>
imagen 4.20
</p>

<p>
The Internet’s address assignment strategy is known as <code>Classless Interdomain
Routing (CIDR)</code> [RFC 4632]. CIDR generalizes the notion of subnet addressing. As
with subnet addressing, the 32-bit IP address is divided into two parts and
again has the dotted-decimal form a.b.c.d/x, where x indicates the number of
bits in the first part of the address.
</p>

<p>
The x most significant bits of an address of the form a.b.c.d/x constitute the
network portion of the IP address, and are often referred to as the <code>prefix</code> (or
network prefix) of the address. An organization is typically assigned a block of
contiguous addresses, that is, a range of addresses with a common prefix.
</p>

<p>
The remaining \(32-x\) bits of an address can be thought of as distinguishing
among the devices within the organization, all of which have the same network
prefix. These are the bits that will be considered when forwarding packets at
routers within the organization. These lower-order bits may (or may not) have an
additional subnetting structure, such as that discussed above.
</p>

<p>
Before CIDR was adopted, the network portions of an IP address were constrained
to be 8, 16, or 24 bits in length, an addressing scheme known as <code>classful
addressing</code>, since subnets with 8-, 16-, and 24-bit subnet addresses were known
as class A, B, and C networks, respectively.
</p>


<p>
The IP broadcast address 255.255.255.255. When a host sends a datagram with
destination address 255.255.255.255, the message is delivered to all hosts on
the same subnet. Routers optionally forward the message into neighboring subnets
as well (although they usually don’t).
</p>
</div>

<div id="outline-container-org933ca9e" class="outline-4">
<h4 id="org933ca9e">Obtaining a Block of Addresses</h4>
<div class="outline-text-4" id="text-org933ca9e">
<p>
how an organization gets a block of addresses for its devices?
</p>

<p>
how a device (such as a host) is assigned an address from within the
organization’s block of addresses?
</p>

<p>
In order to obtain a block of IP addresses for use within an organization’s
subnet, a network administrator might first contact its ISP, which would provide
addresses from a larger block of addresses that had already been allocated to
the ISP.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">ISP’s block</td>
<td class="org-left">200.23.16.0/20</td>
<td class="org-left">=11001000 00010111 0001=0000 00000000</td>
</tr>

<tr>
<td class="org-left">Organization 0</td>
<td class="org-left">200.23.16.0/23</td>
<td class="org-left">=11001000 00010111 0001000=0 00000000</td>
</tr>

<tr>
<td class="org-left">Organization 1</td>
<td class="org-left">200.23.18.0/23</td>
<td class="org-left">=11001000 00010111 0001001=0 00000000</td>
</tr>

<tr>
<td class="org-left">Organization 2</td>
<td class="org-left">200.23.20.0/23</td>
<td class="org-left">=11001000 00010111 0001010=0 00000000</td>
</tr>

<tr>
<td class="org-left">&#x2026;</td>
<td class="org-left">&#x2026;</td>
<td class="org-left">&#x2026;</td>
</tr>

<tr>
<td class="org-left">Organization 7</td>
<td class="org-left">200.23.30.0/23</td>
<td class="org-left">=11001000 00010111 0001111=0 00000000</td>
</tr>
</tbody>
</table>

<p>
Is there a global authority that has ultimate responsibility for managing the IP
address space and allocating address blocks to ISPs and other organizations?
</p>

<p>
Indeed there is! IP addresses are managed under the authority of the Internet
Corporation for Assigned Names and Numbers (ICANN) [ICANN 2016], based on
guidelines set forth in [RFC 7020].
</p>
</div>
</div>

<div id="outline-container-org7682b3b" class="outline-4">
<h4 id="org7682b3b">Obtaining a Host Address: The Dynamic Host Configuration Protocol</h4>
<div class="outline-text-4" id="text-org7682b3b">
<p>
Once an organization has obtained a block of addresses, it can assign individual
IP addresses to the host and router interfaces in its organization. A system
administrator will typically manually configure the IP addresses into the
router. Host addresses can also be configured manually, but typically this is
done using the <code>Dynamic Host Configuration Protocol (DHCP)</code> [RFC 2131]. DHCP
allows a host to obtain (be allocated) an IP address automatically.
</p>

<p>
A network administrator can configure DHCP so that a given host receives the
same IP address each time it connects to the network, or a host may be assigned
a <code>temporary IP address</code> that will be different each time the host connects to
the network. In addition to host IP address assignment, DHCP also allows a host
to learn additional information, such as its subnet mask, the address of its
first-hop router (often called the default gateway), and the address of its
local DNS server.
</p>

<p>
DHCP is a client-server protocol. A client is typically a newly arriving host
wanting to obtain network configuration information, including an IP address for
itself. In the simplest case, each subnet will have a DHCP server. If no server
is present on the subnet, a DHCP relay agent (typically a router) that knows the
address of a DHCP server for that network is needed.
</p>

<p>
Figure 4.23 shows a DHCP server attached to subnet 223.1.2/24, with the router
serving as the relay agent for arriving clients attached to subnets 223.1.1/24
and 223.1.3/24.
</p>

<p>
imagen 4.23
</p>

<p>
For a newly arriving host, the DHCP protocol is a four-step process for the
network setting shown in Figure 4.23:
</p>

<ol class="org-ol">
<li>DHCP server discovery. The first task of a newly arriving host is to find a
DHCP server with which to interact. This is done using a DHCP discover
message, which a client sends within a UDP packet to port 67. The UDP packet
is encapsulated in an IP datagram. the IP datagram contains a DHCP discover
message along with the broadcast destination IP address of 255.255.255.255
and a “this host” source IP address of 0.0.0.0. The DHCP client passes the IP
datagram to the link layer, which then broadcasts this frame to all nodes
attached to the subnet.</li>
<li>DHCP server offer(s). A DHCP server receiving a DHCP discover message
responds to the client with a DHCP offer message that is broadcast to all
nodes on the subnet, again using the IP broadcast address of 255.255.255.255.
Since several DHCP servers can be present on the subnet, the client may find
itself in the enviable position of being able to choose from among several
offers. Each server offer message contains the transaction ID of the received
discover message, the proposed IP address for the client, the network mask,
and an IP address lease time—the amount of time for which the IP address will
be valid. It is common for the server to set the lease time to several hours
or days.</li>
<li>DHCP request. The newly arriving client will choose from among one or more
server offers and respond to its selected offer with a DHCP request message,
echoing back the configuration parameters.</li>
<li>DHCP ACK. The server responds to the DHCP request message with a DHCP ACK
message, confirming the requested parameters.</li>
</ol>

<p>
Once the client receives the DHCP ACK, the interaction is complete and the
client can use the DHCP-allocated IP address for the lease duration. Since a
client may want to use its address beyond the lease’s expiration, DHCP also
provides a mechanism that allows a client to renew its lease on an IP address.
</p>

<p>
imagen 4.24
</p>

<p>
In this figure, yiaddr (as in “your Internet address”) indicates the address
being allocated to the newly arriving client
</p>
</div>
</div>

<div id="outline-container-org1e509db" class="outline-4">
<h4 id="org1e509db">Network Address Translation (NAT)</h4>
<div class="outline-text-4" id="text-org1e509db">
<p>
[RFC 2663; RFC 3022]
</p>

<p>
Figure 4.25 shows the operation of a NAT-enabled router. The NAT-enabled router,
residing in the home, has an interface that is part of the home network on the
right of Figure 4.25. Addressing within the home network is exactly as we have
seen above—all four interfaces in the home network have the same subnet address
of 10.0.0/24. The address space 10.0.0.0/8 is one of three portions of the IP
address space that is reserved in <code>[RFC 1918]</code> for a <code>private network</code> or a
<code>realm with private addresses</code>, such as the home network in Figure 4.25. A realm
with private addresses refers to a network whose addresses only have meaning to
devices within that network. To see why this is important, consider the fact
that there are hundreds of thousands of home networks, many using the same
address space, 10.0.0.0/24. Devices within a given home network can send packets
to each other using 10.0.0.0/24 addressing. However, packets forwarded beyond
the home network into the larger global Internet clearly cannot use these
addresses (as either a source or a destination address) because there are
hundreds of thousands of networks using this block of addresses. That is, the
10.0.0.0/24 addresses can only have meaning within the given home network. But
if private addresses only have meaning within a given network, how is addressing
handled when packets are sent to or received from the global Internet, where
addresses are necessarily unique?
</p>

<p>
The NAT-enabled router does not look like a router to the outside world. Instead
the NAT router behaves to the outside world as a single device with a single IP
address. In Figure 4.25, all traffic leaving the home router for the larger
Internet has a source IP address of 138.76.29.7, and all traffic entering the
home router must have a destination address of 138.76.29.7. In essence, the
NAT-enabled router is hiding the details of the home network from the outside
world.
</p>

<p>
A <code>NAT translation table</code> is used at the NAT-enabled router to know which
internal host it should forward a datagram. The table includes port numbers and
IP addresses.
</p>
</div>
</div>
</div>

<div id="outline-container-org41f125b" class="outline-3">
<h3 id="org41f125b">IPv6</h3>
<div class="outline-text-3" id="text-org41f125b">
</div>
<div id="outline-container-orgf19da7e" class="outline-4">
<h4 id="orgf19da7e">Datagram Format</h4>
<div class="outline-text-4" id="text-orgf19da7e">
<p>
imagen 4.26
</p>
</div>

<div id="outline-container-org602f89d" class="outline-5">
<h5 id="org602f89d">Changes introduced in the datagram format</h5>
<div class="outline-text-5" id="text-org602f89d">
<dl class="org-dl">
<dt>Expanded addressing capabilities</dt><dd>IPv6 increases the size of the IP address
from 32 to 128 bits. This ensures that the world won’t run out of IP
addresses. In addition to unicast and multicast addresses, IPv6 has introduced
a new type of address, called an anycast address, that allows a datagram to be
delivered to any one of a group of hosts. (This feature could be used, for
example, to send an HTTP GET to the nearest of a number of mirror sites that
contain a given document.)</dd>
<dt>A streamlined 40-byte header</dt><dd>As discussed below, a number of IPv4 fields
have been dropped or made optional. The resulting 40-byte fixed-length header
allows for faster processing of the IP datagram by a router. A new encoding of
options allows for more flexible options processing.</dd>
<dt>Flow labeling</dt><dd>IPv6 has an elusive definition of a flow. RFC 2460 states
that this allows “labeling of packets belonging to particular flows for which
the sender requests special handling, such as a non-default quality of service
or real-time service.” For example, audio and video transmission might likely
be treated as a flow. On the other hand, the more traditional applications,
such as file transfer and e-mail, might not be treated as flows. It is
possible that the traffic carried by a high-priority user (for example,
someone paying for better service for their traffic) might also be treated as
a flow. What is clear, however, is that the designers of IPv6 foresaw the
eventual need to be able to differentiate among the flows, even if the exact
meaning of a flow had yet to be determined.</dd>
</dl>
</div>
</div>

<div id="outline-container-org9976df0" class="outline-5">
<h5 id="org9976df0">Fields</h5>
<div class="outline-text-5" id="text-org9976df0">
<ul class="org-ul">
<li>Version This 4-bit field identifies the IP version number</li>
<li>Traffic class The 8-bit traffic class field, like the TOS field in IPv4, can
be used to give priority to certain datagrams within a flow, or it can be used
to give priority to datagrams from certain applications over datagrams from
other applications (for example, VOIP over SMTP e-mail).</li>
<li>Flow label this 20-bit field is used to identify a flow of datagrams.</li>
<li>Payload length This 16-bit value is treated as an unsigned integer giving the
number of bytes in the IPv6 datagram following the fixed-length, 40-byte
datagram header.</li>
<li>Next header This field identifies the protocol to which the payload of the
datagram will be delivered (for example, to TCP or UDP). The field uses the
same values as the protocol field in the IPv4 header.</li>
<li>Hop limit The contents of this field are decremented by one by each router
that forwards the datagram. If the top limit count reaches zero, the datagram
is discarded.</li>
<li>Source and destination addresses The various formats of the IPv6 128-bit
address are described in RFC 4291.</li>
<li>Data This is the payload portion of the IPv6 datagram. When the datagram
reaches its destination, the payload will be removed from the IP datagram and
passed on to the protocol specified in the next header field.</li>
</ul>
</div>
</div>

<div id="outline-container-org93048b6" class="outline-5">
<h5 id="org93048b6">Some fields are removed from IPv4 to IPv6</h5>
<div class="outline-text-5" id="text-org93048b6">
<ul class="org-ul">
<li>Fragmentation/reassembly IPv6 does not allow for fragmentation and reassembly
at intermediate routers; these operations can be performed only by the source
and destination. If an IPv6 datagram received by a router is too large to be
forwarded over the outgoing link, the router simply drops the datagram and
sends a “Packet Too Big” ICMP error message back to the sender. The sender can
then resend the data, using a smaller IP datagram size. Fragmentation and
reassembly is a time-consuming operation; removing this functionality from the
routers and placing it squarely in the end systems considerably speeds up IP
forwarding within the network.</li>
<li>Header checksum Because the transport-layer (for example, TCP and UDP) and
link-layer (for example, Ethernet) protocols in the Internet layers perform
checksumming, the designers of IP probably felt that this functionality was
sufficiently redundant in the network layer that it could be removed. Once
again, fast processing of IP packets was a central concern. Recall that the
header checksum needed to be recomputed at every router. As with fragmentation
and reassembly, this too was a costly operation in IPv4.</li>
<li>Options An options field is no longer a part of the standard IP header.
However, it has not gone away. Instead, the options field is one of the
possible next headers pointed to from within the IPv6 header. That is, just as
TCP or UDP protocol headers can be the next header within an IP packet, so too
can an options field. The removal of the options field results in a
fixed-length, 40-byte IP header.</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orge05ebba" class="outline-4">
<h4 id="orge05ebba">Transitioning from IPv4 to IPv6</h4>
<div class="outline-text-4" id="text-orge05ebba">
<p>
imagen 4.27
</p>

<p>
The approach to IPv4-to-IPv6 transition that has been most widely adopted in
practice involves <code>tunneling [RFC 4213]</code>.
</p>

<p>
Suppose two IPv6 nodes (in this example, B and E in Figure 4.27) want to
interoperate using IPv6 datagrams but are connected to each other by intervening
IPv4 routers.
</p>

<p>
We refer to the intervening set of IPv4 routers between two IPv6 routers as a
<code>tunnel</code>, as illustrated in Figure 4.27. With tunneling, the IPv6 node on the
sending side of the tunnel (in this example, B) takes the entire IPv6 datagram
and puts it in the data (payload) field of an IPv4 datagram. This IPv4 datagram
is then addressed to the IPv6 node on the receiving side of the tunnel (in this
example, E) and sent to the first node in the tunnel (in this example, C).
</p>

<p>
The intervening IPv4 routers in the tunnel route this IPv4 datagram among
themselves, just as they would any other datagram, blissfully unaware that the
IPv4 datagram itself contains a complete IPv6 datagram. The IPv6 node on the
receiving side of the tunnel eventually receives the IPv4 datagram (it is the
destination of the IPv4 datagram!), determines that the IPv4 datagram contains
an IPv6 datagram (by observing that the protocol number field in the IPv4
datagram is 41 <code>[RFC 4213]</code>, indicating that the IPv4 payload is a IPv6
datagram), extracts the IPv6 datagram, and then routes the IPv6 datagram exactly
as it would if it had received the IPv6 datagram from a directly connected IPv6
neighbor.
</p>
</div>
</div>
</div>

<div id="outline-container-orgcc4ff59" class="outline-3">
<h3 id="orgcc4ff59">Generalized Forwarding and SDN</h3>
<div class="outline-text-3" id="text-orgcc4ff59">
<p>
Recall from earlier that destination-based forwarding as the two steps of
looking up a destination IP address (“match”), then sending the packet into the
switching fabric to the specified output port (“action”).
</p>

<p>
now consider a significantly more general “match-plus-action” paradigm, where
the “match” can be made over multiple header fields associated with different
protocols at different layers in the protocol stack. The “action” can include
forwarding the packet to one or more output ports (as in destination-based
forwarding), load balancing packets across multiple outgoing interfaces that
lead to a service (as in load balancing), rewriting header values (as in NAT),
purposefully blocking/dropping a packet (as in a firewall), sending a packet to
a special server for further processing and action (as in DPI), and more.
</p>


<p>
In generalized forwarding, a match-plus-action table generalizes the notion of
the destination-based forwarding table. Because forwarding decisions may be made
using network-layer and/or link-layer source and destination addresses, the
forwarding devices are more accurately described as “packet switches” rather
than layer 3 “routers” or layer 2 to these devices as packet switches.
</p>

<p>
imagen 4.28
</p>

<p>
Figure 4.28 Generalized forwarding: Each packet switch contains a
match-plus-action table that is computed and distributed by a remote controller
</p>

<p>
Figure 4.28 shows a match-plus-action table in each packet switch, with the
table being computed, installed, and updated by a remote controller. We note
that while it is possible for the control components at the individual packet
switch to interact with each other, in practice generalized match-plus-action
capabilities are implemented via a remote controller that computes, installs,
and updates these tables.
</p>

<p>
OpenFlow a highly visible and successful standard that has pioneered the notion
of the match-plus-action forwarding abstraction and controllers, as well as the
SDN revolution more generally.
</p>

<p>
Each entry in the match-plus-action forwarding table, known as a <code>flow table</code> in
OpenFlow, includes:
</p>
<ul class="org-ul">
<li>A set of header field values to which an incoming packet will be matched. As
in the case of destination-based forwarding, hardware-based matching is most
rapidly performed in TCAM memory, with more than a million destination address
entries being possible. A packet that matches no flow table entry can be
dropped or sent to the remote controller for more processing. In practice, a
flow table may be implemented by multiple flow tables for performance or cost
reasons.</li>
<li>A set of counters that are updated as packets are matched to flow table
entries. These counters might include the number of packets that have been
matched by that table entry, and the time since the table entry was last
updated.</li>
<li>A set of actions to be taken when a packet matches a flow table entry. These
actions might be to forward the packet to a given output port, to drop the
packet, makes copies of the packet and sent them to multiple output ports,
and/or to rewrite selected header fields.</li>
</ul>
</div>

<div id="outline-container-orgd69c6a9" class="outline-4">
<h4 id="orgd69c6a9">Match</h4>
<div class="outline-text-4" id="text-orgd69c6a9">
<p>
imagen 4.29
</p>

<p>
Recall that a link-layer (layer 2) frame arriving to a packet switch will
contain a network-layer (layer 3) datagram as its payload, which in turn will
typically contain a transport-layer (layer 4) segment.
</p>

<p>
OpenFlow’s match abstraction allows for a match to be made on selected fields
from three layers of protocol headers (thus rather brazenly defying the layering
principle). Since we’ve not yet covered the link layer, suffice it to say that
the source and destination MAC addresses are the link-layer addresses associated
with the frame’s sending and receiving interfaces; by forwarding on the basis of
Ethernet addresses rather than IP addresses, we can see that an OpenFlow-enabled
device can equally perform as a router (layer-3 device) forwarding datagrams as
well as a switch (layer-2 device) forwarding frames. The Ethernet type field
corresponds to the upper layer protocol (e.g., IP) to which the frame’s payload
will be de-multiplexed, and the VLAN fields are concerned with so-called virtual
LANs.
</p>

<p>
Flow table entries may also have wildcards. For example, an IP address of
128.119.*.* in a flow table will match the corresponding address field of any
datagram that has 128.119 as the first 16 bits of its address.
</p>

<p>
Each flow table entry also has an associated priority. If a packet matches
multiple flow table entries, the selected match and corresponding action will be
that of the highest priority entry with which the packet matches.
</p>
</div>
</div>

<div id="outline-container-org2aed805" class="outline-4">
<h4 id="org2aed805">Action</h4>
<div class="outline-text-4" id="text-org2aed805">
<p>
each flow table entry has a list of zero or more actions that determine the
processing that is to be applied to a packet that matches a flow table entry. If
there are multiple actions, they are performed in the order specified in the
list.
</p>

<p>
Among the most important possible actions are:
</p>

<dl class="org-dl">
<dt>Forwarding</dt><dd><p>
An incoming packet may be
</p>
<ul class="org-ul">
<li>forwarded to a particular physical output port,</li>
<li>broadcast over all ports (except the port on which it arrived) or</li>
<li>multicast over a selected set of ports.</li>
</ul>
<p>
The packet may be encapsulated and sent to the remote controller for this
device. That controller then may (or may not) take some action on that packet,
including installing new flow table entries, and may return the packet to the
device for forwarding under the updated set of flow table rules.
</p></dd>
<dt>Dropping</dt><dd>A flow table entry with no action indicates that a matched packet
should be dropped.</dd>
<dt>Modify-field</dt><dd>The values in ten packet header fields (all layer 2, 3, and 4
fields except the IP Protocol field) may be re-written before the packet is
forwarded to the chosen output port.</dd>
</dl>
</div>
</div>

<div id="outline-container-org5639910" class="outline-4">
<h4 id="org5639910">OpenFlow Examples of Match-plus-action in Action</h4>
<div class="outline-text-4" id="text-org5639910">
<p>
imagen 4.30
</p>

<p>
The network has 6 hosts (h1 to h6) and three packet switches (s1 to s3), each
with four local interfaces (1 through 4). We’ll consider a number of
network-wide behaviors that we’d like to implement, and the flow table entreies
in s1, s2 and s3 needed to implement this behavior.
</p>
</div>

<div id="outline-container-org7312096" class="outline-5">
<h5 id="org7312096">A First Example: Simple Forwarding</h5>
<div class="outline-text-5" id="text-org7312096">
<p>
As a very simple example, suppose that the desired forwarding behavior is that
packets from h5 or h6 destined to h3 or h4 are to be forwarded from s3 to s1,
and then from s1 to s2 (thus completely avoiding the use of the link between s3
and s2). The flow table entry in s1 would be:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">s1 Flow Table (Example 1)</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Match</td>
<td class="org-left">Action</td>
</tr>

<tr>
<td class="org-left">Ingress Port = 1 ; IP Src = 10.3.*.* ; IP Dst = 10.2.*.*</td>
<td class="org-left">Forward(4)</td>
</tr>

<tr>
<td class="org-left">&#x2026;</td>
<td class="org-left">&#x2026;</td>
</tr>
</tbody>
</table>

<p>
Of course, we’ll also need a flow table entry in s3 so that datagrams sent from
h5 or h6 are forwarded to s1 over outgoing interface 3:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">s3 Flow Table (Example 1)</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Match</td>
<td class="org-left">Action</td>
</tr>

<tr>
<td class="org-left">IP Src = 10.3.*.* ; IP Dst = 10.2.*.*</td>
<td class="org-left">Forward(3)</td>
</tr>

<tr>
<td class="org-left">&#x2026;</td>
<td class="org-left">&#x2026;</td>
</tr>
</tbody>
</table>

<p>
Lastly, we’ll also need a flow table entry in s2 to complete this first example,
so that datagrams arriving from s1 are forwarded to their destination, either
host h3 or h4:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">s2 Flow Table (Example 1)</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Match</td>
<td class="org-left">Action</td>
</tr>

<tr>
<td class="org-left">Ingress port = 2 ; IP Dst = 10.2.0.3</td>
<td class="org-left">Forward(3)</td>
</tr>

<tr>
<td class="org-left">Ingress port = 2 ; IP Dst = 10.2.0.4</td>
<td class="org-left">Forward(4)</td>
</tr>

<tr>
<td class="org-left">&#x2026;</td>
<td class="org-left">&#x2026;</td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org40b8c42" class="outline-5">
<h5 id="org40b8c42">A Second Example: Load Balancing</h5>
<div class="outline-text-5" id="text-org40b8c42">
<p>
As a second example, let’s consider a load-balancing scenario, where datagrams
from h3 destined to 10.1.*.* are to be forwarded over the direct link between s2
and s1, while datagrams from h4 destined to 10.1.*.* are to be forwarded over
the link between s2 and s3 (and then from s3 to s1). Note that this behavior
couldn’t be achieved with IP’s destination-based forwarding. In this case, the
flow table in s2 would be:
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">s2 Flow Table (Example 2)</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Match</td>
<td class="org-left">Action</td>
</tr>

<tr>
<td class="org-left">Ingress port = 3; IP Dst = 10.1.*.*</td>
<td class="org-left">Forward(2)</td>
</tr>

<tr>
<td class="org-left">Ingress port = 4; IP Dst = 10.1.*.*</td>
<td class="org-left">Forward(1)</td>
</tr>

<tr>
<td class="org-left">&#x2026;</td>
<td class="org-left">&#x2026;</td>
</tr>
</tbody>
</table>

<p>
Flow table entries are also needed at s1 to forward the datagrams received from
s2 to either h1 or h2; and flow table entries are needed at s3 to forward
datagrams received on interface 4 from s2 over interface 3 towards s1.
</p>

<p>
See if you can figure out these flow table entries at s1 and s3.
</p>
</div>
</div>

<div id="outline-container-orgeb1f666" class="outline-5">
<h5 id="orgeb1f666">A Third Example: Firewalling</h5>
<div class="outline-text-5" id="text-orgeb1f666">
<p>
As a third example, let’s consider a firewall scenario in which s2 wants only to
receive (on any of its interfaces) traffic sent from hosts attached to s3.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">s2 Flow Table (Example 3)</td>
<td class="org-left">&#xa0;</td>
</tr>

<tr>
<td class="org-left">Match</td>
<td class="org-left">Action</td>
</tr>

<tr>
<td class="org-left">IP Src = 10.3.*.* IP Dst = 10.2.0.3</td>
<td class="org-left">Forward(3)</td>
</tr>

<tr>
<td class="org-left">IP Src = 10.3.*.* IP Dst = 10.2.0.4</td>
<td class="org-left">Forward(4)</td>
</tr>

<tr>
<td class="org-left">&#x2026;</td>
<td class="org-left">&#x2026;</td>
</tr>
</tbody>
</table>

<p>
If there were no other entries in s2’s flow table, then only traffic from
10.3.*.* would be forwarded to the hosts attached to s2.
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org9d5c0e1" class="outline-2">
<h2 id="org9d5c0e1">Control Plane</h2>
<div class="outline-text-2" id="text-org9d5c0e1">
</div>
<div id="outline-container-org51b9400" class="outline-3">
<h3 id="org51b9400">Protocolos de ruteo</h3>
<div class="outline-text-3" id="text-org51b9400">
</div>
<div id="outline-container-org02872dc" class="outline-4">
<h4 id="org02872dc">Algoritmos de ruteo</h4>
<div class="outline-text-4" id="text-org02872dc">
<p>
determine good paths (equivalently,routes), from senders to receivers, through
the network of routers. Typically, a “good” path is one that hasthe least cost.
</p>

<p>
whether the network control planeadopts a per-router control approach or a
logically centralized approach, there must always be a well-defined sequence of
routers that a packet will cross in traveling from sending to receiving host.
</p>


<p>
A graph is used to formulate routing problem
</p>

<p>
Consideraremos a la red como un grafo G=(N,E)
</p>
<ul class="org-ul">
<li>N: Nodos del grafo (routers)</li>
<li>E: aristas que unen los nodos del grafo (links, subredes)</li>
</ul>

<p>
El grafo G, puede ser considerado como un grafo pesado
</p>

<p>
Qué significan los pesos de las aristas?
</p>
<ul class="org-ul">
<li>La distancia de los enlaces (mayor latencia)</li>
<li>La capacidad</li>
<li>El costo de la latencia</li>
</ul>


<p>
Actualización de la notación
</p>
<ul class="org-ul">
<li>c(x,y): costo de la arista entre el par de nodos x e y</li>
<li>Si no hay arista entre x e y —&gt; c(x,y): infinito</li>
</ul>


<p>
Vocabulario
</p>
<ul class="org-ul">
<li>Si c(x,y) &lt; infinito: x e y son vecinos</li>
</ul>


<p>
G=(N,E) grafo de la red
</p>

<p>
c(x,y) &forall; x,y &isin; N costo entre dos nodos cualesquiera
</p>

<p>
\{x<sub>1,x</sub><sub>2,&hellip;,x</sub><sub>k</sub>\} camino hasta el nodo x<sub>k</sub>
</p>

<p>
\displaystyle<sup>k</sup> c(x<sub>i,x</sub><sub>j</sub>) costo de un camino
</p>


<p>
Objetivos
</p>
<ul class="org-ul">
<li>Hallar el camino de costo mínimo entre dos pares de nodos</li>

<li>También llamado el camino más corto (shortest path)</li>
</ul>


<ul class="org-ul">
<li>Únicamente basados en aspectos topológicos</li>
<li>No   determinan   los   caminos   en   función   de   parámetros dinámicos (latencia, congestión)</li>
</ul>

<p>
¿Por qué?
</p>
<ul class="org-ul">
<li>Problemas de estabilidad, afectando todo internet</li>
</ul>
</div>

<div id="outline-container-org77167ec" class="outline-5">
<h5 id="org77167ec">Centralizado (Link-state)</h5>
<div class="outline-text-5" id="text-org77167ec">
<ul class="org-ul">
<li>El algoritmo cuenta con toda la información de la red &gt;(nodos, enlaces,
status)</li>
<li>No importa como consiguió esta información</li>
<li><p>
IMPORTANTE: Independiente si el CP es lógicamente centralizado o opera en
cada router
</p>

<p>
A centralized routing algorithm computes the least-cost path between a source
and destination using complete, global knowledge about the network. That is,
the algorithm takes the connectivity between all nodes and all link costs as
inputs. This then requires that the algorithm somehow obtain this information
before actually performing the calculation. The calculation itself can be run
at one site or could be replicated in the routing component of each and every
router. The key distinguishing feature here, however, is that the algorithm has
complete information about connectivity and link costs. Algorithms with global
state information are often referred to as link-state (LS) algorithms, since
the algorithm must be aware of the cost of each link in the network.
</p>

<p>
¿Cómo se lleva a cabo?
</p>
<ul class="org-ul">
<li>Cada router hace broadcast el estado de sus enlaces</li>
<li>Cada router recibe el broadcast de todos los otros routers</li>
</ul>

<p>
Cada nodo (router) corre el algoritmo de ruteo
</p></li>
</ul>

<p>
this is accomplished by having each node broadcast link-state packets to all
other nodes in the network, with each link-state packet containing the
identities and costs of its attached links.
</p>


<p>
shortest path algorithm <sup><a id="fnr.1" class="footref" href="#fn.1">1</a></sup>
</p>
</div>

<div id="outline-container-orgb3fdf30" class="outline-6">
<h6 id="orgb3fdf30">Oscilaciones</h6>
<div class="outline-text-6" id="text-orgb3fdf30">
<p>
imagen 5.5
</p>

<p>
Before completing our discussion of the LS algorithm, let us consider a
pathology that can arise. Figure 5.5 shows a simple network topology where link
costs are equal to the load carried on the link, for example, reflecting the
delay that would be experienced. In this example, link costs are not symmetric;
that is, c(u, v) equals c(v, u) only if the load carried on both directions on
the link (u, v) is the same. In this example, node z originates a unit of
traffic destined for w, node x also originates a unit of traffic destined for w,
and node y injects an amount of traffic equal to e, also destined for w. The
initial routing is shown in Figure 5.5(a) with the link costs corresponding to
the amount of traffic carried.
</p>

<p>
When the LS algorithm is next run, node y determines (based on the link costs
shown in Figure 5.5(a)) that the clockwise path to w has a cost of 1, while the
counterclockwise path to w (which it had been using) has a cost of 1+e. Hence
y’s least-cost path to w is now clockwise. Similarly, x determines that its new
least-cost path to w is also clockwise, resulting in costs shown in Figure
5.5(b). When the LS algorithm is run next, nodes x, y, and z all detect a
zero-cost path to w in the counterclockwise direction, and all route their
traffic to the counterclockwise routes. The next time the LS algorithm is run,
x, y, and z all then route their traffic to the clockwise routes.
</p>

<p>
Another solution is to ensure that not all routers run the LS algorithm at the
same time.  This seems a more reasonable solution, since we would hope that even
if routers ran the LS algorithm with the same periodicity, the execution
instance of the algorithm would not be the same at each node.
</p>

<p>
Escenario
</p>
<ul class="org-ul">
<li>Topología rombo</li>
<li>c(x,y) := f(tráfico)</li>
<li>Trafico asimétrico (upstream != downstream)</li>
<li>Ramas con diferentes valores</li>
</ul>

<p>
Desync
Forzar que los routers no efectúen Dijkstra en simultáneo
</p>
<ul class="org-ul">
<li>Distintos routers van a tener diferentes caminos mínimos instante a instante</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgf5ea786" class="outline-5">
<h5 id="orgf5ea786">Distribuido (Distance-vector)</h5>
<div class="outline-text-5" id="text-orgf5ea786">
<ul class="org-ul">
<li>Calculado de manera iterativa y distribuida por los routers</li>
<li>Información parcial (sólo se conoce a los vecinos)</li>
<li>Entre vecinos se intercambia información (iterativamente) para reconstruir
información global</li>

<li>Distribuido</li>
</ul>
<p>
each node receives some information from one or more of its directly attached
neighbors, performs a calculation, and then distributes the results of its
calculation back to its neighbors.
</p>

<ul class="org-ul">
<li>Iterativo</li>
</ul>
<p>
the process continues on until no more information is exchanged between
neighbors. (Interestingly, the algorithm is also self-terminating - there is no
signal that the computation should stop; it just stops.)
</p>

<ul class="org-ul">
<li>Asincrónico</li>
<li>it does not require all of the nodes to operate in lockstep with each other.</li>
<li>No necesita coordinación para envíos o cálculos</li>
</ul>
<p>
Bellman-ford equation
dx(y) = min<sub>v</sub>\{c(x,v),dv(y)\} , &exist; (x,v) &isin; E (v es vecino de x)
dx(y) : cost of least-cost path from x to y
</p>


<p>
The basic idea is as follows. Each node x begins with D (y), an estimate of the
cost of the least-cost path from itself to node y, for all nodes, y, in N. Let
Dx=[Dx(y):y &isin; N] be node x’s distance vector, which is the vector of cost
estimates from x to all other nodes, y, in N. With the DV algorithm, each node x
maintains the following routing information:
</p>
<ul class="org-ul">
<li>For each neighbor v, the cost c(x, v) from x to directly attached neighbor, v</li>
<li>Node x’s distance vector, that is, Dx=[Dx(y):y &isin; N], containing x’s estimate
of its cost to all destinations, y, in N</li>
<li>The distance vectors of each of its neighbors, that is, Dv=[Dv(y):y &isin; N] for
each neighbor v of x</li>
</ul>

<p>
In the distributed, asynchronous algorithm, from time to time, each node sends a
copy of its distance vector to each of its neighbors. When a node x receives a
new distance vector from any of its neighbors w, it saves w’s distance vector,
and then uses the Bellman-Ford equation to update its own distance vector as
follows:
</p>

<p>
Dx(y) = min<sub>v</sub>\{c(x,v)+Dv(y)\} for each node y in N
</p>

<p>
If node x’s distance vector has changed as a result of this update step, node x
will then send its updated distance vector to each of its neighbors, which can
in turn update their own distance vectors. Miraculously enough, as long as all
the nodes continue to exchange their distance vectors in an asynchronous
fashion, each cost estimate D (y) converges to d (y), the actual cost of the
least-cost path from node x to node y.
</p>
</div>


<div id="outline-container-org0e7953e" class="outline-6">
<h6 id="org0e7953e">problemas</h6>
<div class="outline-text-6" id="text-org0e7953e">
<ul class="org-ul">
<li>Loops</li>
<li>Convergencia lenta hasta alcanzar el equilibrio</li>
</ul>

<p>
Solucion:
</p>
<ul class="org-ul">
<li>Si dz(x) utiliza a y, entonces z anuncia a y dz(x)=inf</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org4c5589a" class="outline-5">
<h5 id="org4c5589a">LS vs DV</h5>
<div class="outline-text-5" id="text-org4c5589a">
<p>
Velocidad de convergencia
</p>
<ul class="org-ul">
<li>LS ~inmediata (mensajes vía broadcast)</li>
<li>DV (muy) lento. (Cambios vía intermediarios + loops)</li>
</ul>
</div>
</div>


<div id="outline-container-org52c641b" class="outline-5">
<h5 id="org52c641b">Open Shortest Path First (OSPF)</h5>
<div class="outline-text-5" id="text-org52c641b">
</div>
<div id="outline-container-org19110d6" class="outline-6">
<h6 id="org19110d6">Autonomous System (AS)</h6>
<div class="outline-text-6" id="text-org19110d6">
<p>
a form of organization of routers that solves <code>scale</code> and <code>administrative
autonomy</code>, issues observed in practice when routers are viewed individually.
</p>

<dl class="org-dl">
<dt>Scale</dt><dd>As the number of routers becomes large, the overhead involved in
communicating, computing, and storing routing information becomes
prohibitive. Today’s Internet consists of hundreds of millions of
routers. Storing routing information for possible destinations at each of
these routers would clearly require enormous amounts of memory. The overhead
required to broadcast connectivity and link cost updates among all of the
routers would be huge! A distance-vector algorithm that iterated among such a
large number of routers would surely never converge. Clearly, something must
be done to reduce the complexity of route computation in a network as large as
the Internet.</dd>

<dt>Administrative autonomy</dt><dd>the Internet is a network of ISPs, with each ISP
consisting of its own network of routers. An ISP generally desires to operate
its network as it pleases (for example, to run whatever routing algorithm it
chooses within its network) or to hide aspects of its network’s internal
organization from the outside. Ideally, an organization should be able to
operate and administer its network as it wishes, while still being able to
connect its network to other outside networks.</dd>
</dl>

<p>
a group of routers that are under the same administrative control.
</p>

<p>
An autonomous system is identified by its globally unique autonomous system
number (ASN) [RFC 1930].
</p>

<p>
AS numbers,like IP addresses, are assigned by ICANN regional registries.
</p>

<p>
Cada AS permite libertad para tomar decisiones de ruteo
</p>
<ul class="org-ul">
<li>Protocolo de ruteo</li>
<li>Broadcast</li>
</ul>
</div>
</div>

<div id="outline-container-org245ed6a" class="outline-6">
<h6 id="org245ed6a">OSPF</h6>
<div class="outline-text-6" id="text-org245ed6a">
<p>
Protocolo de ruteo interno (intra-AS)
</p>

<p>
[RFC 2328]
</p>

<p>
OSPF is a link-state protocol that uses flooding of link-state information and a
Dijkstra’s least-cost path algorithm.
</p>

<p>
With OSPF, each router constructs a complete topological map of the entire
autonomous system (AS). Each router then locally runs Dijkstra’s shortest-path
algorithm to determine a shortest-path tree to all subnets, with itself as the
root node.
</p>

<p>
OSPF does not mandate a policy for how link weights are set.
</p>

<p>
With OSPF, a router broadcasts routing information to all other routers in the
autonomous system, not just to its neighboring routers. A router broadcasts
link-state information whenever there is a change in a link’s state (for
example, a change in cost or a change in up/down status). It also broadcasts a
link’s state periodically (at least once every 30 minutes), even if the link’s
state has not changed.
</p>

<p>
OSPF advertisements are contained in OSPF messages that are carried directly by
IP, with an upper-layer protocol of 89 for OSPF. Thus, the OSPF protocol must
itself implement functionality such as reliable message transfer and link-state
broadcast. The OSPF protocol also checks that links are operational (via a HELLO
message that is sent to an attached neighbor) and allows an OSPF router to
obtain a neighboring router&rsquo;s database of network-wide link state.
</p>
</div>

<div id="outline-container-org8f89e60" class="outline-7">
<h7 id="org8f89e60">advantages</h7>
<div class="outline-text-7" id="text-org8f89e60">
<dl class="org-dl">
<dt>Security</dt><dd>Exchanges between OSPF routers (for example, link-state updates)
can be authenticated. With authentication, only trusted routers can
participate in the OSPF protocol within an AS, thus preventing malicious
intruders from injecting incorrect information into router tables. By default,
OSPF packets between routers are not authenticated and could be forged. Two
types of authentication can be configured:
<ul class="org-ul">
<li><code>simple authentication</code>, the same password is configured on each
router. When a router sends an OSPF packet, it includes the password in
plaintext. Clearly not very secure.</li>
<li><code>MD5 authentication</code> is based on shared secret keys that are configured in
all the routers. For each OSPF packet that it sends, the router computes
the MD5 hash of the content of the OSPF packet appended with the secret
key. Then the router includes the resulting hash value in the OSPF
packet. The receiving router, using the preconfigured secret key, will
compute an MD5 hash of the packet and compare it with the hash value that
the packet carries, thus verifying the packet’s authenticity. Sequence
numbers are also used with MD5 authentication to protect against replay
attacks.</li>
</ul></dd>

<dt>Multiple same-cost paths</dt><dd>When multiple paths to a destination have the same
cost, OSPF allows multiple paths to be used (that is, a single path need not
be chosen for carrying all traffic when multiple equal-cost paths exist).</dd>

<dt>Integrated support for unicast and multicast routing</dt><dd>Multicast OSPF (MOSPF)
[RFC 1584] provides simple extensions to OSPF to provide for multicast
routing. MOSPF uses the existing OSPF link database and adds a new type of
link-state advertisement to the existing OSPF link-state broadcast mechanism.</dd>

<dt>Support for hierarchy within a single AS</dt><dd>An OSPF autonomous system can be
configured hierarchically into areas. Each area runs its own OSPF link-state
routing algorithm, with each router in an area broadcasting its link state to
all other routers in that area. Within each area, one or more area border
routers are responsible for routing packets outside the area. Lastly, exactly
one OSPF area in the AS is configured to be the backbone area. The primary
role of the backbone area is to route traffic between the other areas in the
AS. The backbone always contains all area border routers in the AS and may
contain non-border routers as well. Inter-area routing within the AS requires
that the packet be first routed to an area border router (intra-area routing),
then routed through the backbone to the area border router that is in the
destination area, and then routed to the final destination.</dd>
</dl>
</div>
</div>
</div>
</div>
</div>


<div id="outline-container-org0b7f10e" class="outline-4">
<h4 id="org0b7f10e">clasificacion de protocolos</h4>
<div class="outline-text-4" id="text-org0b7f10e">
</div>
<div id="outline-container-org0a79c02" class="outline-5">
<h5 id="org0a79c02">estático vs dinámico</h5>
<div class="outline-text-5" id="text-org0a79c02">
</div>
<div id="outline-container-org7c2966a" class="outline-6">
<h6 id="org7c2966a">estático</h6>
<div class="outline-text-6" id="text-org7c2966a">
<ul class="org-ul">
<li>Paths determinados por humanos</li>
<li>¿Cuándo es útil?
<ul class="org-ul">
<li>Topologías que cambian poco frecuentemente</li>
<li>Topologías pequeñas</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org3d64501" class="outline-6">
<h6 id="org3d64501">dinámico</h6>
<div class="outline-text-6" id="text-org3d64501">
<ul class="org-ul">
<li>Reconfiguración de path automático ante
<ul class="org-ul">
<li>Cambios de topología</li>
<li>Variables de calidad de la red</li>
</ul></li>
<li>Problemas: pueden generar loops y oscilaciones</li>
</ul>
</div>
</div>
</div>
</div>
</div>


<div id="outline-container-org6079e96" class="outline-3">
<h3 id="org6079e96"><span class="todo TODO">TODO</span> Control Plane en SDNs</h3>
<div class="outline-text-3" id="text-org6079e96">
</div>
<div id="outline-container-org23b7e3c" class="outline-4">
<h4 id="org23b7e3c">Elementos de una arq SDN</h4>
<div class="outline-text-4" id="text-org23b7e3c">
</div>
<div id="outline-container-org63bc5ae" class="outline-5">
<h5 id="org63bc5ae">Flow table</h5>
<div class="outline-text-5" id="text-org63bc5ae">
<p>
Basada en múltiples headers de múltiples capas (L4, IP, Link)
</p>
</div>
</div>

<div id="outline-container-orgbd422a5" class="outline-5">
<h5 id="orgbd422a5">Separacion de DP y CP</h5>
<div class="outline-text-5" id="text-orgbd422a5">
<ul class="org-ul">
<li>DP: Memorias de acceso rápido donde se ejecuta m+a</li>
<li>CP: Servers y SW remoto donde se calculan las FT</li>
</ul>
</div>
</div>

<div id="outline-container-orge570b59" class="outline-5">
<h5 id="orge570b59">Funciones de red externas</h5>
<div class="outline-text-5" id="text-orge570b59">
<ul class="org-ul">
<li>SDN controller
<ul class="org-ul">
<li>Control del estado de SW y enlaces</li>
<li>Envió de estados a las apps</li>
<li>comunicacion y escritura de SW</li>
</ul></li>
<li>Aplicaciones de red</li>
</ul>
</div>
</div>

<div id="outline-container-orge8bdd56" class="outline-5">
<h5 id="orge8bdd56">APIs</h5>
<div class="outline-text-5" id="text-orge8bdd56">
<p>
Medios para que Apps puedan actuar sobre el funcionamiento de la red
</p>
</div>
</div>
</div>

<div id="outline-container-orgcd6a30e" class="outline-4">
<h4 id="orgcd6a30e">Cambios de arquitectura del Control Plane</h4>
<div class="outline-text-4" id="text-orgcd6a30e">
</div>
<div id="outline-container-orgf94f192" class="outline-5">
<h5 id="orgf94f192">pre sdn</h5>
<div class="outline-text-5" id="text-orgf94f192">
<ol class="org-ol">
<li>Monolítica: CP y DP en el mismo dispositivo</li>
<li>Integración vertical: HW + OS@CP desarrollados por el fabricante</li>
<li>Inflexibilidad: Imposibilidad de alterar funciones preestablecidas por el
fabricante en el CP</li>
</ol>
</div>
</div>

<div id="outline-container-orgd5d9a5c" class="outline-5">
<h5 id="orgd5d9a5c">sdn</h5>
<div class="outline-text-5" id="text-orgd5d9a5c">
<ol class="org-ol">
<li>Diversificación de los servicios</li>
<li>SW &amp; HW provistos por diferentes proveedores</li>
<li>Ecosistema similar al OSes @ PCs</li>
</ol>
</div>
</div>
</div>


<div id="outline-container-org046b86d" class="outline-4">
<h4 id="org046b86d">Elementos del CP</h4>
<div class="outline-text-4" id="text-org046b86d">
</div>
<div id="outline-container-org639fd7d" class="outline-5">
<h5 id="org639fd7d">Controlador SDN</h5>
<div class="outline-text-5" id="text-org639fd7d">
</div>
<div id="outline-container-org97eebda" class="outline-6">
<h6 id="org97eebda">Capa de comunicacion</h6>
<div class="outline-text-6" id="text-org97eebda">
<p>
Entre el controlador y los SW
</p>

<p>
Necesidad:
</p>
<ul class="org-ul">
<li>Transmitir información entre ambos</li>
<li>SW notifiquen eventos (links levantados o caidos)</li>
</ul>

<p>
Openflow
</p>
</div>
</div>

<div id="outline-container-org67bafdf" class="outline-6">
<h6 id="org67bafdf">Control de estado de la red</h6>
<div class="outline-text-6" id="text-org67bafdf">
<ul class="org-ul">
<li>Acá se toman las decisiones (ej. Configurar las tablas)</li>
<li>Necesita información en casi tiempo real del estado de toda la red</li>
<li>Configura Forwarding Table de cada uno de los Switches</li>
<li>Se mantiene un copia local de cada una de las Forwarding Tables distribuidas</li>
</ul>
</div>
</div>

<div id="outline-container-org9827e47" class="outline-6">
<h6 id="org9827e47">Interfaz con las applicaciones</h6>
<div class="outline-text-6" id="text-org9827e47">
<p>
interaccion via apis
</p>

<p>
Consumidores
</p>
<ul class="org-ul">
<li>Apps: Intervienen en la toma decisiones</li>
</ul>

<p>
Prestaciones
</p>
<ul class="org-ul">
<li>Permite que <code>rw</code>? los estados de la red y/o las FTs</li>
</ul>

<p>
<b>**</b>
</p>

<p>
Implementación
</p>
<ul class="org-ul">
<li>Lógicamente centralizado</li>
<li>Hardware distribuido</li>
</ul>

<p>
Ejemplos
</p>
<ul class="org-ul">
<li>SDN controller puede correr en un cluster de un datacenter</li>
<li>Las App de control de red pueden ser remotas</li>
</ul>

<p>
Implementaciones propietarias
</p>
<ul class="org-ul">
<li>ONIX</li>
<li>Juniper -&gt; Juniper Contrail</li>
<li>Google -&gt; La usada en B4</li>
</ul>

<p>
Implementaciones de código abierto
</p>
<ul class="org-ul">
<li>OpenDaylight</li>
<li>ONOS</li>
<li>POX</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgecfb98d" class="outline-5">
<h5 id="orgecfb98d">Openflow</h5>
<div class="outline-text-5" id="text-orgecfb98d">
<p>
Opera entre el controlador SDN y un Switch SDN
</p>

<p>
OpenFlow: TCP:6653
</p>
</div>

<div id="outline-container-org4e0f3ee" class="outline-6">
<h6 id="org4e0f3ee">Mensajes OpenFlow:</h6>
<div class="outline-text-6" id="text-org4e0f3ee">
</div>
<div id="outline-container-org97f1800" class="outline-7">
<h7 id="org97f1800">Controlador -&gt; Switch</h7>
<div class="outline-text-7" id="text-org97f1800">
<ol class="org-ol">
<li>Configuración Consultar o fijar parámetros en el SW</li>
<li>Modificar estado</li>
</ol>
<p>
Agregar o remover entradas en la FT
</p>
<ol class="org-ol">
<li>Leer estado</li>
</ol>
<p>
Acceder a las estadísticas del SW
</p>
<ol class="org-ol">
<li>Enviar paquete</li>
</ol>
<p>
Función: enviar un pckt al SW y fwd por un puerto específico
</p>
</div>
</div>

<div id="outline-container-org17fbb00" class="outline-7">
<h7 id="org17fbb00">Switch -&gt; controlador</h7>
<div class="outline-text-7" id="text-org17fbb00">
<ol class="org-ol">
<li>Entrada removida Notifica entrada de la FT fue removida (ACK de modificar
entrada o expiró la entrada)</li>
<li>Port Status Reporte de up/down de puertos o enlaces</li>
<li>Packet-in if match(pckt) == NULL: Se podría enviar al SW para que decida que
hacer</li>
</ol>
</div>
</div>
</div>
</div>

<div id="outline-container-org479a111" class="outline-5">
<h5 id="org479a111">Aplicaciones de control de red</h5>
<div class="outline-text-5" id="text-org479a111">
</div>
<div id="outline-container-org5f4424b" class="outline-6">
<h6 id="org5f4424b">Google B4</h6>
<div class="outline-text-6" id="text-org5f4424b">
<p>
Google utiliza un SDNs para el manejo de su red
</p>

<p>
1.¿Dónde?
</p>
<ul class="org-ul">
<li>Manejo de la red entre sus datacenters y sus PoPs @ (IXPs &amp; ISPs)</li>
</ul>
<p>
2.¿Qué logra?
</p>
<ul class="org-ul">
<li>Enlaces utilizados &gt; 70%</li>
</ul>
<p>
3.¿Por qué es posible?
</p>
<ul class="org-ul">
<li>Google controla toda la infra de punta a punta (DC, SW, enlaces y svrs)</li>
<li>La cantidad de Datacenters en acción es reducida (~10)</li>
</ul>
<p>
4.¿Donde corre el Data Plane?
</p>
<ul class="org-ul">
<li>En algún Datacenter de Google</li>
</ul>
<p>
5.¿Cómo se comunica el controlador con los SW?
</p>
<ul class="org-ul">
<li>Opción out of band: Una red aparte</li>
</ul>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-orgce86657" class="outline-3">
<h3 id="orgce86657">asd</h3>
<div class="outline-text-3" id="text-orgce86657">
</div>
<div id="outline-container-orgf798c35" class="outline-4">
<h4 id="orgf798c35">camino minimo</h4>
<div class="outline-text-4" id="text-orgf798c35">
</div>
<div id="outline-container-org1f1ed85" class="outline-5">
<h5 id="org1f1ed85">dijkstra</h5>
<div class="outline-text-5" id="text-org1f1ed85">
<p>
Paso #1: Elegimos un nodoSería el router desde donde se va a calcular el
algoritmo
</p>

<p>
Paso #2: Costo a los nodos vecinos
</p>

<p>
Paso #3:  Costo  de  los  vecinos  a sus vecinos
</p>

<p>
O(n<sup>2</sup>)
</p>

<p>
ver libro!
</p>
</div>
</div>
</div>




<div id="outline-container-org08b8707" class="outline-4">
<h4 id="org08b8707">bellman-ford</h4>
<div class="outline-text-4" id="text-org08b8707">
<p>
ver libro
</p>
</div>
</div>
</div>
</div>

<div id="outline-container-org6f37892" class="outline-2">
<h2 id="org6f37892">Capa de Enlace - DOCSIS y ARP</h2>
<div class="outline-text-2" id="text-org6f37892">
<p>
6-6.1, 6.3-6.4.1
</p>

<p>
we naturally wonder how packets are sent across the individual links that make
up the end-to-end communication path.
</p>

<p>
How are the network-layer datagrams encapsulated in the link-layer frames for
transmission over a single link?
</p>

<p>
Are different link-layer protocols used in the different links along the
communication path?
</p>

<p>
How are transmission conflicts in broadcast links resolved?
</p>

<p>
Is there addressing at the link layer and, if so, how does the link-layer
addressing operate with the network-layer addressing we learned about in Chapter
4?
</p>

<p>
And what exactly is the difference between a switch and a router?
</p>

<p>
there are two fundamentally ­different types of link-layer channels. The first
type are broadcast channels, which connect multiple hosts in wireless LANs,
satellite networks, and hybrid fiber-coaxial cable (HFC) access networks. Since
many hosts are connected to the same broadcast communication channel, a
so-called medium access protocol is needed to coordinate frame transmission. In
some cases, a central controller may be used to coordinate transmissions; in
other cases, the hosts themselves coordinate transmissions.
</p>

<p>
The second type of link-layer channel is the point-to-point communication link,
such as that often found between two routers connected by a long-distance link,
or between a user’s office computer and the nearby Ethernet switch to which it
is connected. Coordinating access to a point-to-point link is simpler; the
reference material on this book’s Web site has a detailed discussion of the
Point-to-Point Protocol (PPP), which is used in settings ranging from dial-up
service over a telephone line to high-speed point-to-point frame transport over
fiber-optic links.
</p>
</div>

<div id="outline-container-org5cbdd2c" class="outline-3">
<h3 id="org5cbdd2c">Intro to the link layer</h3>
<div class="outline-text-3" id="text-org5cbdd2c">
<p>
node: any devicethat runs a link-layer protocol
</p>

<p>
Nodes include hosts, routers, switches, and WiFiaccess points. We will also
refer to the communication channels that connect adjacent nodes along the
communication path as links
</p>

<p>
imagen 6.1
</p>
</div>

<div id="outline-container-org8e77368" class="outline-4">
<h4 id="org8e77368">The Services Provided by the Link Layer</h4>
<div class="outline-text-4" id="text-org8e77368">
<p>
Although the basic service of any link layer is to move a datagram from one node
to an adjacent nodeover a single communication link, the details of the provided
service can vary from one link-layerprotocol to the next.
</p>
</div>

<div id="outline-container-orga26525b" class="outline-5">
<h5 id="orga26525b">Framing</h5>
<div class="outline-text-5" id="text-orga26525b">
<p>
Almost all link-layer protocols encapsulate each network-layer datagram within a
link-layer frame before transmission over the link. A frame consists of a data
field, in which the network-layer datagram is inserted, and a number of header
fields. The structure of the frame is specified by the link-layer
protocol. We’ll see several different frame formats when we examine specific
link-layer protocols in the second half of this chapter.
</p>
</div>
</div>

<div id="outline-container-org74b57e3" class="outline-5">
<h5 id="org74b57e3">Link access</h5>
<div class="outline-text-5" id="text-org74b57e3">
<p>
A medium access control (MAC) protocol specifies the rules by which a frame is
transmitted onto the link. For point-to-point links that have a single sender at
one end of the link a nda single receiver at the other end of the link, the MAC
protocol is simple (or nonexistent)—the sender can send a frame whenever the
link is idle. The more interesting case is when multiple nodes share a single
broadcast link—the so-called multiple access problem. Here, the MAC protocol
serves to coordinate the frame transmissions of the many nodes.
</p>
</div>
</div>

<div id="outline-container-orga2ab5ea" class="outline-5">
<h5 id="orga2ab5ea">Reliable delivery</h5>
<div class="outline-text-5" id="text-orga2ab5ea">
<p>
When a link-layer protocol provides reliable delivery service, it guarantees to
move each network-layer datagram across the link without error. Recall that
certain transport-layer protocols (such as TCP) also provide a reliable delivery
service. Similar to a transport-layer reliable delivery service, a link-layer
reliable delivery service can be achieved with acknowledgments and
retransmissions (see Section 3.4). A link-layer reliable delivery service is
often used for links that are prone to high error rates, such as a wireless
link, with the goal of correcting an error locally—on the link where the error
occurs—rather than forcing an end-to-end retransmission of the data by a
transport- or application-layer protocol. However, link-layer reliable delivery
can be considered an unnecessary overhead for low bit-error links, including
fiber, coax, and many twisted-pair copper links. For this reason, many wired
link-layer protocols do not provide a reliable delivery service.
</p>
</div>
</div>

<div id="outline-container-orgeec02a9" class="outline-5">
<h5 id="orgeec02a9">Error detection and correction</h5>
<div class="outline-text-5" id="text-orgeec02a9">
<p>
The link-layer hardware in a receiving node can incorrectly decide that a bit in
a frame is zero when it was transmitted as a one, and vice versa. Such bit
errors are introduced by signal attenuation and electromagnetic noise. Because
there is no need to forward a datagram that has an error, many link-layer
protocols provide a mechanism to detect such bit errors.
</p>

<p>
This is done by having the transmitting node include error-detection bits in the
frame, and having the receiving node perform an error check. Recall from
Chapters 3 and 4 that the Internet’s transport layer and network layer also
provide a limited form of error detection—the Internet checksum. Error detection
in the link layer is usually more sophisticated and is implemented in
hardware. Error correction is similar to error detection, except that a receiver
not only detects when bit errors have occurred in the frame but also determines
exactly where in the frame the errors have occurred (and then corrects these
errors).
</p>
</div>
</div>
</div>

<div id="outline-container-org1d83242" class="outline-4">
<h4 id="org1d83242">Where Is the Link Layer Implemented?</h4>
<div class="outline-text-4" id="text-org1d83242">
<p>
for the most part, the link layer is implemented in a network adapter, also
sometimes known as a <code>network interface card (NIC)</code>. At the heart of the network
adapter is the link-layer controller, usually a single, special-purpose chip
that implements many of the link-layer services (framing, link access, error
detection, and so on). Thus, much of a link-layer controller’s functionality is
implemented in hardware.
</p>

<p>
imagen 6.2
</p>

<p>
On the sending side, the controller takes a datagram that has been created and
stored in host memory by the higher layers of the protocol stack, encapsulates
the datagram in a link-layer frame (filling in the frame’s various fields), and
then transmits the frame into the communication link, following the link-access
protocol.
</p>

<p>
On the receiving side, a controller receives the entire frame, and extracts the
network-layer datagram.
</p>

<p>
If the link layer performs error detection, then it is the sending controller
that sets the error-detection bits in the frame header and it is the receiving
controller that performs error detection
</p>

<p>
while most of the link layer is implemented in hardware, part of the link layer
is implemented in software that runs on the host’s CPU. The software components
of the link layer implement higher-level link-layer functionality such as
assembling link-layer addressing information and activating the controller
hardware.
</p>

<p>
On the receiving side, link-layer software responds to controller
interrupts (e.g., due to the receipt of one or more frames), handling error
conditions and passing a datagram up to the network layer. Thus, the link layer
is a combination of hardware and software—the place in the protocol stack where
software meets hardware.
</p>
</div>
</div>
</div>

<div id="outline-container-org6127a5c" class="outline-3">
<h3 id="org6127a5c">Multiple Access Links and Protocols</h3>
<div class="outline-text-3" id="text-org6127a5c">
<p>
A <code>point-to-point link</code> consists of a single sender at one end of the link and
a single receiver at the other end of the link. Many link-layer protocols have
been designed for point-to-point links; the point-to-point protocol (PPP) and
high-level data link control (HDLC) are two such protocols.
</p>

<p>
a <code>broadcast link</code>, can have multiple sending and receiving nodes all connected
to the same, single, shared broadcast channel. The term broadcast is used here
because when any one node transmits a frame, the channel broadcasts the frame
and each of the other nodes receives a copy. Ethernet and wireless LANs are
examples of broadcast link-layer technologies.
</p>

<dl class="org-dl">
<dt>the <code>multiple access problem</code></dt><dd><p>
how to coordinate the access of multiple
sending and receiving nodes to a shared broadcast channel
</p>

<p>
Computer networks have protocols — so-called multiple access protocols — by
which nodes regulate their transmission into the shared broadcast channel.
</p>

<p>
When multiple nodes transmit frames on a channel at the same time, all of the
nodes receive multiple frames at the same time; that is, the transmitted frames
<code>collide</code> at all of the receivers. Typically, when there is a collision, none of
the receiving nodes can make any sense of any of the frames that were
transmitted. Thus, all the frames involved in the collision are lost, and
the broadcast channel is wasted during the collision interval.
</p>

<p>
It is the multiple access protocol&rsquo;s job to coordinate the transmission of the
active nodes.
</p>

<p>
we can classify just about any multiple access protocol as belonging to one of
three categories:
</p>
<ul class="org-ul">
<li>channel partitioning protocols,</li>
<li>random access protocols, and</li>
<li>taking-turns protocols.</li>
</ul>

<p>
desirable characteristics of a multiple-access-protocol for a broadcast channel
of rate \(R\) bits per second:
</p>
<ol class="org-ol">
<li>When only one node has data to send, that node has a throughput of R bps.</li>
<li>When M nodes have data to send, each of these nodes has a throughput of R/M
bps. This need not necessarily imply that each of the M nodes always has an
instantaneous rate of R/M, but rather that each node should have an average
transmission rate of R/M over some suitably defined interval of time.</li>
<li>The protocol is decentralized; that is, there is no master node that
represents a single point of failure for the network.</li>
<li>The protocol is simple, so that it is inexpensive to implement.</li>
</ol></dd>
</dl>
</div>


<div id="outline-container-orgd3892d7" class="outline-4">
<h4 id="orgd3892d7">Channel Partitioning Protocols</h4>
<div class="outline-text-4" id="text-orgd3892d7">
<p>
N: number of nodes on the channel.
</p>

<p>
R: transmission rate of the channel in bps.
</p>
</div>

<div id="outline-container-org36e0d20" class="outline-5">
<h5 id="org36e0d20">Time Division Multiplexing (TDM)</h5>
<div class="outline-text-5" id="text-org36e0d20">
<p>
TDM divides time into <code>time frames</code> and further divides each time frame into
N <code>time slots</code>.
</p>

<p>
Each time slot is then assigned to one of the N nodes.
</p>

<p>
Whenever a node has a packet to send, it transmits the packet’s bits during its
assigned time slot in the revolving TDM frame.
</p>

<ul class="org-ul">
<li>Pros
<ul class="org-ul">
<li>Avoids collisions</li>
<li>Fair division of the bandwidth among the nodes</li>
<li>Each node gets a dedicated transmission rate of R/N bps during each frame
time.</li>
</ul></li>
<li>Cons
<ul class="org-ul">
<li>a node is limited to an average rate of R/N bps even when it is the only
node with packets to send</li>
<li>a node must always wait for its turn in the transmission sequence even when
it is the only node with packets to send</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgb968efa" class="outline-5">
<h5 id="orgb968efa">Frequency Division Multiplexing (FDM)</h5>
<div class="outline-text-5" id="text-orgb968efa">
<p>
FDM divides the R bps channel into different frequencies (each with a bandwidth
of R/N) and assigns each frequency to one of the N nodes.
</p>

<p>
FDM thus creates N smaller channels of R/N bps out of the single, larger R bps
channel.
</p>

<ul class="org-ul">
<li>Pros
<ul class="org-ul">
<li>Avoids collisions</li>
<li>Fair division of the bandwidth among the nodes</li>
<li>Each node gets a dedicated transmission rate of R/N bps during each frame
time.</li>
</ul></li>
<li>Cons
<ul class="org-ul">
<li>a node is limited to an average rate of R/N bps even when it is the only
node with packets to send</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org3305e2d" class="outline-5">
<h5 id="org3305e2d">Code Division Multiple Access (CDMA)</h5>
<div class="outline-text-5" id="text-org3305e2d">
<p>
CDMA assigns a different code to each node.
</p>

<p>
Each node then uses its unique code to encode the data bits it sends. If the
codes are chosen carefully, CDMA networks have the property that different nodes
can transmit simultaneously and yet have their respective receivers correctly
receive a sender’s encoded data bits (assuming the receiver knows the sender’s
code) in spite of interfering transmissions by other nodes.
</p>
</div>
</div>
</div>

<div id="outline-container-org4a18085" class="outline-4">
<h4 id="org4a18085">Random Access Protocols</h4>
<div class="outline-text-4" id="text-org4a18085">
<p>
a transmitting node always transmits at the full rate of the channel
</p>

<p>
When there is a collision, each node involved in the collision repeatedly
retransmits its frame (that is, packet) until its frame gets through without a
collision.
</p>

<p>
The node retransmits the frame after waiting for a random delay independently
from the other nodes and hopefully it will be able to send the frame into the
channel without a collision.
</p>
</div>

<div id="outline-container-org6ffeeda" class="outline-5">
<h5 id="org6ffeeda">Slotted ALOHA</h5>
<div class="outline-text-5" id="text-org6ffeeda">
<p>
Assume the following:
</p>
<ul class="org-ul">
<li>All frames consist of exactly L bits.</li>
<li>Time is divided into slots of size L/R seconds (that is, a slot equals the
time to transmit one frame).</li>
<li>Nodes start to transmit frames only at the beginnings of slots.</li>
<li>The nodes are synchronized so that each node knows when the slots begin.</li>
<li><p>
If two or more frames collide in a slot, then all the nodes detect the
collision event before the slot ends.
</p>

<p>
Operation:
</p>
<ul class="org-ul">
<li>When the node has a fresh frame to send, it waits until the beginning of the
next slot and transmits the entire frame in the slot.</li>
<li>If there isn’t a collision, the node has successfully transmitted its frame
and thus need not consider retransmitting the frame. (The node can prepare a
new frame for transmission, if it has one.)</li>
<li><p>
If there is a collision, the node detects the collision before the end of the
slot. The node retransmits its frame in each subsequent slot with probability
\(p\) until the frame is transmitted without a collision.
</p>

<p>
allows a node to transmit continuously at the full rate, R, when that node is
the only active node. (A node is said to be active if it has frames to send.)
</p>

<p>
highly decentralized, because each node detects collisions and independently
decides when to retransmit.
</p>

<p>
however,it requires the slots to be synchronized in the nodes
</p>

<p>
Slotted ALOHA works well when there is only one active node, but how ­efficient
is it when there are multiple active nodes?
</p>

<p>
imagen 6.10
</p>

<ol class="org-ol">
<li>when there are multiple active nodes, a certain fraction of the slots will
have collisions and will therefore be “wasted.”</li>
<li>another fraction of the slots will be empty because all active nodes refrain
from transmitting as a result of the probabilistic transmission policy.</li>
</ol>

<p>
the only “unwasted” slots will be those in which exactly one node transmits. A
slot in which exactly one node transmits is said to be a <code>successful slot</code>. The
efficiency of a slotted multiple access protocol is defined to be the long-run
fraction of successful slots in the case when there are a large number of active
nodes, each always having a large number of frames to send.
</p>

<p>
the maximum efficiency of the protocol is given by p=1/e=0,37.
</p>

<p>
when a large number of nodes have many frames to transmit,then (at best) only 37
percent of the slots do useful work. Thus the effective transmission rate of
the channel is not R bps but only 0.37 R bps!
</p></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgce8de9b" class="outline-5">
<h5 id="orgce8de9b">ALOHA</h5>
<div class="outline-text-5" id="text-orgce8de9b">
<p>
The first ALOHA protocol was unslotted and fully decentralized.
</p>

<p>
when a frame first arrives (that is, a network-layer datagram is passed down
from the network layer at the sending node), the node immediately transmits the
frame in its entirety into the broadcast channel.
</p>

<p>
Si se detecta una colision, inmediatamente luego de transmitir el frame
colisionado, se retransmite el frame con una probabilidad \(p\). Si no se detecta
colision, se transmite el proximo frame con probabilidad \(p\).
</p>

<p>
siempre se espera a que se termine de transmitir el frame.
</p>

<p>
imagen 6.11
</p>

<p>
the maximum efficiency of the protocol is given by p=1/(2e).
</p>

<p>
La mitad de eficiencia que el protocolo slotted ALOHA. Esto es porque es
totalmente descentralizado.
</p>
</div>
</div>

<div id="outline-container-org4cb7c89" class="outline-5">
<h5 id="org4cb7c89">Carrier Sense Multiple Access (CSMA)</h5>
<div class="outline-text-5" id="text-org4cb7c89">
<dl class="org-dl">
<dt>Listen before speaking</dt><dd>If someone else is speaking, wait until they are
finished. In the networking world, this is called <code>carrier sensing</code> —a node
listens to the channel before transmitting. If a frame from another node is
currently being transmitted into the channel, a node then waits until it
detects no transmissions for a short amount of time and then begins
transmission.</dd>

<dt>If someone else begins talking at the same time, stop talking</dt><dd><p>
In the
networking world, this is called <code>collision detection</code> —a transmitting node
listens to the channel while it is transmitting. If it detects that another
node is transmitting an interfering frame, it stops transmitting and waits a
random amount of time before repeating the sense-and-transmit-when-idle
cycle.
</p>

<p>
imagen 6.12
</p>

<p>
From Figure 6.12, it is evident that the end-to-end <code>channel propagation delay</code>
of a broadcast channel—the time it takes for asignal to propagate from one of
the nodes to another—will play a crucial role in determining its
performance. The longer this propagation delay, the larger the chance that a
carrier-sensing node is not yet able to sense a transmission that has already
begun at another node in the network.
</p></dd>
</dl>
</div>
</div>

<div id="outline-container-orgcbae014" class="outline-5">
<h5 id="orgcbae014">Carrier Sense Multiple Access with Collision Detection (CSMA/CD)</h5>
<div class="outline-text-5" id="text-orgcbae014">
<p>
When a node performs collision detection, itceases transmission as soon as it
detects a collision.
</p>

<p>
imagen 6.13
</p>

<ol class="org-ol">
<li>The adapter obtains a datagram from the network layer, prepares a link-layer
frame, and puts the frame adapter buffer.</li>
<li>If the adapter senses that the channel is idle (that is, there is no signal
energy entering the adapter from the channel), it starts to transmit the
frame. If, on the other hand, the adapter senses that the channel is busy, it
waits until it senses no signal energy and then starts to transmit the frame.</li>
<li>While transmitting, the adapter monitors for the presence of signal energy
coming from other adapters using the broadcast channel.</li>
<li>If the adapter transmits the entire frame without detecting signal energy
from other adapters, the adapter is finished with the frame. If, on the other
hand, the adapter detects signal energy from other adapters while
transmitting, it aborts the transmission (that is, it stops transmitting its
frame).</li>
<li><p>
After aborting, the adapter waits a random amount of time and then returns to
step 2.
</p>

<p>
Como se elije el tiempo de espera aleatorio?
</p>

<dl class="org-dl">
<dt><code>binary exponential backoff</code> algorithm</dt><dd><p>
when transmitting a frame that has
already experienced \(n\) collisions, a node chooses the value of \(K\) at random
from \(\{0,1,2,2^{n-1}\}\)
</p>

<p>
example
</p>

<p>
We define the <code>efficiency of CSMA/CD</code> to be the long-run fraction of time during
which frames are being transmitted on the channel without collisions when there
is a large number of active nodes, with each node having a large number of
frames to send.
</p>

<p>
let \(d_{prop}\) denote the maximum time it takes signal energy to propagate
between any two adapters.
</p>

<p>
Let \(d_{trans}\) be the time to transmit a maximum-size frame
</p>

<p>
\[Efficiency = 11 + 5 * \frac{d_{prop}}{d_{trans}}\]
</p></dd>
</dl></li>
</ol>
</div>
</div>
</div>

<div id="outline-container-org9d4489d" class="outline-4">
<h4 id="org9d4489d">Taking Turn Protocols</h4>
<div class="outline-text-4" id="text-org9d4489d">
</div>
<div id="outline-container-org3eb14b3" class="outline-5">
<h5 id="org3eb14b3">Polling Protocol</h5>
<div class="outline-text-5" id="text-org3eb14b3">
<p>
The polling protocol requires one of the nodes to be designated as a master
node. The master node polls each of the nodes in a round-robin fashion.
</p>

<p>
In particular, the master node first sends a message to node 1, saying that it
can transmit up to some maximum number of frames. After node 1 transmits some
frames, the master node tells node 2 it can transmit up to the maximum number of
frames. (The master node can determine when a node has finished sending its
frames by observing the lack of a signal on the channel.)
</p>

<p>
The procedure continues in this manner, with the master node polling each of the
nodes in a cyclic manner.
</p>

<ul class="org-ul">
<li>Cons:
<ul class="org-ul">
<li>polling delay: amount of time required to notify a node that it can
transmit.</li>
<li>if the master node fails, the channel becomes inioperative.</li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgd309a5b" class="outline-5">
<h5 id="orgd309a5b">Token-passing protocol</h5>
<div class="outline-text-5" id="text-orgd309a5b">
<p>
A small, special-purpose frame known as a <code>token</code> is exchanged among the
nodes in some fixed order. When a node receives a token, it holds onto the
token only if it has some frames to transmit; otherwise, it immediately
forwards the token to the next node. If a node does have frames to transmit
when it receives the token, it sends up to a maximum number of frames and
then forwards the token to the next node. Token passing is decentralized and
highly efficient. But for example, the failure of one node can crash the
entire channel. Or if a node accidentally neglects to release the token,
then some recovery procedure must be invoked to get the token back in
circulation.
</p>
</div>
</div>
</div>

<div id="outline-container-org3c5250e" class="outline-4">
<h4 id="org3c5250e">Data-Over-Cable Service Interface Specifications (DOCSIS)</h4>
<div class="outline-text-4" id="text-org3c5250e">
<p>
Recall that a cable access network typically connects several thousand
residential cable modems to a cable modem termination system (CMTS) at the
cable network headend.
</p>

<p>
DOCSIS uses FDM to divide the downstream (CMTS to modem) and upstream (modem
to CMTS) network segments into multiple frequency channels.
</p>

<p>
Each downstream channel is 6 MHz wide, with a maximum throughput of approximately
40 Mbps per channel;
</p>

<p>
Each upstream channel has a maximum channel width of 6.4 MHz, and a
maximum upstream throughput of approximately 30 Mbps.
</p>

<p>
Each channel (upstream and downstream) is a broadcast channel.
</p>

<p>
Frames transmitted on the downstream channel by the CMTS are received by all
cable modems receiving that channel; since there is just a single CMTS
transmitting into the downstream channel, there is no multiple access
problem.
</p>

<p>
In the upstream direction, since multiple cable modems share the same
upstream channel (frequency) to the CMTS, collisions can potentially occur.
</p>

<p>
imagen 6.14
</p>

<p>
each upstream channel is divided into intervals of time (TDM-like), each
containing a sequence of mini-slots during which cable modems can transmit to
the CMTS. The CMTS explicitly grants permission to individual cable modems to
transmit during specific mini-slots by sending a
control message known as a <code>MAP message</code> on a downstream channel to specify
which cable modem (with data to send) can transmit during which mini-slot for
the interval of time specified in the control message.
</p>

<p>
mini-slots are allocated to cable modems, so there are no colliding
transmissions during a mini-slot.
</p>

<p>
to know which cable modems have data to send (and therefore, be assigned a
mini-slot), the cable modems send mini-slot-request frames to the CMTS during
a special set of interval mini-slots that are dedicated for this purpose.
</p>

<p>
These mini-slot-request frames are transmitted in a random access manner and
so may collide with each other. A cable modem can neither sense whether the
upstream channel is busy nor detect collisions. Instead, the cable modem
infers that its mini-slot-request frame experienced a collision if it does
not receive a response to the requested allocation in the next downstream
control message. When a collision is inferred, a cable modem uses binary
exponential backoff to defer the retransmission of its mini-slot-request
frame to a future time slot. When there is little traffic on the upstream
channel, a cable modem may actually transmit data frames during slots
nominally assigned for mini-slot-request frames (and thus avoid having to
wait for a mini-slot assignment).
</p>
</div>
</div>
</div>

<div id="outline-container-orga097224" class="outline-3">
<h3 id="orga097224">Switched Local Area Networks</h3>
<div class="outline-text-3" id="text-orga097224">
<p>
imagen 6.15
</p>

<p>
Figure 6.15 shows a switched local network connecting three departments, two
servers and a router with four switches. Because these switches operate at the
link layer, they switch link-layer frames (rather than network-layer
datagrams), don’t recognize network-layer addresses, and don’t use routing
algorithms to determine paths through the network of layer-2 switches. Instead
of using IP addresses, we will soon see that they use link-layer addresses to
forward link-layer frames through the network of switches.
</p>
</div>

<div id="outline-container-org6c6d59e" class="outline-4">
<h4 id="org6c6d59e">Link-Layer Addressing and ARP</h4>
<div class="outline-text-4" id="text-org6c6d59e">
</div>
<div id="outline-container-org86ddde7" class="outline-5">
<h5 id="org86ddde7">Mac Addresses</h5>
<div class="outline-text-5" id="text-org86ddde7">
<p>
hosts and routers have adapters (network interfaces) which have a link-layer
address.
</p>

<p>
host or router with multiple network interfaces will thus have multiple
link-layer addresses associated with it, just as it would also have multiple
IP addresses associated with it. however, link-layer switches do not have
link-layer addresses associated with their interfaces that connect to hosts
and routers. This is because the job of the link-layer switch is to carry
datagrams between hosts and routers; a switch does this job transparently,
that is, without the host or router having to explicitly address the frame
to the intervening switch.
</p>


<p>
these address are called <code>LAN addresses</code>, <code>physical addresses</code> or <code>MAC
addresses</code>, and are of 6 bytes long. They can be changed via software.
</p>

<p>
no two adapters have the same address, because the ieee manages the mac
address space. an adapter manufacturer can buy an address space of 2<sup>24</sup>
addresses, having an available space to manufacture 2<sup>24</sup> adapters.
</p>

<p>
An adapter’s MAC address has a flat structure (as opposed to a hierarchical
structure) and doesn’t change no matter where the adapter goes.
</p>

<p>
When an adapter wants to send a frame to some destination adapter, the
sending adapter inserts the destination adapter’s MAC address into the frame
and then sends the frame into the LAN.  As we will soon see, a switch
occasionally broadcasts an incoming frame onto all of its interfaces.
because frames are broadcasted to all adapters, an adapters has to check if
a frame is addressed to it.  However, sometimes a sending adapter does want
all the other adapters on the LAN to receive and process the frame it is
about to send. In this case, the sending adapter inserts a special <code>MAC
broadcast address</code> into the destination address field of the frame.
</p>
</div>
</div>

<div id="outline-container-org5734651" class="outline-5">
<h5 id="org5734651">Address Resolution Protocol (ARP)</h5>
<div class="outline-text-5" id="text-org5734651">
<p>
imagen 6.17
</p>

<p>
each host and router has a single IP address and single MAC address
</p>

<p>
assume in this section that the switch broadcasts all frames; that is,
whenever a switch receives a frame on one interface, it forwards the frame
on all of its other interfaces.
</p>

<p>
uppose that the host with IP address 222.222.222.220 wants to send an IP
datagram to host 222.222.222.222. In this example, both the source and
destination are in the same subnet.
</p>

<p>
To send a datagram, the source must give its adapter not only the IP
datagram but also the MAC address for destination 222.222.222.222. The
sending adapter will then construct a link-layer frame containing the
destination’s MAC address and send the frame into the LAN.
</p>

<p>
But how does the sending host, know the MAC address of destination host?
</p>

<p>
The solution is ARP. n ARP module in the sending host takes any IP address
on the same LAN as input, and returns the corresponding MAC address.
</p>

<p>
ARP resolves an IP address to a MAC address for hosts and router interfaces
in the same subnet.
</p>

<p>
Each host and router has an <code>ARP table</code> in its memory, which contains
mappings of IP addresses to MAC addresses.
</p>

<p>
imagen 6.18
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-right" />

<col  class="org-left" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-right">IP Address</td>
<td class="org-left">MAC Address</td>
<td class="org-right">TTL</td>
</tr>

<tr>
<td class="org-right">222.222.222.221</td>
<td class="org-left">88-B2-2F-54-1A-0F</td>
<td class="org-right">13:45:00</td>
</tr>

<tr>
<td class="org-right">222.222.222.223</td>
<td class="org-left">5C-66-AB-90-75-B1</td>
<td class="org-right">13:52:00</td>
</tr>
</tbody>
</table>

<p>
The ARP table also contains a time-to-live (TTL) value, which indicates when
each mapping will be deleted from the table. Note that a table does not
necessarily contain an entry for every host and router on the subnet; some
may have never been entered into the table, and others may have expired.
</p>

<p>
if the ARP table doesn’t currently have an entry for the destination, the
sender uses the ARP protocol to resolve the address. First, the sender
constructs a special packet called an <code>ARP packet</code>. An ARP packet has
several fields, including the sending and receiving IP and MAC
addresses. Both ARP query and response packets have the same format. The
purpose of the ARP query packet is to query all the other hosts and routers
on the subnet to determine the MAC address corresponding to the IP address
that is being resolved.
</p>

<p>
the query ARP message is sent within a broadcast frame, whereas the response
ARP message is sent within a standard frame.
</p>

<p>
ARP is plug-and-play; that is, an ARP table gets built ­automatically—it
doesn’t have to be configured by a system administrator. And if a host
becomes disconnected from the subnet, its entry is eventually deleted from
the other ARP tables in the subnet.
</p>

<p>
ARP is a protocol that arguably lies in between the link layer and the
network layer.
</p>
</div>
</div>

<div id="outline-container-orgcef1812" class="outline-5">
<h5 id="orgcef1812">Sending a datagram off the subnet</h5>
<div class="outline-text-5" id="text-orgcef1812">
<p>
imagen 6.19
</p>

<p>
Each host has exactly one IP address and one adapter.
</p>

<p>
Each router has an IP address for each of its interfaces. For each router
interface there is also an ARP module (in the router) and an adapter.
</p>

<p>
Because the router in Figure 6.19 has two interfaces, it has two IP addresses,
two ARP modules, and two adapters. Of course, each adapter in the network has
its own MAC address
</p>

<p>
El adaptador del nodo que envia, debe completar en el frame de capa de enlace,
la direccion MAC de el router que conecta a las subredes. Esto se conoce por
ARP. Una vez que el datagrama llega al router, este lo acepta porque esta
destinado para el mismo. El router inspecciona el paquete y observa que el
paquete (de capa de red) IP esta destinado a una IP de la otra subred.  El
router inspecciona en la tabla ARP para obtener la direccion MAC del nodo
receptor, y como direccion MAC de origen del nuevo datagrama, coloca la
direccion MAC del adaptador conectado a la subred destino.
</p>
</div>
</div>
</div>
</div>
</div>
<div id="outline-container-org4cde17b" class="outline-2">
<h2 id="org4cde17b">Capa de Enlace - LAN Ethernet</h2>
<div class="outline-text-2" id="text-org4cde17b">
</div>
<div id="outline-container-orgd64de9c" class="outline-3">
<h3 id="orgd64de9c">Switched LANs 524</h3>
<div class="outline-text-3" id="text-orgd64de9c">
</div>
<div id="outline-container-org95aaf9c" class="outline-4">
<h4 id="org95aaf9c">Ethernet</h4>
<div class="outline-text-4" id="text-org95aaf9c">
</div>
<div id="outline-container-org32369a3" class="outline-5">
<h5 id="org32369a3">Frame Ethernet</h5>
<div class="outline-text-5" id="text-org32369a3">
<p>
imagen 6.20
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">Preamble</td>
<td class="org-left">Destination Address</td>
<td class="org-left">Source Address</td>
<td class="org-left">Type</td>
<td class="org-left">Data</td>
<td class="org-left">CRC</td>
</tr>
</tbody>
</table>

<dl class="org-dl">
<dt>Data field (46 to 1,500 bytes)</dt><dd>This field carries the IP datagram. The
maximum transmission unit (MTU) of Ethernet is 1,500 bytes. This means that if
the IP datagram exceeds 1,500 bytes, then the host has to fragment the
datagram. The minimum size of the data field is
46 bytes. This means that if the IP datagram is less than 46 bytes, the data
field has to be “stuffed” to fill it out to 46 bytes. When stuffing is used,
the data passed to the network layer contains the stuffing as well as an IP
datagram. The network layer uses the length field in the IP datagram header to
remove the stuffing.</dd>

<dt>Destination address (6 bytes)</dt><dd>This field contains the MAC address of the
destination adapter. When the destination adapter receives an Ethernet frame
whose destination address is either their own address or the MAC broadcast
address, it passes the contents of the frame’s data field to the network
layer; if it receives a frame with any other MAC address, it discards the
frame.</dd>

<dt>Source address (6 bytes)</dt><dd>This field contains the MAC address of the adapter
that transmits the frame onto the LAN.</dd>

<dt>Type field (2 bytes)</dt><dd>The type field permits Ethernet to multiplex
network-layer protocols. To understand this, we need to keep in mind that
hosts can use other network-layer protocols besides IP. In fact, a given host
may support multiple network-layer protocols using different protocols for
different applications. For this reason, when the Ethernet frame arrives at
destination adapter, the dest adapter needs to know to which network-layer
protocol it should pass (demultiplex) the contents of the data field. IP and
other network-layer protocols (for example, Novell IPX or AppleTalk) each have
their own, standardized type number. Furthermore, the ARP protocol has its own
type number, and if the arriving frame contains an ARP packet , the ARP packet
will be demultiplexed up to the ARP protocol. Note that the type field is
analogous to the protocol field in the network-layer datagram and the
port-number fields in the transport-layer segment; all of these fields serve
to glue a protocol at one layer to aprotocol at the layer above.</dd>

<dt>Cyclic redundancy check (CRC) (4 bytes)</dt><dd>the purpose of the CRC field is to
allow the receiving adapter, to detect bit errors in the frame. When a frame
fails the CRC check, the receiving adapter simply discards the frame.</dd>

<dt>Preamble (8 bytes)</dt><dd><p>
The Ethernet frame begins with an 8-byte preamble
field. Each of the first 7 bytes of the preamble has a value of 10101010; the
last byte is 10101011. The first 7 bytes of the preamble serve to “wake up”
the receiving adapters and to synchronize their clocks to that of the sender’s
clock. Why should the clocks be out of synchronization? Keep in mind that the
sender adapter aims to transmit the frame at 10 Mbps, 100 Mbps, or 1 Gbps,
depending on the type of Ethernet LAN. However, because nothing is absolutely
perfect, the sender adapter will not transmit the frame at exactly the target
rate; there will always be some drift from the target rate, a drift which is
not known a priori by the other adapters on the LAN. A receiving adapter can
lock onto the sender adapter’s clock simply by locking onto the bits in the
first 7 bytes of the preamble. The last 2 bits of the eighth byte of the
preamble (the first two consecutive 1s) alert the receiving adapter that the
“important stuff” is about to come.
</p>

<p>
All of the Ethernet technologies provide connectionless service to the network
layer. (no handshake)
</p></dd>
</dl>
</div>
</div>

<div id="outline-container-org848f2c5" class="outline-5">
<h5 id="org848f2c5">Tecnologias Ethernet 535</h5>
<div class="outline-text-5" id="text-org848f2c5">
<p>
The IEEE 802.3 CSMA/CD (Ethernet) working group has standardized multiple
Ethernet technologies.
</p>

<p>
For example: 10BASE-T, 10BASE-2, 100BASE-T, 1000BASE-LX, 10GBASE-T and 40GBASE-T
</p>

<p>
The first part refers to the speed of the standard.
</p>

<p>
“BASE” refers to baseband Ethernet, meaning that the physical media only carries
Ethernet traffic;
</p>

<p>
The final part of the acronym refers to the physical media itself; Ethernet is
both a link-layer and a physical-layer specification and is carried over a
variety of physical media including coaxial cable, copper wire, and
fiber. Generally, a “T” refers to twisted-pair copper wires.
</p>

<p>
```
habla de Mb ehter y Gb ether
```
</p>

<p>
Con el uso de switches en ethernet, debido al accionar de los switches (store
and forward), la necesidad de tener direcciones mac fue dejando de tener
importancia.
</p>
</div>
</div>
</div>

<div id="outline-container-org2f42ec6" class="outline-4">
<h4 id="org2f42ec6">Switches de capa de enlace</h4>
<div class="outline-text-4" id="text-org2f42ec6">
<p>
el rol del switch es recibir frames de capa de enlace entrantes y
redireccionarlos en enlaces salientes.
</p>

<p>
el switch en si es <code>transparente</code> para los hosts y routers en la subred, esto
es, los hosts y routers envian frames a otros hosts y routers, en vez de
hacerlo a un switch.
</p>

<p>
la tasa de arribos a una interfaz de salida de un switch pueden exceder a la
capacidad del enlace de dicha interfaz. Para resolver esto, se utilizan
buffers en las interfaces de salida de los switches, justo como los routers
tienen buffers para los datagramas.
</p>
</div>

<div id="outline-container-orge856cc6" class="outline-5">
<h5 id="orge856cc6">Forwarding y Filtering</h5>
<div class="outline-text-5" id="text-orge856cc6">
<p>
<code>Filtering</code> is the switch function that determines whether a frame should be
forwarded to some interface or should just be dropped.
</p>

<p>
<code>Forwarding</code> is the switch function that determines the interfaces to which a
frame should be directed, and then moves the frame to those interfaces.
</p>

<p>
Switch filtering and forwarding are done with a switch table. The switch
table contains entries for some, but not necessarily all, of the hosts and
routers on a LAN.
</p>

<p>
An entry in the switch table contains:
</p>
<ol class="org-ol">
<li>a MAC address,</li>
<li>the switch interface that leads toward that MAC address, and</li>
<li><p>
the time at which the entry was placed in the table.
</p>

<p>
imagen 6.22
</p>

<p>
modern packet switches can be configured to forward on the basis of layer-2
destination MAC addresses (i.e., function as a layer-2 switch) or layer-3 IP
destination addresses (i.e., function as a layer-3 router).
</p>

<p>
Traditional switch tables (non-SDN context) are constructed differently from
a routers forwarding tables
</p>

<p>
when a frame arrives at the switch, its destination MAC address is indexed in
the switch table where:
</p>
<ul class="org-ul">
<li>There is no entry in the table for the destination MAC. In this case, the
switch forwards copies of the frame to the output buffers preceding <i>all</i>
interfaces address, the switch broadcasts the frame.</li>
<li>There is an entry but the frame is coming from a LAN segment that contains
adapter with that MAC address. There being no need to forward the frame to
any of the other interfaces, the switch performs the filtering function by
discarding the frame.</li>
<li>There is an entry in the table but this time the asociated interface to the
MAC is different from the LAN segment where the MAC came from. In this
case, the frame needs to be forwarded to the LAN segment attached to said
interface. The switch performs its forwarding function by putting the frame
in an output buffer that precedes that interface.</li>
</ul></li>
</ol>
</div>
</div>

<div id="outline-container-org274e04c" class="outline-5">
<h5 id="org274e04c">self learning</h5>
<div class="outline-text-5" id="text-org274e04c">
<p>
the switch table is built automatically, dynamically, and
autonomously—without any intervention from a network administrator or from a
configuration protocol.
</p>

<ol class="org-ol">
<li>The switch table is initially empty.</li>
<li><p>
For each incoming frame received on an interface, the switch stores in
its table
</p>
<ol class="org-ol">
<li>the MAC address in the frame’s source address field,</li>
<li>the interface from which the frame arrived, and</li>
<li>the current time.</li>
</ol>
<p>
In this manner the switch records in its table the LAN segment on which
the sender resides. If every host in the LAN eventually sends a frame,
then every host will eventually get recorded in the table.
</p></li>
<li><p>
The switch deletes an address in the table if no frames are received with
that address as the source address after some period of time (the aging
time).
</p>

<p>
Switches are <code>plug-and-play</code> devices because they require no intervention
from a network administrator or user. A network administrator wanting to
install a switch need do nothing more than connect the LAN segments to the
switch interfaces. The administrator need not configure the switch tables at
the time of installation or when a host is removed from one of the LAN
segments. Switches are also full-duplex, meaning any switch interface can
send and receive at the same time.
</p></li>
</ol>
</div>
</div>

<div id="outline-container-orgf85339b" class="outline-5">
<h5 id="orgf85339b">Propiedades</h5>
<div class="outline-text-5" id="text-orgf85339b">
<p>
Ventajas por sobre enlaces de broadcast (buses o hubs):
</p>
<dl class="org-dl">
<dt>Elimination of collisions</dt><dd>In a LAN built from switches (and without
hubs), there is no wasted bandwidth due to collisions! The switches buffer
frames and never transmit more than one frame ona segment at any one
time. As with a router, the maximum aggregate throughput of a switch is the
sum of all the switch interface rates.</dd>
<dt>Heterogeneous links</dt><dd>Because a switch isolates one link from another, the
different links in the LAN can operate at different speeds and can run over
different media. A switch is ideal for mixing legacy equipment with new
equipment.</dd>
<dt>Management</dt><dd>In addition to providing enhanced security, a switch also
eases network management. For example, if an adapter malfunctions and
continually sends Ethernet frames (called a jabbering adapter), a switch
can detect the problem and internally disconnect the malfunctioning
adapter. Similarly, a cable cut disconnects only that host that was using
the cut cable to connect to the switch. In the days of coaxial cable, many
a network manager spent hours “walking the line” (or more accurately,
“crawling the floor”) to find the cable break that brought down the entire
network. Switches also gather statistics on bandwidth usage, collision
rates, and traffic types, and make this information available to the
network manager.This information can be used to debug and correct problems,
and to plan how the LAN should evolve in the future.</dd>
</dl>
</div>
</div>

<div id="outline-container-org4b3fff6" class="outline-5">
<h5 id="org4b3fff6">SNIFFING A SWITCHED LAN: SWITCH POISONING</h5>
<div class="outline-text-5" id="text-org4b3fff6">
<p>
When a host is connected to a switch, it typically only receives frames that
are intended for it. For example, consider a switched LAN in Figure
6.17. When host A sends a frame to host B, and there is an entry for host B
in the switch table, then the switch will forward the frame only to host
</p>
<ol class="org-ol">
<li>If host C happens to be running a sniffer, host C will not be able to
sniff this A-to-B frame. Thus, in a switched-LAN environment (in contrast to
a broadcast link environment suchas 802.11 LANs or hub–based Ethernet LANs),
it is more difficult for an attacker to sniff frames. However, because the
switch broadcasts frames that have destination addresses that are not in the
switch table, the sniffer at C can still sniff some frames that are not
intended for C. Furthermore, a sniffer will be able sniff all Ethernet
broadcast frames with broadcast destination address FF–FF–FF–FF–FF–FF. A
well-known attack against a switch, called switch poisoning,is to send tons
of packets to the switch with many different bogus source MAC addresses,
thereby filling the switch table with bogus entries and leaving no room for
the MAC addresses of the legitimate hosts. This causes the switch to
broadcast most frames, which can then be picked up by the sniffer [Skoudis
2006]. As this attack is rather involved even for a sophisticated attacker,
switches are significantly less vulnerable to sniffing than are hubs and
wireless LANs.</li>
</ol>
</div>
</div>

<div id="outline-container-org1d6dc9c" class="outline-5">
<h5 id="org1d6dc9c">Switches vs Routers 541</h5>
<div class="outline-text-5" id="text-org1d6dc9c">
<p>
routers are store-and-forward packet switches that forward packets using
network-layer addresses. Although a switch is also a store-and-forward packet
switch, it is fundamentally different from a router in that it forwards packets
using MAC addresses
</p>

<p>
imagen 6.15 &#x2026; ?
</p>

<p>
imagen 6.24
</p>
</div>

<div id="outline-container-org7b8a4ce" class="outline-6">
<h6 id="org7b8a4ce">Switches</h6>
<div class="outline-text-6" id="text-org7b8a4ce">
</div>
<div id="outline-container-org88eacc8" class="outline-7">
<h7 id="org88eacc8">Ventajas</h7>
<div class="outline-text-7" id="text-org88eacc8">
<ul class="org-ul">
<li>plug and play</li>
<li>relatively high filtering and forwarding rates
<ul class="org-ul">
<li></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-org27eeee7" class="outline-7">
<h7 id="org27eeee7">Desventajas</h7>
<div class="outline-text-7" id="text-org27eeee7">
<ul class="org-ul">
<li>limits network topology to a tree, otherwise, broadcast frames could cycle
infinitely. (frames do not have ttl)</li>
<li>a large network requires a large ARP table in hosts and routers</li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orged93c42" class="outline-6">
<h6 id="orged93c42">Routers</h6>
<div class="outline-text-6" id="text-orged93c42">
</div>
<div id="outline-container-org2a7cbcd" class="outline-7">
<h7 id="org2a7cbcd">Ventajas</h7>
<div class="outline-text-7" id="text-org2a7cbcd">
<ul class="org-ul">
<li>does not limit the topology. because of ttl of the packet and the addressing
is hierarchical</li>
<li>provide firewall protection against broadcast stomrs</li>
</ul>
</div>
</div>

<div id="outline-container-org51e101d" class="outline-7">
<h7 id="org51e101d">Desventajas</h7>
<div class="outline-text-7" id="text-org51e101d">
<ul class="org-ul">
<li>not plug and play. needs IP address to be configured</li>
<li>packet-processing-time is larger for routers than for switches</li>
</ul>
</div>
</div>
</div>


<div id="outline-container-org382aa3d" class="outline-6">
<h6 id="org382aa3d">.</h6>
<div class="outline-text-6" id="text-org382aa3d">
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-left">Hubs</td>
<td class="org-left">Routers</td>
<td class="org-left">Switches</td>
</tr>

<tr>
<td class="org-left">Traffic isolation</td>
<td class="org-left">No</td>
<td class="org-left">Yes</td>
<td class="org-left">Yes</td>
</tr>

<tr>
<td class="org-left">Plug and play</td>
<td class="org-left">Yes</td>
<td class="org-left">No</td>
<td class="org-left">Yes</td>
</tr>

<tr>
<td class="org-left">Optimal routing</td>
<td class="org-left">No</td>
<td class="org-left">Yes</td>
<td class="org-left">No</td>
</tr>
</tbody>
</table>
</div>
</div>
</div>
</div>


<div id="outline-container-org88b9d0d" class="outline-4">
<h4 id="org88b9d0d">VLAN</h4>
<div class="outline-text-4" id="text-org88b9d0d">
<p>
imagen 6.15
</p>

<p>
desventajas encontradas en la configuracion de la imagen
</p>
<dl class="org-dl">
<dt>Lack of traffic isolation</dt><dd>Although the hierarchy localizes group traffic to
within a single switch, broadcast traffic must still traverse the entire
institutional network. Limiting the scope of such broadcast traffic would
improve LAN performance. Perhaps more importantly, it also may be desirable to
limit LAN broadcast traffic for security/privacy reasons. This type of
isolation could be provided by replacing the center switch in Figure 6.15 with
a router. We’ll see shortly that this isolation also can be achieved via a
layer-2-switch solution.</dd>
<dt>Inefficient use of switches</dt><dd>If instead of three groups, the institution had
10 groups, then 10 first-level switches would be required. If each group were
small, say less than 10 people, then a single 96-port switch would likely be
large enough to accommodate everyone, but this single switch would not provide
traffic isolation.</dd>
<dt>Managing users</dt><dd><p>
If an employee moves between groups, the physical cabling
must be changed to connect the employee to a different switch in Figure
6.15. Employees belonging to two groups make the problem even harder
</p>

<p>
Fortunately, each of these difficulties can be handled by a switch that supports
<code>virtual local areanetworks (VLANs)</code>.
</p>

<p>
a switch that supports VLANs allows multiple virtual lans to be defined over a
single physical lan infrastructure. Hosts within a VLAN communicate with each
other as if they (and no other hosts) were connected to the switch. In a
<code>port-based</code> VLAN, the switch’s ports (interfaces) are divided into groups by
the network manager. Each group constitutes a VLAN, with the ports in each VLAN
forming a broadcast domain (i.e., broadcast traffic from one port can only reach
other ports in the group).
</p>

<p>
Figure 6.25 shows a single switch with 16 ports. Ports 2 to 8 belong to the EE
VLAN, while ports 9 to 15 belong to the CS VLAN (ports 1 and 16 are
unassigned). This VLAN solves all of the difficulties noted above — EE and CS
VLAN frames are isolated from each other, the two switches in Figure 6.15 have
been replaced by a single switch, and if the user at switch port 8 joins the CS
Department, the network operator simply reconfigures the VLAN software so that
port 8 is now associated with the CS VLAN. One can easily imagine how the VLAN
switch is configured and operates — the network manager declares a port to
belong to a given VLAN (with undeclared ports belonging to a default VLAN) using
switch management software, a table of port-to-VLAN mappings is maintained
within the switch; and switch hardware only delivers frames between ports
belonging to the same VLAN.
</p>

<p>
imagen 6.25
</p>

<p>
How can traffic from the EE Department be sent to the CS Department? One way to
handle this would be to connect a VLAN switch port (e.g., port 1 in Figure 6.25)
to an external router and configure that port to belong both the EE and CS
VLANs. In this case, even though the EE and CS departments share the same
physical switch, the logical configuration would look as if the EE and CS
departments had separate switches connected via a router. An IP datagram going
from the EE to the CS department would first cross the EE VLAN to reach the
router and then be forwarded by the router back over the CS VLAN to the CS host.
</p></dd>
</dl>


<p>
Suppose now that members of EE VLAN and CS VLAN are in a different building, but
want to be connected to their respective department VLAN. A way to interconnect
VLAN switches is known as <code>VLAN Trunking</code> (imagen 6.26 b).
</p>

<p>
imagen 6.26
</p>

<p>
A special port on each switch (port 16 on the left switch and port 1 on the
right switch) is configured as a <code>trunk port</code> to interconnect the two VLAN
switches. The trunk port belongs to all VLANs, and frames sent to any VLAN are
forwarded over the <code>trunk link</code> to the other switch.
</p>

<p>
(Este puerto especial es para hacer broadcast a todos los switches. Luego el
switch que recibe se encarga de hacer broadcast a los puertos de la VLAN
correspondiente)
</p>

<p>
How does a switch know that a frame arriving on a trunk port belongs to a
particular VLAN? The IEEE has defined an extended Ethernet frame format,
802.1Q, for frames crossing a VLAN trunk.
</p>

<p>
imagen 6.27
</p>

<p>
the 802.1Q frame consists of the standard Ethernet frame with a four-byte <code>VLAN
tag</code> added into the header that carries the identity of the VLAN to which the
frame belongs. The VLAN tag is added into a frame by the switch at the sending
side of a VLAN trunk, parsed, and removed by the switch at the receiving side of
the trunk. The VLAN tag consists of
</p>
<ul class="org-ul">
<li>a 2-byte Tag Protocol Identifier (TPID) field (with a fixed hexadecimal value
of 81-00),</li>
<li>a 2-byte Tag Control Information field that contains a 12-bit VLAN identifier
field, and</li>
<li>a 3-bit priority field that is similar in intent to the IP datagram TOS field.</li>
</ul>


<p>
Tambien existen la VLANs basadas en MAC. El administrador de red, configura un
conjunto de direcciones MAC que pertenecen a una VLAN.
</p>

<p>
Otra forma de definir VLANs es basada en protocolos de capa de red (IPv4, IPv6,
Appletalk) y otros criterios.
</p>
</div>
</div>
</div>

<div id="outline-container-orgafba3ef" class="outline-3">
<h3 id="orgafba3ef">Virtualizacion de enlaces 548</h3>
<div class="outline-text-3" id="text-orgafba3ef">
</div>
<div id="outline-container-org54ca15d" class="outline-4">
<h4 id="org54ca15d">Multiprotocol Label Switching (MPLS)</h4>
<div class="outline-text-4" id="text-org54ca15d">
<p>
The goal was not to abandon the destination-based IP datagram-forwarding
infrastructure for one based on fixed-length labels and virtual circuits, but to
augment it by selectively labeling datagrams and allowing routers to forward
datagrams based on fixed-length labels (rather than destination IP addresses)
when possible. Importantly, these techniques work hand-in-hand with IP, using IP
addressing and routing. The IETF unified these efforts in the MPLS protocol
[RFC 3031, RFC 3032], effectively blending VC techniques into a routed datagram
network.
</p>

<p>
imagen 6.28
</p>

<p>
La imagen muestra un frame que es transmitido entre dispositivos que soportan
MPLS. Contiene un header MPLS entre los headers de capa 2 y 3 (Ethernet e IP
respectivamente).
</p>

<p>
Dentro del header MPLS se encuetra:
</p>
<ul class="org-ul">
<li>el label</li>
<li>3 bits para uso experimental</li>
<li>1 bit &ldquo;S&rdquo; para indicar el fin de una serie de header MPLS &ldquo;apilados&rdquo;</li>
<li><p>
campo ttl
</p>

<p>
Un router que soporta MPLS se lo llama <code>label-switched router</code> ya que envia
frames MPLS con solo observar el label MPLS y buscarlo en la forwarding table.
Es decir, no extrae la IP de destino y busca el LPM en la forwarding table.
</p>

<p>
Como sabe el router que su vecino soporta MPLS? Como asocia un label con una
direccion IP?
</p>

<p>
imagen 6.29
</p>

<ul class="org-ul">
<li>routers R1 through R4 are MPLS capable.</li>
<li>R5 and R6 are standard IP routers.</li>
<li>R1 has advertised to R2 and R3 that it (R1) can route to destination A, with
MPLS label 6.</li>
<li>Router R3 has advertised to router R4 that it can route to destinations A and
D, with labels 10 and 12 respectively.</li>
<li><p>
Router R2 has also advertised to router R4 that it (R2) can reach destination
A, with a MPLS label 8.
</p>

<p>
Note that router R4 has two MPLS paths to reach A.
</p>

<p>
R5, R6, A y D, estan conectados mediante la infraestructura MPLS, tanto como una
switched LAN y red de ATMs que conectan dispositivos IP, y tanto como estos, los
routers que soportan MPLS lo hacen sin tocar el header IP de un paquete.
</p>

<p>
En el libro no se ve el protocolo utilizado por los routers MPLS para distribuir
los labels.
</p>

<p>
ver [RFC 3468] [RFC 3209].
</p>

<p>
tampoco se especifica como se calculan los caminos por los cuales los paquetes
deben fluir dentro de la red de routers MPLS, ni tampoco como se recolecta la
informacion de los enlaces en dichos caminos.
</p>

<p>
Lo mas importante de MPLS es la capacidad de manejo de trafico que permite
realizar. El protocolo IP especificaria un solo camino de costo minimo por el
cual un paquete debe seguir, pero MPLS provee varias rutas. Tambien se puede
usar MPLS para restaurar caminos de forma mas rapida en caso de haber fallas de
enlace.
</p>

<p>
tambien se usan para implementar VPNs y separar el trafico de clientes de VPN y
clientes regulares.
</p></li>
</ul></li>
</ul>
</div>
</div>
</div>

<div id="outline-container-orgdb3e68a" class="outline-3">
<h3 id="orgdb3e68a">Netwroking de Data Centers 552</h3>
<div class="outline-text-3" id="text-orgdb3e68a">
<p>
Each data center has its own <code>data center network</code> that interconnects its hosts
with each other and interconnects the data center with the Internet.
</p>

<p>
The worker bees in a data center are the hosts: They serve content (e.g., Web
pages and videos), store e-mails and documents, and collectively perform
massively distributed computations (e.g., distributed index computations for
search engines). The hosts in data centers, called <code>blades</code>, are generally
commodity hosts that include CPU, memory, and disk storage. The hosts are
stacked in racks, with each rack typically having 20 to 40 blades.
</p>

<p>
At the top of each rack there is a switch, named the <code>Top of Rack (TOR) switch</code>,
that interconnects the hosts in the rack with each other and with other switches
in the data center.
</p>

<p>
Each host is also assigned its own data-center-internal IP address.
</p>

<p>
two types of traffic:
</p>
<ol class="org-ol">
<li>traffic flowing between external clients and internal hosts the data center
network includes one or more <code>border routers</code> to handle this type of
traffic. The data center network therefore interconnects the racks with each
other and connects the racks to the border routers.</li>
<li><p>
traffic flowing between internal hosts.
</p>

<p>
imagen 6.30
</p></li>
</ol>
</div>

<div id="outline-container-org42a6965" class="outline-4">
<h4 id="org42a6965">Balance de Carga</h4>
<div class="outline-text-4" id="text-org42a6965">
<p>
Los data centers proveen varias aplicaciones de forma concurrente. para
soportar multiples pedidos de clientes externos, cada aplicacion esta
asociada a una IP publica. dentro del data center, las consultas se redirigen
a un <code>load balancer</code> para que distribuya las consultas entre los
hosts. Tambien se lo llama <code>layer 4 switch</code> porque puede tomar desiciones de
forwardeo basado en el puerto destino (capa 4) y tambien por la ip de destino
del paquete (capa 3). Luego de que el host procese la consulta, se la envia
de vuelta al load balancer, para que este se lo devuelva al cliente.
</p>

<p>
el load balancer tambien funciona como NAT, al traducir las ip publicas a
direcciones ip internas
</p>
</div>
</div>

<div id="outline-container-org5cdf792" class="outline-4">
<h4 id="org5cdf792">Arquitectura Jerarquica</h4>
<div class="outline-text-4" id="text-org5cdf792">
<p>
imagen 6.30
</p>

<p>
At the top of the hierarchy, the border router connects to access routers. Below
each access router there are three tiers of switches. Each access router
connects to a top-tier switch, and each top-tier switch connects to multiple
second-tier switches and a load balancer. Each second-tier switch in turn
connects to multiple racks via the racks’ TOR switches (third-tier
switches). All links typically use Ethernet for their link-layer and
physical-layer protocols, with a mix of copper and fiber cabling.
</p>

<p>
Como es necesario para aplicaciones en la nube proveer servicios de forma
continua, los data centers tambien incluyen equipo y enlaces redundantes. For
example, each TOR switch can connect to two tier-2 switches, and each access
router, tier-1 switch, and tier-2 switch can be duplicated and integrated into
the design.
</p>

<p>
observe that the hosts below each access router form a single subnet. In order
to localize ARP broadcast traffic, each of these subnets is further partitioned
into smaller VLAN subnets.
</p>

<p>
La arquitectura de la imagen resuelve el problema de escalabilidad, pero sufre
de limitacion en capacidad entre hosts (host-to-host capacity). Puede haber
multiples conexiones simultaneas entre hosts de un lado de la red hacia el otro,
pero la velocidad del enlace de los switches y routers es limitante. Una
solucion es obtener switches y routers de mayor velocidad, pero esto es mas
costoso.
</p>
</div>
</div>

<div id="outline-container-orgf79e8db" class="outline-4">
<h4 id="orgf79e8db">Tendencias</h4>
<div class="outline-text-4" id="text-orgf79e8db">
<p>
Para reducir costos y mejorar el delay y throughput, se crean nuevas
arquitecturas y protocolos de red.
</p>
</div>

<div id="outline-container-org496f60f" class="outline-5">
<h5 id="org496f60f">Fully Connected Topology</h5>
<div class="outline-text-5" id="text-org496f60f">
<p>
Un metodo es reemplazar la jerarquia de switches y routers por una <code>fully
connected topology</code>.
</p>

<p>
imagen 6.31
</p>

<p>
cada switch de tier 1 se conecta con todos los switches de tier 2 para:
</p>
<ol class="org-ol">
<li>que el trafico host-to-host no suba por mas arriba que el tier de switches</li>
<li>con \(n\) switches de tier 1, hay \(n\) caminos disjuntos entre 2 switches de
tier 2 cualesquiera.</li>
</ol>
</div>
</div>

<div id="outline-container-orge959bdf" class="outline-5">
<h5 id="orge959bdf">Modular Data Center (MDC)</h5>
<div class="outline-text-5" id="text-orge959bdf">
<p>
Se crean pequeños data centers dentro de contenedores de 12 metros y se envian a
la ubicacion del data center, en la cual se interconectan los contenedores entre
si y la internet. Una vez desplegado, un contenedor es dificil de mantener por
lo que estan diseñados para operar con menor performance a medida que se van
degradando.
</p>

<p>
con este metodo, hay dos tipos de redes:
</p>
<ol class="org-ol">
<li>las redes dentro de los contenedores y</li>
<li>la red que conecta a cada contenedor</li>
</ol>

<p>
la dificultad se encuetra en el 2do tipo de red, ya que hay problemas para
proveer un alto ancho de banda para conexiones host-to-host.
</p>


<p>
en topologias altamente interconectadas, el diseño de algoritmos de ruteo entre
switches se vuelve un problema. soluciones: (1) random routing, (2) multiples
NICs en cada host y conectar cada interfaz a un switch de bajo costo y dejar que
los hosts resulevan el ruteo del trafico entre los switches.
</p>
</div>
</div>
</div>
</div>
</div>

<div id="outline-container-org5981f63" class="outline-2">
<h2 id="org5981f63">Redes Moviles</h2>
<div class="outline-text-2" id="text-org5981f63">
</div>
<div id="outline-container-org9e6f674" class="outline-3">
<h3 id="org9e6f674"><span class="done DONE">DONE</span> introduccion</h3>
<div class="outline-text-3" id="text-org9e6f674">
<p>
imagen 7.1
</p>

<p>
Elementos en una red inalambrica:
</p>
<dl class="org-dl">
<dt>Hosts inalambricos</dt><dd>son dispositivos end-system que ejecutan
aplicaciones. puede ser una laptop, tablet, smartphone, pc. pueden no ser
mobiles en si.</dd>
<dt>Enlaces inalambricos</dt><dd>un host se conecta a una <code>base station</code> o a otro host
inalambrico mediante un <code>enlace de comunicacion inalambrico</code>. Different
wireless link technologies have different transmission rates and can transmit
over different distances.</dd>
<dt>Base station</dt><dd><p>
no tiene equivalente en redes alambricas. es responsable de
enviar y reciir datos (paquetes) desde y hacia un host inalambrico que esta
asociado a esa base station. tambien es responsable de coordinar la
transmicion de multiples hosts inalambricos a los cuales esta asociada, que
significa que:
</p>
<ol class="org-ol">
<li>el host esta dentro del rango de comunicacion de la base station</li>
<li>el host utiliza dicha base station para entregar datos entre el host y el
resto de la red.</li>
</ol>
<p>
unos ejemplos de base stations son: torres de celulares y access points en
802.11.
</p>

<p>
los hosts asociados a una base station , se les dice que operan en <code>modo
  infraestructura</code>, ya que los servicios tradicionales (ej: asignacion de
direccion y ruteo) son provistos por la red a la cual la base station esta
conectada.
</p>

<p>
En redes <code>ad hoc</code>, los hosts inalambricos no tienen tal infraestructura a la
cual conectarse. en este caso, los hosts mismos son responsables de proveer
estos servicios, tales como routeo, asignacion de direcciones, traduccion de
nombre DNS, etc.
</p>

<p>
cuando un host inalambrico se mueve del rango de una base station y entra en el
rango de otra, cambia el punto de ingreso a la red, proceso conocido como
<code>handoff</code>.
</p></dd>

<dt>Infraestructura de red</dt><dd>red mas grande a la cual un host inalambrico desea
conectarse.</dd>
</dl>


<p>
PUBLIC WIFI ACCESS: COMING SOON TO A LAMP POST NEAR YOU?
</p>


<p>
clasificacion de redes inalambricas:
</p>
<ul class="org-ul">
<li>si un paquete en la red realiza exactamente un solo salto o multiples saltos</li>
<li>si existe infraestructura (como una base station) en la red</li>

<li id="un salto, con infraestructura">redes con una base station conectada a una
red alambrica mas grande (ej: internet). toda comunicacion es entre la base
station y el host inalambrico (sin intermediarios). Ej: aula, cafe,
biblioteca, red celular 4G LTE.</li>
<li id="un salto, sin infraestructura">no hay base statios, sin embargo un nodo
puede coordinar las transmiciones de otros nodos. Ej: redes bluetooth y redes
802.11 en modo ad hoc</li>
<li id="multiples saltos, con infraestructura">la base station esta conectada al
resto de la red, pero algunos nodos pueden llegar a depender de otros nodos
inalambricos intermediarios para poder comunicarse a la base station. Ej:
redes de sensores inalambricos, <code>wireless mesh networks</code>.</li>
<li id="multiples saltos, sin infraestructura">no hay base station y los nodos deben
depender de otros para poder llegar al destino. Los nodos pueden ser mobiles,
por lo que la conectividad puede cambiar, un tipo de redes conocido como
<code>mobile ad hoc networks (MANETs)</code>.</li>
</ul>
</div>
</div>

<div id="outline-container-org08c9107" class="outline-3">
<h3 id="org08c9107"><span class="todo TODO">TODO</span> Enlaces inalambricos y caracteristicas de la red</h3>
<div class="outline-text-3" id="text-org08c9107">
<p>
Diferencias entre enlaces alambricos e inalambricos:
</p>
<dl class="org-dl">
<dt>Decresiente intensidad de señal</dt><dd>la radiacion electromagnetica se atenua
cuando pasa a traves de materia e incluso al solamente propagarse en el vacio.</dd>
<dt>Interferencia con otras fuentes</dt><dd>fuentes de transmicion en la misma
frecuencia interfieren unas con otras.</dd>
<dt>Propagacion Multipath</dt><dd>ocurre cuando porciones de la onda electromagnetica
se reflejan sobre ojetos y el suelo (tierra?), tomando caminos de distancias
diferentes entre el que envia y el que recibe. esto provoca que se reciba una
señal distorsionada.</dd>
</dl>

<p>
Por estas caracteristicas mencionadas, los errores son mas comunes en enlaces
inalambricos que en los alambricos.
</p>

<p>
Nos enfocamos ahora en como un host recibe una señal electromagnetica. esta
señal es una combinacion de la señal orinal transmitida por el emisor y ruido
del medio. se utiliza la relacion <code>señal a ruido</code> (<code>signal-to-noise ratio
(SNR)</code>) para medir las intensidades relativas.
</p>

<p>
imagen 7.3
</p>

<p>
imagen 7.4
</p>

<p>
bit error rate (BER) , probabilidad de que un bit transmitido es recibido con
error
</p>

<dl class="org-dl">
<dt>For a given modulation scheme, the higher the SNR, the lower the BER</dt><dd>Since
a sender can increase the SNR by increasing its transmission power, a sender
can decrease the probability that a frame is received in error by increasing
its transmission power. Note, however, that there is arguably little practical
gain in increasing the power beyond a certain threshold, say to decrease the
BER from to . There are also disadvantages associated with increasing the
transmission power: More energy must be expended by the sender (an important
concern for battery-powered mobile users), and the sender’s transmissions are
more likely to interfere with the transmissions of another sender (see Figure
7.4(b)).</dd>
<dt>For a given SNR, a modulation technique with a higher bit transmission rate (whether in error or not) will have a higher BER</dt><dd>For
example, in Figure 7.3, with an SNR of 10 dB, BPSK modulation with a
transmission rate of 1 Mbps has a BER of less than , while with QAM16
modulation with a transmission rate of 4 Mbps, the BER is , far too high to be
practically useful. However, with an SNR of 20 dB, QAM16 modulation has a
transmission rate of 4 Mbps and a BER of , while BPSK modulation has a
transmission rate of only 1 Mbps and a BER that is so low as to be (literally)
“off the charts.” If one can tolerate a BER of , the higher transmission rate
offered by QAM16 would make it the preferred modulation technique in this
situation. These considerations give rise to the final characteristic,
described next.</dd>
<dt>Dynamic selection of the physical-layer modulation technique can be used to adapt the modulation technique to channel conditions</dt><dd>The
SNR (and hence the BER) may change as a result of mobility or due to changes
in the environment. Adaptive modulation and coding are used in cellular data
systems and in the 802.11 WiFi and 4G cellular data networks . This allows,
for example, the selection of a modulation technique that provides the highest
transmission rate possible subject to a constraint on the BER, for given
channel characteristics.</dd>
</dl>

<p>
A higher and time-varying bit error rate is not the only difference between a
wired and wireless link.
</p>


<p>
en enlaces inalambricos, no todos los nodos pueden recibir transmiciones de
otros nodos.
</p>

<p>
Suppose that Station A is transmitting to Station B. Suppose also
that Station C is transmitting to Station B.
</p>

<ol class="org-ol">
<li>se llama <code>hidden terminal problem</code> cuando obstrucciones fisicas en el medio
(montaña, eficio), no permiten que A y C escuchen las transmiciones del otro,
a pesar de que sus señales se interfieren en el destino B. (7.4 a)</li>
<li>otro escenario que resulta en que el enlace no sea detectable ocurre cuando
la señal que se progapa, se ve atenuada debido a la distancia entre los
nodos. esto se conoce como <code>fading</code> (atenuacion) (7.4 b)</li>
</ol>
</div>

<div id="outline-container-orgd517635" class="outline-4">
<h4 id="orgd517635"><span class="todo TODO">TODO</span> Code Division Multiple Access (CDMA) :protocolo de acceso al medio:</h4>
<div class="outline-text-4" id="text-orgd517635">
<p>
pertenece a la familia de protocolos de particion de canal (entre acceso
aleatorio y por turnos).
</p>

<p>
en CDMA cada bit enviado esta codificado al multiplicar el bit por la señal (el
codigo) que cambia a una velocidad mucho mas grande (<code>chipping rate</code>) que la
secuencia de bits original
</p>

<p>
imagen 7.5
</p>

<p>
con la presencia de emisores que interfieren entre si, CDMA permite poder
discernir una señal codificada de otra.
</p>

<p>
CDMA asume que las señales transmitidas son aditivas, esto es, por ejemplo, si 3
emisores envian un 1, y cuarto emisor envia -1 durante el mismo mini-slot,
entoces la señal recibida en todos los receptores en dicho mini-slot es 2
(1+1+1-1=2).
</p>

<p>
&#x2026;
</p>
</div>
</div>
</div>

<div id="outline-container-org3d0feab" class="outline-3">
<h3 id="org3d0feab"><span class="done DONE">DONE</span> WiFi: 802.11 Wireless LANs</h3>
<div class="outline-text-3" id="text-org3d0feab">
<p>
existen diferentes estandares dentro de la familia del protocolo 802.11 (Wifi)
</p>

<p>
utilizan la misma estructura la el frame de capa de enlace, pueden reducir la
velocidad de transmicion para extender el alcance, son compatibles con versiones
anteriores.
</p>

<p>
sin embargo tienen diferencias en la capa fisica. operan en dos rangos de
frecuencias 2.4-2.485 GHz y 5.1-5.8 GHz.
</p>

<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">Standard</th>
<th scope="col" class="org-left">Frequency range</th>
<th scope="col" class="org-left">data range</th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">802.11b</td>
<td class="org-left">2.4 GHz</td>
<td class="org-left">up to 11 Mbps</td>
</tr>

<tr>
<td class="org-left">802.11a</td>
<td class="org-left">5 GHz</td>
<td class="org-left">up to 54 Mbps</td>
</tr>

<tr>
<td class="org-left">802.11g</td>
<td class="org-left">2.4 GHz</td>
<td class="org-left">up to 54 Mbps</td>
</tr>

<tr>
<td class="org-left">802.11n</td>
<td class="org-left">2.5 GHz and 5 GHz</td>
<td class="org-left">up to 450 Mbps</td>
</tr>

<tr>
<td class="org-left">802.11ac</td>
<td class="org-left">5 GHz</td>
<td class="org-left">up to 1300 Mbps</td>
</tr>
</tbody>
</table>
</div>

<div id="outline-container-orgc60f728" class="outline-4">
<h4 id="orgc60f728">Architecture</h4>
<div class="outline-text-4" id="text-orgc60f728">
<p>
una pieza fundamental es el <code>basic service set (BSS)</code>. un bss contiene una o mas
estaciones inalambricas y una base station central, conocido como <code>access point
(AP)</code>.
</p>

<p>
imagen 7.7
</p>

<p>
una lan inalambrica con un AP se la llama tambien <code>infrastructure wireless lan</code>,
con infrastructure refiriendose al AP y su conexion con la internet.
</p>

<p>
la imagen 7.8 muestra una red ad hoc, una red si un control central ni
conexiones con el exterior.
</p>
</div>

<div id="outline-container-org22ad801" class="outline-5">
<h5 id="org22ad801">Canales y Asociacion</h5>
<div class="outline-text-5" id="text-org22ad801">
<p>
en el protocolo 802.11 cada estacion inalambrica debe estar asociada a un AP
antes de enviar o recibir datos de capa de red.
</p>

<p>
cuando se instala un AP, se asigna un <code>Service Set Identifier (SSID)</code> al
AP. tambien se debe asignar un numero de canal al AP. dentro del rango de
frecuencia 2.4 GHz, se definen 11 canales parcialmente solapados. Dos canales no
estan solapados si y solo si estan separados por 4 o mas canales.
</p>


<p>
Una <code>Wifi jungle</code> es una ubicacion fisica en donde una estacion inalambrica
recibe señales suficientemente fuertes de dos o mas APs. para poder unirse una
subred del AP, una estacion debe <code>asociarse</code> al AP. se debe crear un enlace
virtual.
</p>


<p>
el protocolo 802.11 requiere que los AP envien <code>beacon frames</code> de forma
periodica que contienen el SSID y MAC del AP. El dispositivo escanea los 11
canales buscando beacon frames.
</p>

<p>
el proceso de escanear canales y escuchar por beacon frames se conoce como
<code>passive scanning</code>
</p>

<p>
imagen 7.9 a
</p>

<p>
Un dispositivo inalambrico tambien puede realizar <code>active scanning</code>, al hacer
broadcast de un <code>probe frame</code> que es recibido por todos los APs dentro del rango
del dispositivo. Los APs responden al frame y el dispositivo elige entre uno de
ellos.
</p>

<p>
imagen 7.9 b
</p>

<p>
Luego de seleccionar el AP al cual asociarse, el dispositivo envia al AP un
<code>asociation request frame</code> y el AP le responde. (parecido a DHCP)
</p>

<p>
Luego, Tipicamente para unirse a la subred del AP, el dispositivo envia un
mensaje DHCP.
</p>

<p>
En algunos casos se quiere que el dispositivo se autenticado por el AP. esto se
puede realizar mediante:
</p>
<ul class="org-ul">
<li>validacion de MAC</li>
<li>usuario y contraseña</li>
</ul>

<p>
en ambos casos el AP puede comunicarse con un servidor autenticador separado,
para poder proveer a varios APs simultaneamente, centralizando la informacion
sensible, matener la complejidad y costo del AP bajo.
</p>
</div>
</div>
</div>

<div id="outline-container-orgbf169ef" class="outline-4">
<h4 id="orgbf169ef">Protocolo MAC 802.11</h4>
<div class="outline-text-4" id="text-orgbf169ef">
<p>
como protocolo de acceso al medio compartido, los diseñadores de 802.11 optaron
por uno de tipo de acceso aleatorio llamado <code>Carrier Sense Multiple Access
(CSMA) with Collision avoidance (CSMA/CA)</code>. Cada estacion escucha el enlace
antes de transmitir y espera mientras este ocupado.
</p>

<p>
diferencias entre protocolos MAC de Ethernet y 802.11:
</p>
<ol class="org-ol">
<li>ethernet utiliza <code>collision detection</code>, 802.11 utiliza <code>collision avoidance</code></li>
<li>Wifi utiliza un esquema de <code>acknowledgement/retransmission (ARQ)</code> de capa de
enlace.</li>
</ol>


<p>
ethernet utiliza <code>collision detection</code>, lo que significa que un dispositivo
transmite y escucha el canal al mismo tiempo. si escucha ruido, entonces para de
transmitir y espera un tiempo aleatorio para volver a hacerlo.
no se utiliza <code>collision detection</code> de ethernet porque:
</p>
<ul class="org-ul">
<li>la señal recibida es en general mas atenuada con respecto a la señal de
transmicion, por que hacer que el dispositivo haga ambos al mismo tiempo es
mas costoso.</li>
<li>se podria no detectar una colision debido a <code>hidden terminal problem</code> y
<code>fading</code>.</li>
</ul>


<p>
en wifi, una vez que se empieza a transmitir un frame, se hace por completo.
</p>
</div>

<div id="outline-container-orgc877a71" class="outline-5">
<h5 id="orgc877a71">esquema de acknowledgement de capa de enlace</h5>
<div class="outline-text-5" id="text-orgc877a71">
<p>
debido a que un frame puede no llegar intacto a su destino, se utilizan
acknowledgement a nivel de capa de enlace. Cuando se recibe un frame que pasa
CRC (Cyclic Redundancy Check), se espera una cierta cantidad de tiempo <code>Short
Inter-frame Spacing (SIFS)</code> y luego responde con un acknowledgement
</p>

<p>
si la estacion que transmite no recibe un acuse de recibo dentro de una cierta
cantidad de tiempo, asume que ocurrio un error y retransmite el frame utilizando
CSMA/CA. Si no recibe acuse de recibo luego de retransmitir varias veces, se
descarta el frame.
</p>
</div>
</div>

<div id="outline-container-org166bd53" class="outline-5">
<h5 id="org166bd53">CSMA/CA</h5>
<div class="outline-text-5" id="text-org166bd53">
<ol class="org-ol">
<li>si el canal esta libre, se transmite al frame luego de un periodo de tiempo
corto conocido como <code>Distributed Inter-frame Space (DIFS)</code>.</li>
<li>en otro caso, se elige un tiempo aleatorio de espera utilizando binary
exponential backoff. espera a que es canal este libre, espera DIFS, espera el
tiempo aleatorio determinado. si se detecta señal en el canal, se congela el
timer.</li>
<li>cuando el tiempo de espera llega a cero, se retransmite el frame por completo
y espera por un acuse de recibo.</li>
<li>si se tiene otro frame para transmitir, empieza CSMA/CA desde el paso 2. si
no se recibe acknowledgement, se vuelve al paso 2 con bin-exp-backoff mas
grande.</li>
</ol>

<p>
por que se espera un tiempo mas luego de detectar a que el canal este libre?
si se transmite al momento de detectar al canal libre, se puede generar una
colision.
debido a que wifi no detecta colision y luego aborta la transmicion, un frame
que sufre de colision seria transmitido por completo.
</p>
</div>
</div>
</div>

<div id="outline-container-org75ecc44" class="outline-4">
<h4 id="org75ecc44">Hidden terminals: RTS y CTS</h4>
<div class="outline-text-4" id="text-org75ecc44">
<p>
existe un esquema opcional que ayuda a prevenir colisiones incluso con la presencia de
hidden terminals.
</p>

<p>
imagen 7.11
</p>

<p>
si ambos hosts de la imagen transmiten al mismo tiempo, ocurre una colision en
la cercania del AP y ninguno de los hosts se da cuenta.
</p>

<p>
para evitar el problema, se usan los frames de control <code>Request to Send (RTS)</code> y
<code>Clear to Send (CTS)</code> para reservar el acceso al canal. si se quiere transmitir,
se envia un RTS al AP indicando el tiempo requierido para transmitir sus datos y
un frame de acuse de recibo (?). Luego el AP recibe el RTS y responde con un CTS
explicitando que el dispositivo puede transmitir y los demas deben esperar por
la duracion que pidio el transmisor.
</p>

<p>
imagen 7.12
</p>

<p>
la performance puede mejorar en dos formas:
</p>
<ol class="org-ol">
<li>se mitiga el problema del hidden terminal, ya que los frames largos son
transmitidos luego de que el canal sea reservado.</li>
<li>porque los frames de control son cortos, la duracion de la colision de frames
RTS y CTS es corta</li>
</ol>

<p>
a pesar de ayudar a reducir colisiones, puede introducir delays y consume
recursos del canal. es por esto que en general se utiliza RTS/CTS para el envio
de frames largos.
</p>
</div>
</div>

<div id="outline-container-org4632435" class="outline-4">
<h4 id="org4632435">802.11 como enlace punto a punto</h4>
<div class="outline-text-4" id="text-org4632435">
<p>
si dos nodos cuentan con antenas direccionales, pueden apuntarse entre si y
formar un enlace punto a punto.
</p>
</div>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1">1</a></sup> <div class="footpara"><p class="footpara"></p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
Last update: 2020-09-27 13:58
</div>
</body>
</html>
