<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-09-16 Wed 14:52 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Procesos Estocásticos</title>
<meta name="generator" content="Org mode" />
<link rel="stylesheet" type="text/css" href="/res/org.css"/>
<script type="text/javascript" src="/res/org-info.js"></script>
<script type="text/javascript" src="../res/org-info.js"></script>
<link rel="stylesheet" type="text/css" href="../res/org.css"/>
</head>
<body>
<div id="content">

<div id="outline-container-orgb7c2364" class="outline-1">
<h1 id="orgb7c2364"><span class="section-number-1">1</span> Introducción</h1>
<div class="outline-text-1" id="text-1">
</div>
<div id="outline-container-org68e2103" class="outline-2">
<h2 id="org68e2103"><span class="section-number-2">1.1</span> Qué es un proceso estocástico o aleatorio?</h2>
<div class="outline-text-2" id="text-1-1">
<p>
Cuando una magnitud de interés S(t) depende del tiempo t y, a su vez, es de
naturaleza aleatoria, tenemos un proceso estocástico o aleatorio unidimensional
(temporal).
</p>

<p>
S(t)
</p>

<p>
t
Tiempo continuo
</p>

<p>
S(tn)
</p>

<p>
t
Tiempo discreto
t0 t1 t2 t3
</p>

<p>
tn
</p>

<p>
La variable aleatorio S(t) o S(tn) puede ser a su vez continua o discreta.
</p>

<p>
Ejemplos de procesos 1D:
</p>
<ul class="org-ul">
<li>Ej. 1: Tirar una moneda 1 vez por segundo y anotar el resultado.</li>
<li>Ej. 2: Número de vehículos en un estacionamiento en función del tiempo.</li>
<li>Ej. 3: Temperatura de un sensor en función del tiempo.</li>
<li>Ej. 4: Número de disparos (spikes) de una neurona en un intervalo de tiempo</li>
<li>Ej. 5: Número de clientes en una cola de un banco en función del tiempo.</li>
</ul>

<p>
Formalmente: un proceso estocástico es una familia de variables aleatorias
indexadas por el tiempo, espacio u otra variable.
</p>




<p>
Procesos estocásticos
</p>

<p>
Cada repetición del experimento aleatorio da como resultado una realización
del proceso estocástico.
</p>





<p>
Procesos estocásticos multidimensionales:
Campos aleatorios
</p>


<p>
6
</p>

<p>
Ejemplo: Procesamiento de Imágenes 2D
</p>

<p>
Xi,j
</p>


<p>
Xi
</p>

<p>
1
</p>

<p>
Xi,j Xi+1,j
</p>

<p>
1,j
</p>

<p>
Xi,j+1
</p>

<p>
2D random field:
</p>

<p>
{Xi,j }
</p>

<p>
8i = 1, 2, . . . , I
8j = 1, 2, . . . , J
</p>


<p>
Applicación de campos aleatorios para el “denoising” de
imágenes astronómicas
</p>



<p>
Cosmic Microwave
Background (CMB) - Satellite Planck
</p>

<p>
Resultado de “denoising” usando un modelo de campo aleatorio
Gaussiano con correlaciones de largo alcance para el CMB y ruido
Gaussiona nocorrelacionado para el ruido
</p>

<p>
“Long Correlation Gaussian Random Fields: Parameter Estimation and Noise Reduction”, C. F. Caiafa,
A. N. Proto, E. E. Kuruoglu, Digital Signal Processing, Volume 17, pp. 819-835, 2007 (Elsevier)
</p>




<p>
Modelos probabilísticos para procesos Estocásticos
</p>



<p>
La descripción completa de un proceso estocástico X(tn) está dada por la
distribución conjunta de todas las variables aleatorias que lo conforman, es
decir:
</p>

<p>
Para hacer manejable la matemática es necesario simplificar las
probabilidades conjuntas a casos más sencillos.
Por ejemplo: Asumir variables aleatorias i.i.d. (independent and identically
distributed)
</p>


<p>
Casos particulares con variables i.i.d
</p>


<p>
Caracterización de procesos a través de momentos
</p>


<p>
Describir a los procesos a través de momentos en lugar de sus distribuciones
conjuntas
Media
Varianza
Autocorrelación
</p>

<p>
Ejercicio: demostrar que
</p>

<p>
V AR[Xn ] =
</p>

<p>
2
</p>

<p>
for all n
</p>


<p>
RX (n1 , n2 ) =
</p>

<p>
2
</p>

<p>
{ m0
</p>

<p>
2
</p>


<ul class="org-ul">
<li>m2</li>
</ul>

<p>
si n1 = n2
6 n2
si n1 =
</p>


<p>
Procesos estocásticos estacionarios
</p>

<p>
11
</p>

<p>
Un proceso es estacionario si la distribución conjunta de cualquier conjunto
de variables no depende de un desplazamiento en el tiempo.
</p>

<p>
Ejercicio: demostrar que si un proceso es estacionario, entonces se cumple:
</p>

<p>
Wide-sense stationary (WSS)
</p>

<p>
Nota importante:
proceso estacionario
Wide-sense stationary (WSS)
</p>


<p>
Wide-sense stationary (WSS)
proceso estacionario
</p>


<p>
Procesos de Poisson
</p>

<p>
Procesos de Poisson
</p>

<p>
13
</p>

<p>
Un proceso de Poisson surge al considerar número de eventos que ocurren a
lo largo del tiempo.
Ejemplos:
</p>
<ul class="org-ul">
<li>Número de clientes que llegan a un banco.</li>
<li>Número de disparos en una neurona.</li>
<li>Número de llamadas telefónicas recibidas en una central.</li>
</ul>

<p>
N(t): Número de eventos
x: Eventos
Tiempo continuo
Valores discretos
</p>





<p>
Derivación de un proceso de Poisson
como el límite de una binomial
</p>

<p>
14
</p>

<p>
M: Número de celdas
t : Ancho de celdas
p: Probabilidad éxito
</p>

<p>
Discretizamos el eje de tiempos y
consideramos la variable de Bernoulli
U(n) = 1 si hay un evento en la celda n
o U(n) = 0 si no lo hay, con una
probabilidad de éxito igual a p.
</p>


<p>
Binomial:
Consideremos: M ! 1 , p ! 0 ,
</p>

<p>
P [N (t) = k] ⇡ lim
</p>

<p>
M (M
</p>



<p>
t!0 y
</p>

<p>
1)(M
</p>

<p>
M !1
</p>

<p>
k
</p>

<p>
M + O(M
= lim
M !1
k!
</p>

<p>
k 1
</p>

<p>
k
</p>

<p>
) ( t)
Mk
</p>


<p>
✓
</p>

<p>
1
</p>

<p>
t
M
</p>

<p>
◆M
</p>

<p>
p
=
constante
t
</p>

<ol class="org-ol">
<li>· · · (M</li>
</ol>
<p>
k!
k
</p>

<p>
k + 1)
</p>

<p>
( t)
= lim
M !1 k!
</p>

<p>
t
Finalmente: P [N (t) = k] = e
k!
</p>

<p>
t
</p>

<p>
k
</p>

<p>
✓
</p>

<p>
✓
</p>

<p>
1
</p>

<p>
t
M
</p>

<p>
◆k ✓
</p>

<p>
t
M
</p>

<p>
t
M
</p>

<p>
1
</p>

<p>
◆M
</p>

<p>
◆M
</p>

<p>
k
</p>

<p>
k
</p>

<p>
M !1
</p>

<p>
e
</p>

<p>
t
</p>



<p>
Proceso de Poisson
</p>



<p>
Proceso de Poisson: propiedades (I)
</p>

<p>
15
</p>

<pre class="example">
Tasa de arribos o frecuencia de eventos por unidad de tiempo
</pre>


<p>
Media y varianza
Incrementos independientes
</p>

<p>
N (t)
</p>

<p>
A
(k1 )
</p>


<p>
B
(k2 )
</p>



<p>
x
</p>

<p>
t1
</p>

<p>
P [N (t2 )
</p>



<p>
t2
</p>

<p>
x x
</p>


<p>
t3
</p>

<p>
N (t1 ) = k1 , N (t4 )
</p>



<p>
xx x
</p>



<p>
P [A, B] = P [A]P [B]
</p>

<p>
x
</p>

<p>
x
</p>

<p>
x
</p>

<p>
x
</p>

<p>
t4
</p>

<p>
N (t3 ) = k2 ] = P [N (t2 )
</p>

<p>
t
</p>


<p>
N (t1 ) = k1 ]P [N (t4 )
</p>

<p>
N (t3 ) = k2 ]
</p>

<p>
Incrementos estacionarios
</p>

<p>
N (t)
</p>

<p>
A
(k)
</p>

<p>
B
(k)
</p>



<p>
x
</p>

<p>
x
</p>


<p>
t1
</p>

<p>
x
</p>


<p>
x
</p>

<p>
x xx
</p>


<p>
t1 + ⌧
</p>

<p>
P [N (t1 + ⌧ )
</p>


<p>
P [A] = P [B]
</p>


<p>
t2
</p>

<p>
x
</p>

<p>
x
</p>

<p>
x
</p>

<p>
x
</p>

<p>
t2 + ⌧
</p>

<p>
N (t1 ) = k] = P [N (t2 + ⌧ )
</p>



<p>
t
</p>

<p>
N (t2 ) = k] = P [N (⌧ ) = k]
</p>


<p>
Proceso de Poisson: propiedades (I)
</p>

<p>
16
</p>

<p>
Variable aleatoria Zn: tiempos entre eventos
</p>

<p>
Memoryless property:
</p>





<p>
Proceso de Poisson: Ejemplos
</p>



<p>
17
</p>



<p>
Proceso de Poisson: Ejemplos
</p>



<p>
18
</p>



<p>
Simulación de un proceso de Poisson
</p>

<p>
19
</p>

<p>
Método para generar eventos de Poisson
1- Generar muestras de una distribución exponencial Z
con parámetro :
</p>


<p>
z1, z2, …, zN.
2- Definir tiempos de eventos conteo de Poisson
t0=0, t1=t0+z1, t2=t1+z2, t3=t2+z3, …
N(t0)=0, N(t1)=N(t0)+1 , N(t2)=N(t1)+1, …
</p>
</div>
</div>
</div>





<div id="outline-container-org6044719" class="outline-1">
<h1 id="org6044719"><span class="section-number-1">2</span> Material de Lectura</h1>
<div class="outline-text-1" id="text-2">
<ul class="org-ul">
<li>Sobre la clase de hoy:
<ul class="org-ul">
<li>Libros (Mínimo):
<ul class="org-ul">
<li>[15] Probability, Statistics, and Random Processes for Eletrical
Engineerging, 3rd<sub>Ed</sub>. - Leon-Garcia., Cap 9.1-4.</li>
<li>[5] Intuitive Probability and Random Processes Using MatLab - Steven M.
Kay, Cap 21.</li>
</ul></li>

<li>Libros (opcional):
<ul class="org-ul">
<li>[16] Performance Modeling and Design of Computer Systems: Queueing Theory
in Action, Mor Harchol-Balter, 2013, Cap 11</li>
<li>[19] Introduction to probability models, Ross Sheldon, 2007, Cap 5.</li>
</ul></li>
</ul></li>

<li>Leer para la próxima clase:
<ul class="org-ul">
<li>Libros (Mínimo):
<ul class="org-ul">
<li>[15] Probability, Statistics, and Random Processes for Eletrical Engineerging, 3rd<sub>Ed</sub>. - Leon-Garcia., Cap 11.</li>
<li>[5] Intuitive Probability and Random Processes Using MatLab - Steven M. Kay, Cap 22.</li>
</ul></li>

<li>Libros (opcional):
<ul class="org-ul">
<li>[16] Performance Modeling and Design of Computer Systems: Queueing Theory
in Action, Mor Harchol-Balter, 2013, Cap 8-10</li>
<li>[19] Introduction to probability models, Ross Sheldon, 2007, Cap 4.</li>
</ul></li>
</ul></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
Last update: 2020-09-16 14:52
</div>
</body>
</html>
