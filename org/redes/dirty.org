* Redes de Computadoras y la Internet
# resumen
- components that make up a network
- network edge
  - end systems and network applications running in the network
- network core
  - links and the switches that transport data
  - access networks and physical media that connect end systems to the network
    core.

the Internet is a network of networks, and we’ll learn how these networks
connect with each other.

** Que es la Internet?

dos descripciones

*** Descripcion de Componentes

The Internet es una red de computadores que interconecta millones de dispostivos
alrededor del mundo.

Estos dispositivos son llamados =hosts= o =end systems=.

imagen 1.1

Los End-systems estan conectados a traves de una red de ~enlaces de
comunicacion~ (coaxial,cobre,fibra,radio) y ~conmutadores de paquetes~.

Diferentes enlaces pueden transmitir datos a diferentes velocidades, con la
~velocidad de transmision~ de un enlace medida en bits/segundo.

# fragmentacion (no ip), encapsulado, paquetes, reemsablado en el end-system
When one end system has data to send to another end system, the sending end
system segments the data and adds header bytes to each segment. The resulting
packages of information, known as ~packets~, are then sent through the network
to the destination end system, where they are reassembled into the original
data.

# paquet switch, puertos de entrada/salida, forwarding
A packet switch takes a packet arriving on one of its incoming communication
links and forwards that packet on one of its outgoing communication links.

# switch de capa-de-enlace, router
The two most prominent types in today’s Internet are routers and link-layer
switches. Both types of switches forward packets toward their ultimate
destinations. Link-layer switches are typically used in access networks,
while routers are typically used in the network core.

# ruta
The sequence of communication links and packet switches traversed by a
packet from the sending end system to the receiving end system is known as a
=route= or =path= through the network.

# analogia con rutas de vehiculos
Thus, in many ways, packets are analogous to trucks, communication links are
analogous to highways and roads, packet switches are analogous to intersections,
and end systems are analogous to buildings. Just as a truck takes a path through
the transportation network, a packet takes a path through a computer network.

# isp
End systems access the Internet through ~Internet Service Providers (ISPs)~,
including residential ISPs such as
- local cable or telephone companies
- corporate ISPs
- university ISPs
- ISPs that provide WiFi access in airports, hotels, coffee shops, and other public places
- cellular data ISPs, providing mobile access to our smartphones and other devices.

#+begin_quote
Each ISP is in itself a network of packet switches and communication links.

ISPs provide a variety of types of network access to the end systems,
including residential broadband access such as cable modem or DSL,
high-speed local area network access, and mobile wireless access.

ISPs also provide ­ Internet access to content providers, connecting Web
sites and video servers directly to the Internet.
#+end_quote

The Internet is all about connecting end systems to each other, so the ISPs
that provide access to end systems must also be interconnected.  These
lower-tier ISPs are interconnected through national and international
upper-tier ISPs such as Level 3 Communications, AT&T, Sprint, and NTT. An
upper-tier ISP consists of high-speed routers interconnected with high-speed
fiber-optic links. Each ISP network, whether upper-tier or lower-tier,
is managed independently, runs the IP protocol (see below), and conforms to
certain naming and address conventions.

# protocolos
End systems, packet switches, and other pieces of the Internet run ~protocols~
that *control the sending and receiving of information within the Internet*.

# tcp, ip
The ~Transmission Control Protocol (TCP)~ and the ~Internet Protocol (IP)~ are
two of the most important protocols in the Internet. The IP protocol specifies
the format of the packets that are sent and received among routers and end
systems. The Internet’s principal protocols are collectively known as TCP/IP.

# estandarizacion
Internet standards are developed by the ~Internet Engineering Task Force (IETF)~
[IETF 2016]. The IETF standards documents are called requests for comments
(RFCs) .

*** Descripcion de Servicios

La internet tambien se puede describir como una *infraestructura que provee
servicios a aplicaciones*.

# aplicaciones
In addition to traditional applications such as e-mail and Web surfing,
Internet applications include mobile smartphone and tablet applications,
including:
- Internet messaging,
- mapping with real-time road-traffic information,
- music streaming from the cloud,
- movie and television streaming,
- online social networks,
- video conferencing,
- multi-person games,
- location-based recommendation systems.

The applications are said to be ~distributed applications~, since they
involve multiple end systems that exchange data with each other.

# aplicaciones en net-edge (no net-core)
#+begin_quote
Internet applications run on end systems - they do not run in the packet
switches in the network core. Although packet switches facilitate the
exchange of data among end systems, they are not concerned with the
application that is the source or sink of data.
#+end_quote

# ej, wdym infraestructura?
How does one program running on one end system instruct the Internet to deliver
data to another program running on another end system?

End systems attached to the Internet provide a ~socket interface~ that specifies
how a program running on one end system asks the Internet infrastructure to
*deliver data to a specific destination program running on another end system*.

This Internet socket interface is a set of rules that the sending program must
follow so that the Internet can deliver the data to the destination program.

# analogia comunicacion-entre-aplicaciones y servicio-postal
Thus, the postal service has its own “postal service interface,” or set of
rules, that Alice must follow to have the postal service deliver her letter to
Bob. In a similar manner, the Internet has a socket interface that the program
sending data must follow to have the Internet deliver the data to the program
that will receive the data.

The postal service, of course, provides more than one service to its customers.
It provides express delivery, reception confirmation, ordinary use, and many
more services. In a similar manner, the Internet provides multiple services to
its applications.

*** TODO Que es un protocolo?

serie de reglas/comportamientos bien definidos que llevan al cumplimiento de un
objetivo.

algoritmo

intercambio de mensajes que desencadenan eventos/comportamientos/otros
mensajes para lograr un objetivo

it takes two (or more) communicating entities running the same protocol in
order to accomplish a task.

imagen 1.2

transmision y recepcion de mensajes y un conjunto de acciones convencionales
tomadas cuando estos mensajes son enviados y recibidos

All activity in the Internet that involves two or more communicating remote
entities is governed by a protocol.

# ejemplos
For example, hardware-implemented protocols in two physically connected
computers control the flow of bits on the “wire” between the two network
interface cards; congestion-control protocols in end systems control the
rate at which packets are transmitted between sender and receiver;
protocols in routers determine a packet’s path from source to destination.

#+begin_quote
A protocol defines the format and the order of messages exchanged between
two or more communicating entities, as well as the actions taken on the
transmission and/or receipt of a message or other event.
#+end_quote

** TODO Network Edge :networkedge:
the computers and other devices connected to the Internet are often referred
to as end systems. They are referred to as end systems because they sit at
the edge of the Internet.

imagen 1.3

End systems are also referred to as hosts because they host (ie, run)
application programs such as
- a Web browser/server
- an e-mail client/server

  host = end system

  # clientes y servidores
  Hosts are sometimes further divided into two categories: ~clients~ and
  ~servers~. Informally, clients tend to be desktop and mobile PCs,
  smartphones, and so on, whereas servers tend to be more powerful machines
  that store and distribute Web pages, stream video, relay e-mail, and so on.

  Today, most of the servers from which we receive search results, e-mail, Web
  pages, and videos reside in large ~data centers~.

*** Redes de acceso
the network that physically connects an end system to the first router (also
known as the “edge router”) on a path from the end system to any other
distant end system.

imagen 1.4

**** Home Access: DSL, Cable, FTTH, Dial-Up, and Satellite

let’s begin our overview of access networks by considering how homes
connect to the Internet.

the two most prevalent types of broadband residential access are digital
subscriber line (DSL) and cable.

A residence typically obtains DSL Internet access from the same local
telephone company (telco) that provides its wired local phone access. Thus,
when DSL is used, a customer’s telco is also its ISP.

# explicacion de dsl
each customer’s DSL modem uses the existing telephone line to exchange data
with a digital subscriber line access multiplexer (DSLAM) located in the
telco’s local central office (CO). The home’s DSL modem takes digital data
and translates it to high-frequency tones for transmission over telephone
wires to the CO; the analog signals from many such houses are translated
back into digital format at the DSLAM.

The residential telephone line carries both data and traditional telephone
signals simultaneously, which are encoded at different frequencies:
- A high-speed downstream channel, in the 50 kHz to 1 MHz band
- A medium-speed upstream channel, in the 4 kHz to 50 kHz band
- An ordinary two-way telephone channel, in the 0 to 4 kHz band

  imagen 1.5

  This approach makes the single DSL link appear as if there were three
  separate links, so that a telephone call and an Internet connection can
  share the DSL link at the same time.

  On the customer side, a splitter separates the data and telephone signals
  arriving to the home and forwards the data signal to the DSL modem. On the
  telco side, in the CO, the DSLAM separates the data and phone signals and
  sends the data into the Internet. Hundreds or even thousands of households
  connect to a single DSLAM

  # explicacion de cable
  While DSL makes use of the telco’s existing local telephone infrastructure,
  cable Internet access makes use of the cable television company’s existing
  cable television infrastructure. A residence obtains cable Internet access
  from the same company that provides its cable television

  fiber optics connect the cable head end to neighborhood-level junctions,
  from which traditional coaxial cable is then used to reach individual
  houses and apartments. Each neighborhood junction typically supports 500 to
  5,000 homes. Because both fiber and coaxial cable are employed in this
  system, it is often referred to as hybrid fiber coax (HFC).

  imagen 1.6

  Cable internet access requires special modems, called cable modems. As with
  a DSL modem, the cable modem is typically an external device and connects to
  the home PC through an Ethernet port.

  At the cable head end, the cable modem termination system (CMTS) serves a
  similar function as the DSL network’s DSLAM-turning the analog signal sent
  from the cable modems in many downstream homes back into digital
  format. Cable modems divide the HFC network into two channels, a downstream
  and an upstream channel.

  As with DSL, access is typically asymmetric, with the downstream channel
  typically allocated a higher transmission rate than the upstream channel.

  The [[DOCSIS] 2.0 standard defines downstream rates up to 42.8 Mbps and
  upstream rates of up to 30.7 Mbps. As in the case of DSL networks, the
  maximum achievable rate may not be realized due to lower contracted data
  rates or media impairments.

  One important characteristic of cable Internet access is that it is a
  =shared broadcast medium=. In particular, every packet sent by the head end
  travels downstream on every link to every home and every packet sent by a
  home travels on the upstream channel to the head end. For this reason, if
  several users are simultaneously downloading a video file on the downstream
  channel, the actual rate at which each user receives its video file will be
  significantly lower than the aggregate cable downstream rate. On the other
  hand, if there are only a few active users and they are all Web surfing,
  then each of the users may actually receive Web pages at the full cable
  downstream rate, because the users will rarely request a Web page at
  exactly the same time. Because the upstream channel is also shared, a
  distributed multiple access protocol is needed to coordinate transmissions
  and avoid collisions. Mas en capitulo 6.

  # fibra
  An up-and-coming technology that provides even higher speeds is ~fiber to
  the home (FTTH)~. FTTH provides an optical fiber path from the CO directly
  to the home.

  There are several competing technologies for optical distribution from the
  CO to the homes.
  - The simplest optical distribution network is called direct fiber, with one
    fiber leaving the CO for each home.
  - More commonly, each fiber leaving the central office is actually shared by
    many homes; it is not until the fiber gets relatively close to the homes
    that it is split into individual customer-specific fibers.

    There are two competing optical-distribution network architectures that
    perform this splitting:
    - active optical networks (AONs) and
    - passive optical networks (PONs).

    imagen 1.7

    # explicacion de imagen 1.7
    Each home has an optical network terminator (ONT), which is connected by
    dedicated optical fiber to a neighborhood splitter. The splitter combines a
    number of homes (typically less than 100) onto a single, shared optical
    fiber, which connects to an optical line ­ terminator (OLT) in the telco’s
    CO. The OLT, providing conversion between optical and electrical signals,
    connects to the Internet via a telco router. In the home, users connect a
    home router (typically a wireless router) to the ONT and access the ­
    Internet via this home router. In the PON architecture, all packets sent
    from OLT to the splitter are replicated at the splitter (similar to a cable
    head end).

    # otras 2 access network
    Two other access network technologies are also used to provide Internet
    access to the home. In locations where DSL, cable, and FTTH are not
    available (e.g., in some rural settings),
    - a satellite link can be used to connect a residence to the Internet at
      speeds of more than 1 Mbps; StarBand and HughesNet are two such satellite
      access providers.
    - Dial-up access over traditional phone lines is based on the same model as
      DSL-a home modem connects over a phone line to a modem in the ISP.
      Compared with DSL and other broadband access networks, dial-up access is
      excruciatingly slow at 56 kbps.

**** Access in the Enterprise (and the Home): Ethernet and WiFi

On corporate and university campuses, and increasingly in home settings, a
local area network (LAN) is used to connect an end system to the edge
router.

Although there are many types of LAN technologies, Ethernet is by far the
most prevalent access technology in corporate, university, and home
networks.

imagen 1.8

Ethernet users use twisted-pair copper wire to connect to an Ethernet
switch, a technology discussed in detail in Chapter 6.

The Ethernet switch, or a network of such interconnected switches, is then
in turn connected into the larger Internet. With Ethernet access, users
typically have 100 Mbps or 1 Gbps access to the Ethernet switch, whereas
servers may have 1 Gbps or even 10 Gbps access.

In a wireless LAN setting, wireless users transmit/receive packets to/from
an access point that is connected into the enterprise’s network (most
likely using wired Ethernet), which in turn is connected to the wired
Internet.


Las tecnologias corporativas (?) de acceso a la red se volvieron comunes en
redes hogareñas.

# tipica red hogareña
imagen 1.9

# explicacion de imagen 1.9
This home network consists of a roaming laptop as well as a wired PC; a
base station (the wireless access point), which communicates with the
wireless PC and other wireless devices in the home; a cable modem,
providing broadband access to the Internet; and a router, which
interconnects the base station and the stationary PC with the cable modem.

**** Wide-Area Wireless Access: 3G and LTE
Increasingly, devices such as iPhones and Android devices are being used to
message, share photos in social networks, watch movies, and stream music
while on the run. These devices employ the same wireless infrastructure
used for cellular telephony to send/receive packets through a base station
that is operated by the cellular network provider. Unlike WiFi, a user need
only be within a few tens of kilometers (as opposed to a few tens of
meters) of the base station.

Telecommunications companies have made enormous investments in so-called
third-generation (3G) wireless, which provides packet-switched wide-area
wireless Internet access at speeds in excess of 1 Mbps. But even
higher-speed wide-area access technologies-a fourth-generation (4G) of
wide-area wireless networks-are already being deployed. LTE (for “Long-Term
Evolution”-a candidate for Bad Acronym of the Year Award) has its roots in
3G technology, and can achieve rates in excess of 10 Mbps. LTE downstream
rates of many tens of Mbps have been reported in commercial deployments.

*** Medios Fiscos

For each transmitter-receiver pair, the bit ((data)) is sent by propagating
electromagnetic waves or optical pulses across a ~physical medium~. The
physical medium can take many shapes and forms and does not have to be of
the same type for each transmitter-receiver pair along the path.

Examples of physical media include:
- twisted-pair
- copper wire
- coaxial cable
- multimode fiber-optic cable
- terrestrial radio spectrum
- satellite radio spectrum

  Physical media fall into two categories: ~guided media~ and ~unguided
  media~.

  With guided media, the waves are guided along a solid medium, such as a
  fiber-optic cable, a twisted-pair copper wire, or a coaxial cable.

  With unguided media, the waves propagate in the atmosphere and in outer
  space, such as in a wireless LAN or a digital satellite channel.

**** Twisted-Pair Copper Wire

The wires are twisted together to reduce the electrical interference from
similar pairs close by.

A wire pair constitutes a single communication link. ~Unshielded twisted
pair (UTP)~ is commonly used for computer networks within a building, that
is, for LANs. Data rates for LANs using twisted pair today range from 10
Mbps to 10 Gbps. The data rates that can be achieved depend on the
thickness of the wire and the distance between transmitter and receiver.

**** Coaxial Cable
coaxial cable consists of two copper conductors, but the two conductors are
concentric rather than parallel. With this construction and special
insulation and shielding, coaxial cable can achieve high data transmission
rates. Coaxial cable is quite common in cable television systems.

In cable television and cable Internet access, the transmitter shifts the
digital signal to a specific frequency band, and the resulting analog
signal is sent from the transmitter to one or more receivers.

Coaxial cable can be used as a guided ~shared medium~. Specifically, a
number of end systems can be connected directly to the cable, with each of
the end systems receiving whatever is sent by the other end systems.


**** Fiber Optics
An optical fiber is a thin, flexible medium that conducts pulses of light,
with each pulse representing a bit. A single optical fiber can support
tremendous bit rates, up to tens or even hundreds of gigabits per
second. They are immune to electromagnetic interference, have very low
signal attenuation up to 100 kilometers, and are very hard to tap.

preferred long-haul guided transmission media, particularly for overseas
links.

The Optical Carrier (OC) standard link speeds range from 51.8 Mbps to 39.8
Gbps

**** Terrestrial Radio Channels
Radio channels carry signals in the electromagnetic spectrum. They are an
attractive medium because they require no physical wire to be installed,
can penetrate walls, provide connectivity to a mobile user,and can
potentially carry a signal for long distances. The characteristics of a
radio channel depend significantly on the propagation environment and the
distance over which a signal is to be carried.

Environmental considerations determine path loss and shadow fading (which
decrease the signal strength as the signal travels over a distance and
around/through obstructing objects), multipath fading (due to signal
reflection off of interfering objects), and interference (due to other
transmissions and electromagnetic signals).

Terrestrial radio channels can be broadly classified into three groups:
those that operate over very short distance (e.g., with one or two meters);
those that operate in local areas, typically spanning from ten to a few
hundred meters; and those that operate in the wide area, spanning tens of
kilometers. Personal devices such as wireless headsets, keyboards, and
medical devices operate over short distances; the wireless LAN technologies
described in Section 1.2.1 use local-area radio channels; the cellular
access technologies use wide-area radio channels.

**** Satellite Radio Channels
A communication satellite links two or more Earth-based microwave
transmitter/ receivers, known as ground stations. The satellite receives
transmissions on one frequency band, regenerates the signal using a
repeater (discussed below), and transmits the signal on another
frequency. Two types of satellites are used in communications:
geostationary satellites and low-earth orbiting (LEO) satellites.

Geostationary satellites permanently remain above the same spot on
Earth. This stationary presence is achieved by placing the satellite in
orbit at 36,000 kilometers above Earth’s surface. This huge distance from
ground station through satellite back to ground station introduces a
substantial signal propagation delay of 280 milliseconds. Nevertheless,
satellite links, which can operate at speeds of hundreds of Mbps, are often
used in areas without access to DSL or cable-based Internet access.

LEO satellites are placed much closer to Earth and do not remain
permanently above one spot on Earth.  They rotate around Earth (just as the
Moon does) and may communicate with each other, as well as with ground
stations. To provide continuous coverage to an area, many satellites need
to be placed in orbit. There are currently many low-altitude communication
systems in development.
** TODO Network Core :networkcore:

the mesh of packet switches and links that interconnects the Internet’s end
systems.

imagen 1.10

There are two fundamental approaches to moving data through a network of
links and switches: circuit switching and packet switching.

*** Packet Switching

In a network application, end systems exchange ~messages~ with each
other. Messages can contain anything the application designer
wants. Messages may perform a control function or can contain data, such as
an e-mail message, a JPEG image, or an MP3 audio file.

To send a message from a source end system to a destination end system, the
source breaks long messages into smaller chunks of data known as
~packets~. Between source and destination, each packet travels through
communication links and ~packet switches~ (for which there are two
predominant types, ~routers~ and ~link-layer switches~).

Packets are transmitted over each communication link at a rate equal to the
full transmission rate of the link. So, if a source end system or a packet
switch is sending a packet of L bits over a link with transmission rate R
bits/sec, then the time to transmit the packet is L / R seconds.

**** Transmision Store-and-Forward

Store-and-forward transmission means that the packet switch must receive
the entire packet before it can begin to transmit the first bit of the
packet onto the outbound link.

imagen 1.11

A router will typically have many incident links, since its job is to
switch an incoming packet onto an outgoing link; in this simple example,
the router has the rather simple task of transferring a packet from one
(input) link to the only other attached link. In this example, the source
has three packets, each consisting of L bits, to send to the destination.

the router cannot transmit the bits it has received; instead it must first
buffer (i.e., “store”) the packet’s bits.

Only after the router has received all of the packet’s bits can it begin to
transmit (i.e., “forward”) the packet onto the outbound link.

Let’s now consider the general case of sending one packet from source to
destination over a path consisting of N links each of rate R (thus, there
are N-1 routers between source and destination).  Applying the same logic
as above, we see that the end-to-end delay is:

d_{end-to-end} = N(\frac{L}{R})

**** Queuing Delays and Packet Loss
Each packet switch has multiple links attached to it. For each attached
link, the packet switch has an output buffer (also called an output queue),
which stores packets that the router is about to send into that link. The
output buffers play a key role in packet switching. If an arriving packet
needs to be transmitted onto a link but finds the link busy with the
transmission of another packet, the arriving packet must wait in the output
buffer. Thus, in addition to the store-and-forward delays, packets suffer
output buffer ~queuing delays~. These delays are variable and depend on the
level of congestion in the network.

an arriving packet may find that the buffer is completely full with other
packets waiting for transmission, lo que provoca ~packet loss~

imagen 1.12

La imagen 1.12 muestra que puede ocurrir delay de encolado

**** Forwarding Tables and Routing Protocols
a router takes a packet arriving on one of its attached communication links
and forwards that packet onto another one of its attached communication
links. But how does the router determine which link it should forward the
packet onto?

In the Internet, every end system has an address called an IP address. When
a source end system wants to send a packet to a destination end system, the
source includes the destination’s IP address in the packet’s header.

As with postal addresses, this address has a hierarchical structure. When a
packet arrives at a router in the network, the router examines a portion of
the packet’s destination address and forwards the packet to an adjacent
router. More specifically, each router has a ~forwarding table~ that maps
destination addresses (or portions of the destination addresses) to that
router’s outbound links.  When a packet arrives at a router, the router
examines the address and searches its forwarding table, using this
destination address, to find the appropriate outbound link. The router then
directs the packet to this outbound link.

# analogia del proceso de ruteo
The end-to-end routing process is analogous to a car driver who does not
use maps but instead prefers to ask for directions.

a router uses a packet’s destination address to index a forwarding table
and determine the appropriate outbound link. But this statement begs yet
another question: How do forwarding tables get set? Are they configured by
hand in each and every router, or does the Internet use a more automated
procedure? This issue will be studied in depth in Chapter 5.

the Internet has a number of special ~routing protocols~ that are used to
automatically set the forwarding tables. A routing protocol may, for
example, determine the shortest path from each router to each destination
and use the shortest path results to configure the forwarding tables in the
routers.

*** Circuit Switching
In circuit-switched networks, the resources needed along a path (buffers,
link transmission rate) to provide for communication between the end systems
are reserved for the duration of the communication session between the end
systems.

In packet-switched networks, these resources are not reserved; a session’s
messages use the resources on demand and, as a consequence, may have to wait
(that is, queue) for access to a communication link.

Before the sender can send the information, the network must establish a
connection between the sender and the receiver. This is a bona fide
connection for which the switches on the path between the sender and
receiver maintain connection state for that connection. In the jargon of
telephony, this connection is called a ~circuit~.

imagen 1.13 ... (no se si hace falta)


The Internet makes its best effort to deliver packets in a timely manner,
but it does not make any guarantees.

**** Multiplexing in Circuit-Switched Networks
A circuit in a link is implemented with either frequency-division
multiplexing (FDM) or time-division multiplexing (TDM).

With FDM, the frequency spectrum of a link is divided up among the
connections established across the link. Specifically, the link dedicates a
frequency band to each connection for the duration of the connection. FM
radio stations also use FDM to share the frequency spectrum between 88 MHz
and 108 MHz, with each station being allocated a specific frequency band.

For a TDM link, time is divided into frames of fixed duration, and each
frame is divided into a fixed number of time slots. When the network
establishes a connection across a link, the network dedicates one time slot
in every frame to this connection. These slots are dedicated for the sole
use of that connection, with one time slot available for use (in every
frame) to transmit the connection’s data.

imagen 1.14

# explicacion de imagen 1.14
With FDM, each circuit continuously gets a fraction of the bandwidth. With
TDM, each circuit gets all of the bandwidth periodically during brief
intervals of time (that is, during slots)

Proponents of packet switching have always argued that circuit switching is
wasteful because the dedicated circuits are idle during ~silent
periods~. For example, when one person in a telephone call stops talking,
the idle network resources (frequency bands or time slots in the links
along the connection’s route) cannot be used by other ongoing connections.

Proponents of packet switching also enjoy pointing out that establishing
end-to-end circuits and reserving end-to-end transmission capacity is
complicated and requires complex signaling software to coordinate the
operation of the switches along the end-to-end path.

**** Packet Switching Versus Circuit Switching
Critics of packet switching have often argued that packet switching is not
suitable for real-time services (for example, telephone calls and video
conference calls) because of its variable and unpredictable end-to-end
delays (due primarily to variable and unpredictable queuing delays).

Proponents of packet switching argue that (1) it offers better sharing of
transmission capacity than circuit switching and (2) it is simpler, more
efficient, and less costly to implement than circuit switching.



Circuit switching pre-allocates use of the transmission link regardless of
demand, with allocated but unneeded link time going unused.

Packet switching on the other hand allocates link use on demand. Link
transmission capacity will be shared on a packet-by-packet basis only among
those users who have packets that need to be transmitted over the link.


*** A Network of Networks

Recall that the overarching goal is to interconnect the access ISPs so that
all end systems can send packets to each other.

One naive approach would be to have each access ISP directly connect with
every other access ISP. Such a ~mesh design~ is, of course, much too costly
for the access ISPs, as it would require each access ISP to have a separate
communication link to each of the hundreds of thousands of other access ISPs
all over the world.

- Network Structure 1 :: interconnects all of the access ISPs with a single
  global transit ISP. Our (imaginary) global transit ISP is a network of
  routers and communication links that not only spans the globe, but also
  has at least one router near each of the hundreds of thousands of
  access ISPs. Of course, it would be very costly for the global ISP to
  build such an extensive network. To be profitable, it would naturally
  charge each of the access ISPs for connectivity, with the pricing
  reflecting (but not necessarily directly proportional to) the amount of
  traffic an access ISP exchanges with the global ISP. Since the access
  ISP pays the global transit ISP, the access ISP is said to be a
  customer and the global transit ISP is said to be a provider.

- Network Structure 2 :: which consists of the hundreds of thousands of
  access ISPs and multiple global ­ transit ISPs. The access ISPs
  certainly prefer Network Structure 2 over Network Structure 1 since
  they can now choose among the competing global transit providers as a
  function of their pricing and services. Note, however, that the global
  transit ISPs themselves must interconnect: Otherwise access ISPs
  connected to one of the global transit providers would not be able to
  communicate with access ISPs connected to the other global transit
  providers.

  is a two-tier hierarchy with global transit providers residing at the
  top tier and access ISPs at the bottom tier. This assumes that global
  transit ISPs are not only capable of getting close to each and every
  access ISP, but also find it economically desirable to do so. In
  reality, although some ISPs do have impressive global coverage and do
  directly connect with many access ISPs, no ISP has presence in each and
  every city in the world. Instead, in any given region, there may be a
  ~regional ISP~ to which the access ISPs in the region connect. Each
  regional ISP then connects to tier-1 ISPs. Tier-1 ISPs are similar to
  our (imaginary) global transit ISP; but tier-1 ISPs, which actually do
  exist, do not have a presence in every city in the world.

- Network Structure 3 :: not only are there multiple competing tier-1 ISPs,
  there may be multiple competing regional ISPs in a region. In such a
  hierarchy, each access ISP pays the regional ISP to which it connects,
  and each regional ISP pays the tier-1 ISP to which it connects. (An
  access ISP can also connect directly to a tier-1 ISP, in which case it
  pays the tier-1 ISP). Thus, there is customer- provider relationship at
  each level of the hierarchy. Note that the tier-1 ISPs do not pay
  anyone as they are at the top of the hierarchy. To further complicate
  matters, in some regions, there may be a larger regional ISP (possibly
  spanning an entire country) to which the smaller regional ISPs in that
  region connect; the larger regional ISP then connects to a tier-1
  ISP. For example, in China, there are access ISPs in each city, which
  connect to provincial ISPs, which in turn connect to national ISPs,
  which finally connect to tier-1 ISPs.

  multi-tier hierarchy

- Network Structure 4 :: Ecosystem consisting of access ISPs, regional ISPs,
  tier-1 ISPs, PoPs, multi-homing, peering, and IXPs
  - ~Points of presence (PoPs)~: PoPs exist in all levels of the
    hierarchy, except for the bottom (access ISP) level. A PoP is simply
    a group of one or more routers (at the same location) in the
    provider’s network where customer ISPs can connect into the provider
    ISP. For a customer network to connect to a provider’s PoP, it can
    lease a high-speed link from a third-party telecommunications
    provider to directly connect one of its routers to a router at the
    PoP.
  - ~Multi-home~: Any ISP (except for tier-1 ISPs) may choose to
    multi-home, that is, to connect to two or more provider ISPs. So, for
    example, an access ISP may multi-home with two regional ISPs, or it
    may multi-home with two regional ISPs and also with a tier-1
    ISP. Similarly, a regional ISP may multi-home with multiple tier-1
    ISPs. When an ISP multi-homes, it can continue to send and receive
    packets into the Internet even if one of its providers has a failure.
  - ~Peering~: The amount that a customer ISP pays a provider ISP
    reflects the amount of traffic it exchanges with the provider. To
    reduce these costs, a pair of nearby ISPs at the same level of the
    hierarchy can peer, that is, they can directly connect their networks
    together so that all the traffic between them passes over the direct
    connection rather than through upstream intermediaries. When two ISPs
    peer, it is typically settlement-free, that is, neither ISP pays the
    other. As noted earlier, tier-1 ISPs also peer with one another,
    settlement-free.
  - ~Internet Exchange Point (IXP)~: a third-party company can create an
    Internet Exchange Point (IXP), which is a meeting point where
    multiple ISPs can peer together. An IXP is typically in a stand-alone
    building with its own switches

- Network Structure 5 :: describes today’s Internet. builds on top of
  Network Structure 4 by adding ~content-provider networks~ or ~content
  delivery networks~.

  Google is currently one of the leading examples of such a
  content-provider network. As of this writing, it is estimated that
  Google has 50–100 data centers distributed across North America,
  Europe, Asia, South America, and Australia. Some of these data centers
  house over one hundred thousand servers, while other data centers are
  smaller, housing only hundreds of servers. The Google data centers are
  all interconnected via Google’s private TCP/IP network, which spans the
  entire globe but is nevertheless separate from the public
  Internet. Importantly, the Google private network only carries traffic
  to/from Google servers. As shown in Figure 1.15, the Google private
  network attempts to “bypass” the upper tiers of the Internet by peering
  (settlement free) with lower-tier ISPs, either by directly connecting
  with them or by connecting with them at IXPs.

  However, because many access ISPs can still only be reached by
  transiting through tier-1 networks, the Google network also connects to
  tier-1 ISPs, and pays those ISPs for the traffic it exchanges with
  them. By creating its own network, a contentprovider not only reduces
  its payments to upper-tier ISPs, but also has greater control of how
  its services are ultimately delivered to end users.

  imagen 1.15

** TODO Delay, Perdida de Paquetes y Throughput en Redes de Conmutadores-de-Paquetes

computer networks:
- constrain throughput (the amount of data per second that can be transferred)
  between end systems
- introduce delays between end systems
- can lose packets

*** Overview of Delay in Packet-Switched Networks

A medida que un paquete es transmitido entre end-systems, este sufre de
varios tipos de delay en cada nodo a lo largo de una ruta.
- delay de procesamiento del nodo ~nodal processing delay~
- delay de encolado ~queuing delay~
- delay de transmision ~transmission delay~
- delay de prograpagion ~propagation delay~

  la suma de todos los delays se llama =delay del nodo=.

**** Tipos de Delay

imagen 1.16

Our goal is to characterize the nodal delay at router A.

As part of its end-to-end route between source and destination, a packet is
sent from the upstream node through router A to router B.

Note that router A has an outbound link leading to router B.

This link is preceded by a queue (also known as a buffer).

When the packet arrives at router A from the upstream node, router A
examines the packet’s header to determine the appropriate outbound link for
the packet and then directs the packet to this link.

In this example, the outbound link for the packet is the one that leads to
router B.

A packet can be transmitted on a link only if there is no other packet
currently being transmitted on the link and if there are no other packets
preceding it in the queue; if the link is currently busy or if there are
other packets already queued for the link, the newly arriving packet will
then join the queue.

***** Processing delay

es el tiempo requerido para examinar el encabezado de un paquete y
determinar a donde redireccionar el packet.

incluye otros factores:
- tiempo de verificacion de errores a nivel de bits que ocurrieron durante el
  arribo del paquete.

  luego del procesado del paquete, se envia a la cola del enlace que lleva al
  destino.

***** Queuing Delay

tiempo de espera en la cola hasta que el paquete sea transmitido por el enlace.

depende de la cantidad de paquetes que arribaron antes a la cola y se encuentran
esperado.

si no hay otros paquetes, el delay es 0.

se encuentran en el orden de microsegundos a milisegundos.

***** Transmission Delay
tiempo en que se tarda en enviar todo el paquete por el enlace.

depende del largo del paquete (L bits) y la velocidad de transmision del enlace
(R bits/seg)

el delay es $L/R$.

se encuentran en el orden de microsegundos a milisegundos.

***** Propagation Delay
es el tiempo de propagacion por el enlace entre los nodos.

depende del medio fisico del enlace:
- fibra optica
- aire
- cobre
- etc

  is in the range of 2⋅108 meters/sec to 3⋅108 meters/sec or a little less
  than, the speed of light.

  es la distancia entre nodos divido la velocidad de propagacion

  In WANs, propagation delays are on the order of milliseconds.

***** delay de transmision vs delay de propagacion
- The transmission delay :: the amount of time required for the router to
  push out the packet; it is a function of the packet’s length and the
  transmission rate of the link, but has nothing to do with the distance
  between the two routers.

- The propagation delay :: the time it takes a bit to propagate from one
  router to the next; it is a function of the distance between the two
  routers, but has nothing to do with the packet’s length or the
  transmission rate of the link.

  # analogia de diferencia entre los delays
  imagen 1.17

*** Queuing Delay and Packet Loss

el delay de encolado puede variar de paquete a paquete.

si 10 paquetes arriban a una cola vacia, el primero no sufre de latencia,
mientras que el ultimo debe esperar a los 9 paquetes anteriores sean
transmitidos por el enlace.

para caracterizar el tiempo de encolado se usan medidas estadisticas:
- promedio
- varianza
- probabilidad de que el delay exceda cierto umbral

  When is the queuing delay large and when is it insignificant?  it depends
  on:
  - the rate at which traffic arrives at the queue
  - the transmission rate of the link
  - the nature of the arriving traffic, that is, whether the traffic arrives
    periodically or arrives in bursts.

    the average rate at which bits arrive at the queue is $L_{a}$ bits/sec.

    The ratio $L_{a}/R$, called the ~traffic intensity~, estimates the queuing
    delay.

    si L_{a}/R > 1, los paquetes llegan a la cola a una mayor a la que se pueden
    transmitir, por lo que la cola crece y el delay tiende a infinito.

    Therefore, one of the golden rules in traffic engineering is: /Design your
    system so that the traffic intensity is no greater than 1/.

    si $L_{a}/R \leq 1$, el delay depende de la naturaleza de los arribos:
    - arribos periodicos cada L/R segundos, cada paquete arriba a una cola vacia
    - arribos periodicos pero por rafagas de paquetes, por ej: arriban N paquetes de
      forma simultanea cada (L/R)N segundos, entonces el primer paquete no tiene
      delay, mientras que el N-esimo paquete tiene delay de encolado de (N-1)L/R

    the average queuing delay (creo que es) $\frac{L}{R}\frac{n+1}{2}$

    Si la intensidad de trafico es cercana a 0, tambien los es el delay de
    encolado

    imagen 1.18

    a medida que la intensidad de trafico se acerca a 1, el delay de encolado
    promedio incrementa rapidamente.

**** Packet Loss

debido a que la capacidad de un buffer es finita, el delay de encolado no se
acerca a infinito a medida que la intensidad de trafico se acerca a 1.

cuando una cola esta llena, un paquete entrante no tiene lugar por lo que un
router =descarta= dicho paquete, es decir que se =pierde= el paquete.

desde el punto de vista del end-system, el paquete se envia a la red, pero este
nunca emerge de la red hacia el destino.

la cantidad de paquetes perdidos incrementa a medida que lo hace la intensidad
de trafico.

la performance de un nodo tambien se puede medir en terminos de probabilidad de
que haya perdida de paquetes.

*** End-to-End Delay

Let’s now consider the total delay from source to destination. To get a
handle on this concept, suppose there are N−1 routers between the source
host and the destination host. Let’s also suppose for the moment that the
network is uncongested (so that queuing delays are negligible), the
processing delay at each router and at the source host is d proc , the
transmission rate out of each router and out of the source host is R
bits/sec, and the propagation on each link is d prop . The nodal delays
accumulate and give an end-to- end delay,

$$dend−end = N(dproc+dtrans+dprop)$$

where, once again, dtrans=L/R, where L is the packet size.

*** Throughput

To define throughput, consider transferring a large file from Host A to Host
B across a computer network. This transfer might be, for example, a large
video clip from one peer to another in a P2P file sharing system.

The ~instantaneous throughput~ at any instant of time is the rate (in
bits/sec) at which a Host is receiving a file.

If the file consists of F bits and the transfer takes T seconds for a Host to
receive all F bits, then the ~average throughput~ of the file transfer is F/T
bits/sec.

imagen 1.19

Figure 1.19(a) shows two end systems, a server and a client, connected by
two communication links and a router.

Consider the throughput for a file transfer from the server to the client.
- Let $R_{s}$ denote the rate of the link between the server and the router
- Let $R_{c}$ denote the rate of the link between the router and the client

  Suppose that the only bits being sent in the entire network are those from
  the server to the client.  We now ask, in this ideal scenario, what is the
  server-to-client throughput?

  the server cannot pump bits through its link at a rate faster than
  $R_{s}$ bps; and the router cannot forward bits at a rate faster than
  $R_{c}$ bps.

  If $R_{s}$<$R_{c}$, then the bits pumped by the server will “flow” right
  through the router and arrive at the client at a rate of $R_{s}$ bps, giving
  a throughput of $R_{s}$ bps.

  If $R_{c}$<$R_{s}$, then the router will not be able to forward bits as
  quickly as it receives them. In this case, bits will only leave the router at
  rate $R_{c}$ , giving an end-to-end throughput of $R_{c}$ .

  For this simple two-link network, the throughput is $min\{R_{c}, R_{s} \}$,
  that is, it is the transmission rate of the ~bottleneck link~.

  Figure 1.19(b) now shows a network with N links between the server and the
  client, with the transmission rates of the N links being R1,R2,...,
  RN. Applying the same analysis as for the two-link network, we find that the
  throughput for a file transfer from server to client is $min \{R1,R2,...,
  RN\}$ , which is once again the transmission rate of the bottleneck link
  along the path between server and client.

  imagen 1.20

  the constraining factor for throughput in today’s Internet is typically the
  access network.

  #+begin_quote
  when there is no other intervening traffic, the throughput can simply be
  approximated as the minimum transmission rate along the path between source
  and destination.
  #+end_quote

  #+begin_quote
  The example in Figure 1.20(b) shows that more generally the throughput
  depends not only on the transmission rates of the links along the path, but
  also on the intervening traffic.

  In particular, a link with a high transmission rate may nonetheless be the
  bottleneck link for a file transfer if many other data flows are also
  passing through that link.
  #+end_quote

** TODO Capas de protocolos y sus servicios
*** Arquitectura de capas

Una actividad compleja puede dividirse entre capas, cada una implementando
una funcionalidad. Cada capa, combinada con las capas inferiores, provee mas
funcionalidades/servicios.

Each layer provides its service by
1) performing certain actions within that layer and by
2) using the services of the layer directly below it

   una arquitectura de capas permite especificar parte un sistema complejo y
   grande. tambien permite la modularizacion facilitando el cambio de
   implementacion de un servicio provisto por la capa. mientras que la capa
   proporcione el mismo servicio a la capa superior y utilice los mismos
   serivicios de las capas inferiores, el resto del sistema no se ve alterado.

   organizacion de capas provee estructura para diseño

**** protocol layering

los protocolos pertenecan a cada capa.

- modelo de servicios de una capa :: nos interesan los servicios que cada
  capa ofrece a la capa superior.

  un protocolo de capa puede ser implementado en HW o SW o una combinacion de
  ambos

  desventajas:
  - posible duplicacion de funcionalidad :: recuperacion de errores
  - violacion de separacion de capas :: puede ser que una capa requiera
    informacion disponible en otra capa

    imagen 1.23
    #+caption: Five layer Internet protocol stack
    | Application |
    | Transport   |
    | Network     |
    | Link        |
    | Physical    |

    #+caption: Seven layer ISO OSI reference model
    | Application  |
    | Presentation |
    | Session      |
    | Transport    |
    | Network      |
    | Link         |
    | Physical     |

    When taken together, the protocols of the various layers are called the
    ~protocol stack~. The Internet protocol stack consists of five layers: the
    physical, link, network, transport, and application layers


***** Application Layer

The application layer is where network applications and their
application-layer protocols reside.

The Internet’s application layer includes many protocols, such as the
- HTTP protocol (which provides for Web document request and transfer),
- SMTP (which provides for the transfer of e-mail messages), and
- FTP (which provides for the transfer of files between two end systems).
- DNS (which translates human-friendly names for Internet end systems like
  www.ietf.org to a 32-bit network address)

  An application-layer protocol is distributed over multiple end systems,
  with the application in one end system using the protocol to exchange
  packets of information with the application in another end system. We’ll
  refer to this packet of information at the application layer as a
  ~message~.

***** Transport Layer

The Internet’s transport layer transports application-layer messages
between application endpoints. In the Internet there are two transport
protocols, TCP and UDP, either of which can transport application- layer
messages.

TCP provides a ­ connection-oriented service to its applications. This
service includes guaranteed delivery of application-layer messages to the
destination and flow control (that is, sender/receiver speed matching). TCP
also breaks long messages into shorter ­ segments and provides a
congestion-control mechanism, so that a source throttles its transmission
rate when the network is congested.

The UDP protocol provides a connectionless service to its
applications. This is a no-frills service that provides no reliability, no
flow control, and no congestion control. In this book, we’ll refer to a
transport-layer packet as a ~segment~.

***** Network Layer

The network layer is responsible for moving network-layer packets known as
~datagrams~ from one host to another. The Internet transport-layer protocol
(TCP or UDP) in a source host passes a transport-layer segment and a
destination address to the network layer, just as you would give the postal
service a letter with a destination address. The network layer then
provides the service of delivering the segment to the transport layer in
the destination host.

The Internet’s network layer includes the celebrated IP protocol, which
defines the fields in the datagram as well as how the end systems and
routers act on these fields. There is only one IP protocol, and all
Internet components that have a network layer must run the IP
protocol. The Internet’s network layer also contains routing protocols
that determine the routes that datagrams take between sources and
destinations.

***** Link Layer

delivers the datagram to the next node along the route.

The services provided by the link layer depend on the specific link-layer
protocol that is employed over the link. For example, some link-layer
protocols provide reliable delivery, from transmitting node, over one
link, to receiving node. Note that this reliable delivery service is
different from the reliable delivery service of TCP, which provides
reliable delivery from one end system to another.

Examples of link-layer protocols include Ethernet, WiFi, and the cable
access network’s DOCSIS protocol.

As datagrams typically need to traverse several links to travel from
source to destination, a datagram may be handled by different link-layer
protocols at different links along its route.

link-layer packets are refered as ~frames~.

***** Physical Layer
While the job of the link layer is to move entire frames from one network
element to an adjacent network element, the job of the physical layer is
to move the individual bits within the frame from one node to the next.

The protocols in this layer are again link dependent and further depend on
the actual transmission medium of the link (for example, twisted-pair
copper wire, single-mode fiber optics).

For example, Ethernet has many physical-layer protocols: one for
twisted-pair copper wire, another for coaxial cable, another for fiber,
and so on. In each case, a bit is moved across the link in a different
way.

***** Open Systems Interconnection (OSI) Model

propuesto por la International Organization for Standardization (ISO).

tiene 7 capas.

The functionality of five of these layers is roughly the same as their
similarly named Internet counterparts.

The role of the presentation layer is to provide services that allow
communicating applications to interpret the meaning of data
exchanged. These services include data compression and data encryption as
well as data description (which frees the applications from having to worry
about the internal format in which data are represented/stored-formats that
may differ from one computer to another).

The session layer provides for delimiting and synchronization of data
exchange, including the means to build a checkpointing and recovery scheme.

In the Internet model, these services are delegated to the Application
Layer.

*** Encapsulado

Figure 1.24 shows the physical path that data takes down a sending end
system’s protocol stack, up and down the protocol stacks of an intervening
link-layer switch and router, and then up the protocol stack at the receiving
end system.

imagen 1.24

- link-layer switches implement layers 1 and 2;
- routers implement layers 1 through 3.

  Internet routers are capable of implementing the IP protocol (a layer 3
  protocol), while link-layer switches are not.

  #+begin_quote
  Note that hosts implement all five layers; this is consistent with the view
  that the Internet architecture puts much of its complexity at the edges of
  the network.
  #+end_quote


Figure 1.24 also illustrates the important concept of ~encapsulation~. At the
sending host, an ~application-layer message~ is passed to the transport
layer. In the simplest case, the transport layer takes the message and
appends additional information that will be used by the receiver-side
transport layer. The application-layer message and the transport-layer header
information together constitute the ~transport-layer segment~. The
transport-layer segment thus encapsulates the application-layer message.

The added information might include information allowing the receiver-side
transport layer to deliver the message up to the appropriate application, and
error-detection bits that allow the receiver to determine whether bits in the
message have been changed in route.

The transport layer then passes the segment to the network layer, which adds
network-layer header information such as source and destination end system
addresses, creating a ~network-layer datagram~.

The datagram is then passed to the link layer, which will add its own
link-layer header information and create a ~link-layer frame~.

at each layer, a packet has two types of fields: header fields and a ~payload
field~. The payload is typically a packet from the layer above.

The process of encapsulation can be more complex than that described
above. For example, a large message may be divided into multiple
transport-layer segments (which might themselves each be divided into
multiple network-layer datagrams). At the receiving end, such a segment must
then be reconstructed from its constituent datagrams.

** TODO Networks Under Attack

Viruses are malware that require some form of user interaction to infect the
user’s device. The classic example is an e-mail attachment containing
malicious executable code. If a user receives and opens such an attachment,
the user inadvertently runs the malware on the device.  Typically, such e-mail
viruses are self-replicating: once executed, the virus may send an identical
message with an identical malicious attachment to, for example, every
recipient in the user’s address book.

Worms are malware that can enter a device without any explicit user
interaction. For example, a user may be running a vulnerable network
application to which an attacker can send malware. In some cases, without any
user intervention, the application may accept the malware from the Internet
and run it, creating a worm. The worm in the newly infected device then scans
the Internet, searching for other hosts running the same vulnerable network
application. When it finds other vulnerable hosts, it sends a copy of itself
to those hosts.

Another broad class of security threats are known as ~denial-of-service (DoS)~
attacks. As the name suggests, a DoS attack renders a network, host, or other
piece of infrastructure unusable by legitimate users. Web servers, e-mail
servers, DNS servers, and institutional networks can all be subject to DoS
attacks. Internet DoS attacks are extremely common, with thousands of DoS
ttacks occurring every year.

Most Internet DoS attacks fall into one of three categories:
- Vulnerability attack :: This involves sending a few well-crafted messages to
  a vulnerable application or operating system running on a targeted host. If
  the right sequence of packets is sent to a vulnerable application or
  operating system, the service can stop or, worse, the host can crash.
- Bandwidth flooding :: The attacker sends a deluge of packets to the targeted
  host-so many packets that the target’s access link becomes clogged,
  preventing legitimate packets from reaching the server.
- Connection flooding :: The attacker establishes a large number of half-open
  or fully open TCP connections (TCP connections are discussed in Chapter 3)
  at the target host. The host can become so bogged down with these bogus
  connections that it stops accepting legitimate connections.

* KILL Capa de Aplicacion y HTTP
** Principios de aplicaciones de red

At the core of network application development is writing programs that run on
different end systems and communicate with each other over the network.

# web app
For example, in the Web-application there are two distinct programs that
communicate with each other: the *browser program* running in the user’s host
(desktop, laptop, tablet, smartphone, and so on); and the *Web server program*
running in the Web server host.

# p2p
As another example, in a P2P file-sharing system there is a program in each
host that participates in the file-sharing community. In this case, the
programs in the various hosts may be similar or identical.

Thus, when developing your new application, you need to write software that
will run on multiple end systems.

# net-edge
network-core devices do not function at the application layer but instead
function at lower layers-specifically at the network layer and below. This basic
design (confining application software to the end systems), has facilitated the
rapid development and deployment of a vast array of network applications.

imagen 2.1

*** Arquitecturas de Aplicaciones de Red

- client-server
- p2p

The ~application architecture~, is designed by the application developer and
dictates how the application is structured over the various end systems.

# client-server
In a ~client-server architecture~ , there is a host called the ~server~, which
services requests from many other hosts, called ~clients~.

- clients do not directly communicate with each other
- the server has a fixed, well-known address, called an IP address
- Web, FTP, Telnet, and e-mail
- server always-on

imagen 2.2a

# disponibilidad, data center
Often, a single-server host is incapable of keeping up with all the requests
from clients. For this reason, a data center, housing a large number of hosts,
is often used to create a powerful virtual server. A data center can have
hundreds of thousands of servers, which must be powered and maintained.
Additionally, the service providers must pay recurring interconnection and
bandwidth costs for sending data from their data centers.

# p2p
In a ~P2P architecture~, there is *minimal (or no) reliance on dedicated
servers* in data centers. Instead the application exploits *direct communication
between pairs* of intermittently connected hosts, called peers.

The peers are not owned by the service provider, but are instead desktops and
laptops controlled by users, with most of the peers residing in homes,
universities, and offices.

Because the *peers communicate without passing through a dedicated server*, the
architecture is called peer-to-peer.

One of the most compelling features of P2P architectures is their
*self-scalability*. For example, in a P2P file-sharing application, although
each peer generates workload by requesting files, each peer also adds service
capacity to the system by distributing files to other peers. P2P architectures
are also *cost effective*, since they normally don’t require significant server
infrastructure and server bandwidth (in contrast with clients-server designs
with datacenters). However, P2P applications face *challenges of security,
performance, and reliability due to their highly decentralized structure*.

# arquitecturas hibridas
some applications have ~hybrid architectures~, combining both client-server
and P2P elements. For example, for many instant messaging applications,
servers are used to track the IP addresses of users, but user-to-user
messages are sent directly between user hosts (without passing through
intermediate servers).

*** Procesos Comunicandose
it is not actually programs but ~processes~ that communicate. A process can be
thought of as *a program that is running within an end system*.

how processes running on different hosts (with potentially different operating
systems) communicate?

# mensajes
by exchanging ~messages~ across the computer network. A sending process creates
and sends messages into the network; a receiving process receives these messages
and possibly responds by sending messages back.

imagen 2.1

**** Procesos Cliente-Servidor

A network application consists of pairs of processes that send messages to
each other over a network.

For each pair of communicating processes, we typically label one of the two
processes as the client and the other process as the server.

#+begin_quote
In the context of a communication session between a pair of processes, the
process that initiates the communication (that is, initially contacts the
other process at the beginning of the session) is labeled as the client. The
process that waits to be contacted to begin the session is the server.
#+end_quote

**** La interfaz entre el Proceso y la Red (Socket)
A process sends messages into, and receives messages from, the network through a
software interface called a ~socket~.

imagen 2.3

#+BEGIN_QUOTE
a socket is the interface between the *application layer* and the *transport
layer* within a host. It is also referred to as the Application Programming
Interface (API) between the application and the network, since the socket is the
programming interface with which network applications are built.
#+END_QUOTE

The only control that the application developer has on the transport-layer side
is
1) the choice of transport protocol
2) perhaps the ability to fix a few transport-layer parameters such as maximum
   buffer and maximum segment sizes

Once the application developer chooses a transport protocol (if a choice is
available), the application is built using the transport-layer services
provided by that protocol.

**** Addressing Processes
in order for a process running on one host to send packets to a process
running on another host, the receiving process needs to have an address.

imagen 2.3
(otra vez)

# direccion del end-host, id del proceso
To *identify the receiving process*, two pieces of information need to be
specified:
1) the address of the destination host
2) an identifier that specifies the receiving process in the destination host.

# direccion IP
In the Internet, the host is identified by its ~IP address~.

an IP address is a 32-bit quantity that we can think of as uniquely
identifying the host.

# puerto
the sending process must also identify the receiving process (more specifically,
the receiving socket) running in the host. This is needed because in a host
there could be running many network applications. A destination *port number*
serves this purpose. Popular applications have been assigned specific port
numbers. (http:80, smtp:25)

*** Transport Services Available to Applications

Many networks, including the Internet, provide more than one transport-layer
protocol. When you develop an application, you must choose one of the available
transport-layer protocols. How do you make this choice? Most likely, you would
study the services provided by the available transport-layer protocols, and then
pick the protocol with the services that best match your application’s needs.

Services:
- reliable data transfer
- throughput
- timing
- security

**** Reliable Data Transfer
packets can get lost within a computer network. For example, a packet can
overflow a buffer in a router, or can be discarded by a host or router after
having some of its bits corrupted.

For many applications data loss can have devastating consequences. Thus, to
support these applications, something has to be done to *guarantee that the data
sent by one end of the application is delivered correctly and completely* to the
other end of the application. If a protocol provides such a guaranteed data
delivery service, it is said to provide ~reliable data transfer~.

**** Throughput
*the rate at which the sending process can deliver bits to the receiving
process*

Because other sessions will be sharing the bandwidth along the network path,
and because these other sessions will be coming and going, the available
throughput can fluctuate with time. These observations lead to another
natural service that a transport-layer protocol could provide, namely,
guaranteed available throughput at some specified rate.

With such a service, *the application could request a guaranteed throughput of r bits per sec, and the transport protocol would then ensure that the available throughput is always at least r bits per sec*.

# ej de uso
Such a guaranteed throughput service would appeal to many applications. For
example, if an Internet telephony application encodes voice at 32 kbps, it needs
to send data into the network and have data delivered to the receiving
application at this rate. If the transport protocol cannot provide this
throughput, the application would need to encode at a lower rate (and receive
enough throughput to sustain this lower coding rate) or may have to give up,
since, say, receiving half of the needed throughput is of little or no use to
this Internet telephony application.

Applications that have throughput requirements are said to be
~bandwidth-sensitive~ applications.

While bandwidth-sensitive applications have specific throughput requirements,
~elastic applications~ can make use of as much, or as little, throughput as
happens to be available.

**** Timing

timing guarantees can come in many shapes and forms. An example guarantee might
be that *every bit that the sender pumps into the socket arrives at the
receiver’s socket no more than 100 msec later*. Such a service would be
appealing to interactive real-time applications.

**** Security

For example, in the sending host, a transport protocol can *encrypt all data*
transmitted by the sending process, and in the receiving host, the
transport-layer protocol can *decrypt the data* before delivering the data to
the receiving process. Such a service would provide confidentiality between the
two processes, even if the data is somehow observed between sending and
receiving processes. A transport protocol can also provide other security
services in addition to confidentiality, including *data integrity* and
*end-point authentication*

*** Transport Services Provided by the Internet

protocolos
- TCP
- UDP

que servicios ofrece c/u?

# internet -> TCP UDP
The Internet (more generally, TCP/IP networks) makes two transport protocols
available to applications, UDP and TCP. When you (as an application developer)
create a new network application for the Internet, one of the first decisions
you have to make is whether to use UDP or TCP. Each of these protocols offers a
different set of services to the invoking applications.

imagen 2.4

**** TCP

- orientado a conexion
- Transferencia de Datos Confiable

The TCP service model includes a connection-oriented service and a reliable
data transfer service.

# mensajes de control antes de transferir mensajes, reservar recursos
- Connection-oriented service :: TCP has the client and server exchange
  transport-layer control information with each other before the
  application-level messages begin to flow. This so-called *handshaking*
  procedure alerts the client and server, allowing them to *prepare for* an
  onslaught of *packets*. After the handshaking phase, a TCP connection is said
  to exist between the sockets of the two processes. The connection is a
  full-duplex connection in that the two processes can send messages to each
  other over the connection at the same time. When the application finishes
  sending messages, it must tear down the connection.

# garantia de entrega, sin errores, en orden
- Reliable data transfer service ::  The communicating processes can rely on
  TCP to deliver all data sent without error and in the proper order. When
  one side of the application passes a stream of bytes into a socket, it can
  count on TCP to deliver the same stream of bytes to the receiving socket,
  with no missing or duplicate bytes.

# mecanismo de control de congestion
TCP also includes a congestion-control mechanism, that throttles a sending
process (client or server) when the network is congested between sender and
receiver.TCP congestion control also attempts to limit each TCP connection to
its fair share of network bandwidth.

**** seguridad por tcp
Neither TCP nor UDP provides any encryption.

the Internet community has developed an enhancement for TCP, called ~Secure
Sockets Layer (SSL)~. TCP-enhanced-with-SSL not only does everything that
traditional TCP does but also provides critical process-to-process security
services, including *encryption*, *data integrity*, and *end-point
authentication*.

SSL is not a third Internet transport protocol, but instead is an *enhancement
of TCP implemented in the application layer*.

if an application wants to use the services of SSL, it needs to include SSL code
in both the client and server sides of the application. SSL has its own socket
API that is similar to the traditional TCP socket API.

When an application uses SSL, the sending process passes cleartext data to
the SSL socket; SSL in the sending host then encrypts the data and passes
the encrypted data to the TCP socket. The encrypted data travels over the
Internet to the TCP socket in the receiving process. The receiving socket
passes the encrypted data to SSL, which decrypts the data. Finally, SSL
passes the cleartext data through its SSL socket to the receiving process.

**** UDP Services
UDP is a *no-frills*, lightweight transport protocol, providing minimal
services. UDP is *connectionless*, so there is *no handshaking* before the two
processes start to communicate. UDP provides an *unreliable data transfer
service* (no guarantee that the message will ever reach the receiving process).
Furthermore, *messages may arrive out of order*.

UDP does not include a congestion-control mechanism, so the sending side of UDP
can pump data into the network layer at any rate it pleases.

**** Services Not Provided by Internet Transport Protocols
today’s Internet can often provide satisfactory service to time-sensitive
applications, but it *cannot provide any timing or throughput guarantees*.

imagen 2.5

*** Protocolos de Capa de Aplicacion
But how are these messages structured? What are the meanings of the various
fields in the messages? When do the processes send the messages?

An *application-layer protocol* defines how an application’s processes, running
on different end systems, *pass messages to each other*. In particular, an
application-layer protocol defines:
- The *types of messages exchanged*, for example, request messages and response
  messages
- The *syntax of the message types*, such as the fields in the message and how
  the fields are delineated
- The *semantics of the fields*, that is, the meaning of the information in the
  fields
- *Rules for determining when and how a process sends messages and responds to
  messages*

Some application-layer protocols are specified in RFCs and are in the public
domain. For example, the Web’s application-layer protocol, HTTP (the HyperText
Transfer Protocol [RFC 2616]).

application-layer protocols \in network applications

An application-layer protocol is only one piece of a network application

** La Web y HTTP
the Web operates *on demand*. Users receive what they want, when they want it.
(vs radio & tv broadcast)

everyone can become a publisher at extremely low cost.

*** Overview of HTTP

~HyperText Transfer Protocol (HTTP)~ [RFC 1945] [RFC 2616] is implemented in two
programs: a *client* program and a *server* program. The client program and
server program, executing on different end systems, talk to each other by
exchanging HTTP messages. HTTP defines the structure of these messages and how
the client and server exchange the messages.

A ~Web page~ (also called a document) consists of objects. An ~object~ is
simply a file-such as an HTML file, a JPEG image, a Java applet, or a video
clip-that is addressable by a single URL. Most Web pages consist of a ~base
HTML file~ and several referenced objects.

# ej
For example, if a Web page contains HTML text and five JPEG images, then the Web
page has six objects: the base HTML file plus the five images. The base HTML
file references the other objects in the page with the objects’ URLs.

# url
Each URL has two components: the hostname of the server that houses the object
and the object’s path name. For example, the URL

#+begin_quote
http://www.someSchool.edu/someDepartment/picture.gif
#+end_quote

| hostname           | pathname                    |
| www.someSchool.edu | /someDepartment/picture.gif |

Web servers, which implement the server side of HTTP, house Web objects, each
addressable by a URL.

When a user requests a Web page, the browser sends HTTP request messages for the
objects in the page to the server. The server receives the requests and responds
with HTTP response messages that contain the objects.

*HTTP uses TCP as its underlying transport protocol*. The HTTP client first
initiates a TCP connection with the server. Once the connection is established,
the browser and the server processes access TCP through their socket interfaces.

imagen 2.6

*the server sends requested files to clients without storing any state
information about the client*. If a particular client asks for the same object
twice in a period of a few seconds, the server does not respond by saying that
it just served the object to the client; instead, the server resends the object,
as it has completely forgotten what it did earlier. Because an HTTP server
maintains no information about the clients, HTTP is said to be a ~stateless
protocol~. We also remark that the Web uses the client-server application
architecture

A Web server is always on, with a fixed IP address, and it services requests
from potentially millions of different browsers.

*** Non-Persistent and Persistent Connections
the client and server communicate for an extended period of time, with the
client making a series of requests and the server responding to each of the
requests.

Depending on the application and on how the application is being used, the
series of *requests may be made back-to-back, periodically at regular
intervals*, *or intermittently*.

the application developer needs to make an important decision. Should each
request/response pair be sent over a separate TCP connection, or should all of
the requests and their corresponding responses be sent over the same TCP
connection? In the former approach, the application is said to use
~non-persistent connections~; and in the latter approach, ~persistent
connections~.

**** Non-Persistent Connections

non-persistent connections, where each TCP connection is closed after the server
sends the object-the connection does not persist for other objects.

# ej
Let’s suppose the page consists of a base HTML file and 10 JPEG images, and
that all 11 of these objects reside on the same server. Further suppose the
URL for the base HTML file is

#+begin_quote
http://www.someSchool.edu/someDepartment/home.index
#+end_quote

# procedimiento
Here is what happens:
1. The HTTP client process initiates a TCP connection to the server
   www.someSchool.edu on port number 80, which is the default port number for
   HTTP. Associated with the TCP connection, there will be a socket at the
   client and a socket at the server.
2. The HTTP client sends an HTTP request message to the server via its
   socket. The request message includes the path name /someDepartment/home
   .index . (We will discuss HTTP messages in some detail below.)
3. The HTTP server process receives the request message via its socket,
   retrieves the object /someDepartment/home.index from its storage (RAM or
   disk), encapsulates the object in an HTTP response message, and sends the
   response message to the client via its socket.
4. The HTTP server process tells TCP to close the TCP connection. (But TCP
   doesn’t actually terminate the connection until it knows for sure that the
   client has received the response message intact.)
5. The HTTP client receives the response message. The TCP connection
   terminates. The message indicates that the encapsulated object is an HTML
   file. The client extracts the file from the response message, examines the
   HTML file, and finds references to the 10 JPEG objects.
6. The first four steps are then repeated for each of the referenced JPEG
   objects.

Note that each TCP connection transports exactly one request message and one
response message. Thus, in this example, when a user requests the Web page, 11
TCP connections are generated.

In the steps described above, we were intentionally vague about whether the
client obtains the 10 JPEGs over 10 serial TCP connections, or whether some of
the JPEGs are obtained over parallel TCP connections. Indeed, users can
configure modern browsers to control the degree of parallelism. the use of
parallel connections shortens the response time.

to estimate the amount of time that elapses from when a client requests the base
HTML file until the entire file is received by the client, we define the
~round-trip time (RTT)~, which is the *time it takes for a small packet to
travel from client to server and then back to the client*. The RTT includes
- packet-propagation delays,
- packet-queuing delays in intermediate routers and switches, and
- packet-processing delays.

Now consider what happens when a user clicks on a hyperlink.
- the browser to initiate a TCP connection between the browser and the Web server;
  - this involves a “three-way handshake”
    - the client sends a small TCP segment to the server,
    - the server acknowledges and responds with a small TCP segment, and, finally,
    - the client acknowledges back to the server.
  - The first two parts of the three-way handshake take one RTT.
  - After completing the first two parts of the handshake, the client sends the
    HTTP request message combined with the third part of the three-way handshake
    (the acknowledgment) into the TCP connection.
- Once the request message arrives at the server, the server sends the HTML file
  into the TCP connection. This HTTP request/response eats up another RTT.

# tiempo de http
Thus, roughly, the total response time is two RTTs plus the transmission time at
the server of the HTML file.

imagen 2.7

# desventajas
shortcomings. First,
1. a brand-new connection must be established and maintained for each requested
   object. For each of these connections, TCP buffers must be allocated and TCP
   variables must be kept in both the client and server. This can place a
   significant burden on the Web server, which may be serving requests from
   hundreds of different clients simultaneously.
2. each object suffers a delivery delay of 2 RTTs (1 RTT to establish the TCP
   connection + 1 RTT to request and receive an object).

**** HTTP with Persistent Connections

With HTTP 1.1 persistent connections, *the server leaves the TCP connection open
after sending a response*. Subsequent requests and responses between the same
client and server can be sent over the same connection.

These requests for objects can be made back-to-back, without waiting for replies
to pending requests (pipelining).

Typically, the HTTP server closes a connection when it isn’t used for a certain
time (a configurable timeout interval).

When the server receives the back-to-back requests, it sends the objects
back-to-back.

*** HTTP Message Format

The HTTP specifications [RFC 1945; RFC 2616; RFC 7540] include the definitions
of the HTTP message formats. There are two types of HTTP messages, *request
messages* and *response messages*

**** HTTP Request Message

#+BEGIN_SRC
GET /somedir/page.html HTTP/1.1
Host: www.someschool.edu
Connection: close
User-agent: Mozilla/5.0
Accept-language: fr
#+END_SRC

# ascii
First of all, we see that the message is written in ordinary ASCII text, so
that your ordinary computer-literate human being can read it.

# crlf
Second, we see that the message consists of five lines, each followed by a
carriage return and a line feed. The last line is followed by an additional
carriage return and line feed.

# request line
The first line of an HTTP request message is called the ~request line~, which
has three fields: the *method field*, the *URL field*, and the *HTTP version field*.

The method field can take on several different values, including GET, POST,
HEAD, PUT, and DELETE . The great majority of HTTP request messages use the
GET method. The GET method is used when the browser requests an object, with
the requested object identified in the URL field.

In this example, the browser is requesting the object /somedir/page.html.

The version is self-explanatory; in this example, the browser implements version
HTTP/1.1.

# header lines
the subsequent lines are called the ~header lines~.

The header line ~Host: www.someschool.edu~ specifies the host on which the
object resides. You might think that this header line is unnecessary, as there
is already a TCP connection in place to the host. But the information provided
by the host header line is required by *Web proxy caches*.

By including the ~Connection: close~ header line, the browser is telling the
server that it wants the server to close the connection after sending the
requested object (no persistent).

The ~User- agent:~ header line specifies the user agent, (the browser type that
is making the request to the server). Here the user agent is Mozilla/5.0, a
Firefox browser. This header line is useful because the server can actually send
different versions of the same object to different types of user agents. (Each
of the versions is addressed by the same URL.)

Finally, the ~Accept-language:~ header indicates that the user prefers to
receive a French version of the object, if such an object exists on the
server; otherwise, the server should send its default version.

#+caption: general format of a request message
imagen 2.8

# entity body (of request message)
after the header lines (and the additional carriage return and line feed) there
is an *entity body*. The entity body is empty with the GET method, but is used
with the POST method. An HTTP client often uses the POST method when the user
fills out a form-for example, when a user provides search words to a search
engine. With a POST message, the user is still requesting a Web page from the
server, but the specific contents of the Web page depend on what the user
entered into the form fields. If the value of the method field is POST , then
the entity body contains what the user entered into the form fields.

**** HTTP Response Message

This response message could be the response to the example request message
just discussed.

#+BEGIN_SRC
HTTP/1.1 200 OK
Connection: close
Date: Tue, 18 Aug 2015 15:44:04 GMT
Server: Apache/2.2.3 (CentOS)
Last-Modified: Tue, 18 Aug 2015 15:11:03 GMT
Content-Length: 6821
Content-Type: text/html
(data data data data data ...)
#+END_SRC

It has three sections: an initial ~status line~, six ~header lines~, and
then the ~entity body~.

# entity body
The entity body is the meat of the message - it contains the requested object
itself (represented by ~(data data data data data ... )~).

# status line
The status line has three fields: the *protocol version field*, a *status code*,
and a *corresponding status message*. In this example, the status line indicates
that the server is using HTTP/1.1 and that everything is OK (that is, the server
has found, and is sending, the requested object).

The status code and associated phrase indicate the result of the
request. Some common status codes and associated phrases include:
- 200 OK: Request succeeded and the information is returned in the response.
- 301 Moved Permanently: Requested object has been permanently moved; the new
  URL is specified in Location : header of the response message. The client
  software will automatically retrieve the new URL.
- 400 Bad Request: This is a generic error code indicating that the request
  could not be understood by the server.
- 404 Not Found: The requested document does not exist on this server.
- 505 HTTP Version Not Supported: The requested HTTP protocol version is not
  supported by the server.

# header lines
The server uses the ~Connection: close~ header line to tell the client that
it is going to close the TCP connection after sending the message.

The ~Date:~ header line indicates the time and date when the HTTP response
was created and sent by the server. Note that this is the time when the
server retrieves the object from its file system, inserts the object into
the response message, and sends the response message.

The ~Server:~ header line indicates that the message was generated by an
Apache Web server; it is analogous to the ~User-agent:~ header line in the
HTTP request message.

The ~Last-Modified:~ header line indicates the time and date when the object
was created or last modified. The ~Last-Modified:~ header, which we will
soon cover in more detail, is critical for object caching, both in the local
client and in network cache servers (also known as proxy servers).

The ~Content-Length:~ header line indicates the number of bytes in the
object being sent. The ~Content-Type:~ header line indicates that the object
in the entity body is HTML text.

#+caption: general format of a response message
imagen 2.9

*** User-Server Interaction: Cookies

an HTTP server is *stateless; This simplifies server design* and has permitted
engineers to develop high-performance Web servers that can handle thousands of
simultaneous TCP connections.

However, it is often *desirable to identify users*, either because the server
wishes to *restrict user access* or because it wants to *serve individual
content*. For these purposes, HTTP uses *cookies*, defined in [RFC 6265], which
*allow sites to keep track of users*.

cookie technology has four components:
1) a cookie header line in the HTTP response message;
2) a cookie header line in the HTTP request message;
3) a cookie file kept on the user’s end system and managed by the user’s
   browser;
4) a back-end database at the Web site.

imagen 2.10

# ej
let’s walk through an example of how cookies work. Suppose Susan, who always
accesses the Web using Internet Explorer from her home PC, contacts
Amazon.com for the first time.  Let us suppose that in the past she has
already visited the eBay site. When the request comes into the Amazon Web
server, the server creates a unique identification number and creates an
entry in its back- end database that is indexed by the identification
number. The Amazon Web server then responds to Susan’s browser, including in
the HTTP response a ~Set-cookie:~ header, which contains the identification
number. For example, the header line might be: ~Set-cookie: 1678~

When Susan’s browser receives the HTTP response message, it sees the
Set-cookie: header. The browser then appends a line to the special cookie
file that it manages. This line includes the hostname of the server and the
identification number in the Set-cookie: header. Note that the cookie file
already has an entry for eBay, since Susan has visited that site in the
past. As Susan continues to browse the Amazon site, each time she requests a
Web page, her browser consults her cookie file, extracts her identification
number for this site, and puts a cookie header line that includes the
identification number in the HTTP request. Specifically, each of her HTTP
requests to the Amazon server includes the header line: ~Cookie: 1678~

In this manner, the Amazon server is able to track Susan’s activity at the
Amazon site. Although the Amazon Web site does not necessarily know Susan’s
name, it knows exactly which pages user 1678 visited, in which order, and at
what times! Amazon uses cookies to provide its shopping cart service- Amazon
can maintain a list of all of Susan’s intended purchases, so that she can pay
for them collectively at the end of the session.

If Susan returns to Amazon’s site, say, one week later, her browser will
continue to put the header line Cookie: 1678 in the request messages. Amazon
also recommends products to Susan based on Web pages she has visited at
Amazon in the past. If Susan also registers herself with Amazon- providing
full name, e-mail address, postal address, and credit card information-Amazon
can then include this information in its database, thereby associating
Susan’s name with her identification number (and all of the pages she has
visited at the site in the past!). This is how Amazon and other e-commerce
sites provide “one-click shopping”-when Susan chooses to purchase an item
during a subsequent visit, she doesn’t need to re-enter her name, credit card
number, or address.


From this discussion we see that *cookies can be used to identify a user*. The
first time a user visits a site, the user can provide a user identification
(possibly his or her name). During the subsequent sessions, the browser
passes a cookie header to the server, thereby identifying the user to the
server.  *Cookies can thus be used to create a user session layer on top of
stateless HTTP*. For example, when a user logs in to a Web-based e-mail
application (such as Hotmail), the browser sends cookie information to the
server, permitting the server to identify the user throughout the user’s
session with the application.

*** Web Cache
A ~Web cache~ aka ~proxy server~ is a *network entity that satisfies HTTP
requests on the behalf of an origin Web server*. *The Web cache has its own disk
storage and keeps copies of recently requested objects in this storage.*

a user’s browser can be configured so that all of the user’s HTTP requests are
first directed to the Web cache. Once a browser is configured, *each browser
request for an object is first directed to the Web cache*. As an example,
suppose a browser is requesting the object http://www.someschool.edu/campus.gif.
Here is what happens:

# procedimiento
1. The browser establishes a TCP connection to the Web cache and sends an
   HTTP request for the object to the Web cache.
2. The Web cache checks to see if it has a copy of the object stored
   locally. If it does, the Web cache returns the object within an HTTP
   response message to the client browser.
3. If the Web cache does not have the object, the Web cache opens a TCP
   connection to the origin server, that is, to www.someschool.edu . The Web
   cache then sends an HTTP request for the object into the cache-to-server
   TCP connection. After receiving this request, the origin server sends the
   object within an HTTP response to the Web cache.
4. When the Web cache receives the object, it stores a copy in its local
   storage and sends a copy, within an HTTP response message, to the client
   browser (over the existing TCP connection between the client browser and
   the Web cache).

imagen 2.11

Note that a *cache is both a server and a client at the same time*. When it
receives requests from and sends responses to a browser, it is a server. When it
sends requests to and receives responses from an origin server, it is a client.

# motivacion
Web caching has seen deployment in the Internet for two reasons:
1. a Web cache can substantially *reduce the response time* for a client
   request, particularly if the bottleneck bandwidth between the client and the
   origin server is much less than the bottleneck bandwidth between the client
   and the cache. If there is a high-speed connection between the client and the
   cache, as there often is, and if the cache has the requested object, then the
   cache will be able to deliver the object rapidly to the client.
2. Web caches can *substantially reduce traffic* on an institution’s access link
   to the Internet. By reducing traffic, the institution (for example, a company
   or a university) does not have to upgrade bandwidth as quickly, thereby
   *reducing costs*.

   Furthermore, Web caches can substantially reduce Web traffic in the Internet
   as a whole, thereby improving performance for all applications.

Through the use of ~Content Distribution Networks (CDNs)~, Web caches are
increasingly playing an important role in the Internet. A CDN company installs
many geographically distributed caches throughout the Internet, thereby
localizing much of the traffic.

**** The Conditional GET

mecanismo para actualizar la cache

caching introduces a new problem. the copy of an object residing in the cache
may be stale. In other words, *the object housed in the Web server may have been
modified since the copy was cached at the client*. Fortunately, HTTP has a
mechanism that allows a cache to verify that its objects are up to date. This
mechanism is called the ~conditional GET~.

An HTTP request message is a so-called conditional GET message if
1) the request message uses the GET method and
2) the request message includes an ~If-Modified-Since:~ header line.

# ej, procedimiento
First, on the behalf of a requesting browser, a proxy cache sends a request
message to a Web server:

#+BEGIN_SRC
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
#+END_SRC

Second, the Web server sends a response message with the requested object to
the cache:

#+BEGIN_SRC
HTTP/1.1 200 OK
Date: Sat, 3 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)
Last-Modified: Wed, 9 Sep 2015 09:23:24
Content-Type: image/gif
(data data data data data ...)
#+END_SRC

The cache forwards the object to the requesting browser but also caches the
object locally. Importantly, the cache also stores the last-modified date
along with the object.

Third, one week later, another browser requests the same object via the cache,
and the object is still in the cache. Since this object may have been modified
at the Web server in the past week, the cache performs an up-to-date check by
issuing a conditional GET. Specifically, the cache sends:

#+BEGIN_SRC
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
If-modified-since: Wed, 9 Sep 2015 09:23:24
#+END_SRC

Note that the value of the If-modified-since: header line is exactly equal to
the value of the Last-Modified: header line that was sent by the server one week
ago. *This conditional GET is telling the server to send the object only if the
object has been modified since the specified date*.

Suppose the object has not been modified since 9 Sep 2015 09:23:24.

Then, fourth, the Web server sends a response message to the cache:

#+BEGIN_SRC
HTTP/1.1 304 Not Modified
Date: Sat, 10 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)
(empty entity body)
#+END_SRC

We see that in response to the conditional GET, the Web server still sends a
response message but does not include the requested object in the response
message. Including the requested object would only waste bandwidth and
increase user-perceived response time, particularly if the object is
large. Note that this last response message has 304 Not Modified in the
status line, which tells the cache that it can go ahead and forward its (the
proxy cache’s) cached copy of the object to the requesting browser.

* SMTP y DNS
** Electronic Mail in the Internet
e-mail is an *asynchronous communication medium*; people send and read messages
when it is convenient for them, without having to coordinate with other people’s
schedules.

imagen 2.14

# componentes
it has three major components:
- ~user agents~,
- ~mail servers~, and
- ~Simple Mail Transfer Protocol (SMTP)~

# user-agent
User agents allow users to :
- read,
- reply to,
- forward,
- save, and
- compose messages.

# ej
Alice, sending an e-mail message to a recipient, Bob.

When Alice is finished composing her message, her user agent sends the message
to her mail server, where the message is placed in the mail server’s outgoing
message queue. When Bob wants to read a message, his user agent retrieves the
message from his mailbox in his mail server.

# mailbox
Mail servers form the core of the e-mail infrastructure. Each recipient, has a
~mailbox~ located in one of the mail servers. *A mailbox manages and maintains
the messages that have been sent* to him.

# procedimiento
A typical message starts its journey in
- the sender’s user agent,
- travels to the sender’s mail server, and
- travels to the recipient’s mail server, and
- is deposited in the recipient’s mailbox.
- When Bob wants to access the messages in his mailbox, the mail server
  containing his mailbox authenticates Bob (with usernames and passwords).

# reintentos, fallas
*Alice’s mail server must also deal with failures in Bob’s mail server*. If
Alice’s server cannot deliver mail to Bob’s server, Alice’s server holds the
message in a ~message queue~ and attempts to transfer the message later.
Reattempts are often done every 30 minutes or so; if there is no success after
several days, the server removes the message and notifies the sender (Alice)
with an e-mail message.

# TCP
SMTP is the principal application-layer protocol for Internet electronic mail.
It uses the *reliable data transfer service of TCP* to transfer mail from the
sender’s mail server to the recipient’s mail server.

# cliente-servidor
As with most application-layer protocols, SMTP has two sides:
- a client side, which executes on the sender’s mail server, and
- a server side, which executes on the recipient’s mail server.

*Both* the client and server sides of SMTP run on every mail server. When a
mail server sends mail to other mail servers, it acts as an SMTP client. When
a mail server receives mail from other mail servers, it acts as an SMTP
server.

*** SMTP [RFC 5321]

# 7 bit ascii
restricts the contents of mail messages to simple *7-bit ASCII*.
This restriction made sense in the early 1980s when *transmission capacity was
scarce* and no one was sending large attachments files (image, audio, video).
But today, it requires binary multimedia data to be encoded to ASCII before
being sent over SMTP; and it requires the corresponding ASCII message to be
decoded back to binary after SMTP transport.

# procedimiento
Suppose Alice wants to send Bob a simple ASCII message.
1. Alice invokes her user agent for e-mail, provides Bob’s e-mail address
   (for example, bob@someschool.edu ), composes a message, and instructs the
   user agent to send the message.
2. Alice’s user agent sends the message to her mail server, where it is
   placed in a message queue.
3. The client side of SMTP, running on Alice’s mail server, sees the message
   in the message queue. It opens a TCP connection to an SMTP server, running
   on Bob’s mail server.
4. After some initial SMTP handshaking, the SMTP client sends Alice’s message
   into the TCP connection.
5. At Bob’s mail server, the server side of SMTP receives the message. Bob’s
   mail server then places the message in Bob’s mailbox.
6. Bob invokes his user agent to read the message at his convenience.

imagen 2.15

# end-to-end, sin intermediarios
SMTP does not use intermediate mail servers for sending mail. The TCP connection
is a direct connection between the servers. In particular, if Bob’s mail server
is down, the message remains in Alice’s mail server and waits for a new attempt.
The message does not get placed in some intermediate mail server.

# procedimiento mail-server a mail-server
Let’s now take a closer look at how SMTP transfers a message from a sending
mail server to a receiving mail server.
1. The client SMTP (running on the sending mail server host) has TCP
   establish a connection to port 25 at the server SMTP (running on the
   receiving mail server host).
   - If the server is down, the client tries again later.
2. Once this connection is established, the server and client perform some
   application-layer handshaking.
   - During this SMTP handshaking phase, the SMTP client indicates the e-mail
     address of the sender (the person who generated the message) and the
     e-mail address of the recipient.
3. Once the SMTP client and server have introduced themselves to each other, the
   client sends the message.

SMTP can count on the reliable data transfer service of TCP to get the
message to the server without errors. The client then repeats this process
over the same TCP connection if it has other messages to send to the server;
otherwise, it instructs TCP to close the connection.

#+caption: exchanged messages between Client (C) with hostname ~crepes.fr~ and Server (S) with hostname ~hamburger.edu~.
| server                                           | client                       |
|--------------------------------------------------+------------------------------|
| 220 hamburger.edu                                |                              |
|                                                  | HELO crepes.fr               |
| 250 Hello crepes.fr, pleased to meet you         |                              |
|                                                  | MAIL FROM: <alice@crepes.fr> |
| 250 alice@crepes.fr ... Sender ok                |                              |
|                                                  | RCPT TO: <bob@hamburger.edu> |
| 250 bob@hamburger.edu ... Recipient ok           |                              |
|                                                  | DATA                         |
| 354 Enter mail, end with ”.” on a line by itself |                              |
|                                                  | Do you like ketchup?         |
|                                                  | How about pickles?           |
|                                                  | .                            |
| 250 Message accepted for delivery                |                              |
|                                                  | QUIT                         |
| 221 hamburger.edu closing connection             |                              |

# comandos
the client issued five commands: HELO (an abbreviation for HELLO), MAIL FROM,
RCPT TO, DATA, and QUIT .

The client also sends a line consisting of a single period, which indicates the
end of the message to the server (each message ends with CRLF.CRLF , where CR
and LF stand for carriage return and line feed). The server issues replies to
each command, with each reply having a reply code and some (optional)
explanation.

As *SMTP uses persistent connections*: If the sending mail server has several
messages to send to the same receiving mail server, it can send all of the
messages over the same TCP connection. For each message, the client begins the
process with a new MAIL FROM: crepes.fr , designates the end of message with an
isolated period, and issues QUIT only after all messages have been sent.

*** Comparison with HTTP

Both protocols are used to transfer files from one host to another:
- HTTP transfers files (also called objects) from a Web server to a Web client
  (typically a browser);
- SMTP transfers files (that is, e-mail messages) from one mail server to
  another mail server. When transferring the files, both persistent HTTP and
  SMTP use ~persistent connections~.

However, there are important differences.

1. HTTP is mainly a ~pull protocol~ - someone loads information on a Web server
   and users use HTTP to pull the information from the server at their
   convenience. In particular, the TCP connection is initiated by the machine
   that wants to receive the file. On the other hand, SMTP is primarily a ~push
   protocol~ — the sending mail server pushes the file to the receiving mail
   server. In particular, the TCP connection is initiated by the machine that
   wants to send the file.
2. SMTP requires each message, including the body of each message, to be in
   7-bit ASCII format. If the message contains characters that are not 7-bit
   ASCII (for example, French characters with accents) or contains binary data
   (such as an image file), then the message has to be encoded into 7-bit ASCII.
   HTTP data does not impose this restriction.
3. difference in how a document of text and images (and/or other media types) is
   handled. HTTP encapsulates each object in its own HTTP response message. SMTP
   places all of the message’s objects into one message.

*** Mail Message Formats
when an e-mail message is sent from one person to another, a *header containing
peripheral information* [RFC 5322] (receiver address, return address, date, etc.)
precedes the body of the message itself. This peripheral information is
contained in a series of *header lines*.

RFC 5322 specifies the exact format for mail header lines as well as their
semantic interpretations. As with HTTP, each header line contains readable
text, consisting of a keyword followed by a colon followed by a value.

# campos obligatorios
Every header must have a ~From:~ header line and a ~To:~ header line; a header
may include a ~Subject:~ header line as well as other optional header lines. It
is important to note that *these header lines are different from the SMTP
commands we studied before*. *The commands in that section were part of the ~SMTP handshaking protocol~; the header lines examined in this section are part of the mail message itself*.

# ej
A typical message header looks like this:

#+BEGIN_SRC
From: alice@crepes.fr
To: bob@hamburger.edu
Subject: Searching for the meaning of life.
#+END_SRC

After the message header, a blank line follows (CLRF); then the message body
(in ASCII) follows.

# agregado mio sobre [RFC 5321] y [RFC 5322]
del articulo sobre smtp en wikipedia
#+begin_quote
SMTP defines message transport, not the message content. Thus, it defines the
mail envelope and its parameters, such as the envelope sender, but not the
header (except trace information) nor the body of the message itself. STD 10
and RFC 5321 define SMTP (the envelope), while STD 11 and RFC 5322 define the
message (header and body), formally referred to as the Internet Message
Format.
#+end_quote

*** Mail Access Protocols

Como acceder al mail?
- IMAP
- POP3
- HTTP

Once SMTP delivers the message from Alice’s mail server to Bob’s mail server,
the message is placed in Bob’s mailbox.

today, mail access uses a client-server architecture—the typical *user reads
e-mail with a client that executes on the user’s end system*.

como el usuario accede al mail?
- colocar un mail server en la PC del usuario no es viable, pues los
  mail-servers son cliente y servidor, por lo que la PC del usuario deberia
  estar always-on, reintentar enviar emails, etc.
- se usa un user-agent que se ejecuta en la PC del usuario. Este accede al mailbox de un mail-server provisto por un tercero (ISP, empresa, etc.) (~always-on~, ~shared~)

# smtp usario-userAgent, smtp userAgent-mailServerA, smtp mailServerA-mailServerB
Now let’s consider the path an e-mail message takes when it is sent from
Alice to Bob. We just learned that at some point along the path the e-mail
message needs to be deposited in Bob’s mail server. This could be done simply
by having Alice’s user agent send the message directly to Bob’s mail
server. And this could be done with SMTP—indeed, SMTP has been designed for
pushing e-mail from one host to another. However, typically the sender’s user
agent does not dialogue directly with the recipient’s mail server. Instead,
as shown in Figure 2.16, Alice’s user agent uses SMTP to push the e-mail
message into her mail server, then Alice’s mail server uses SMTP (as an SMTP
client) to relay the e-mail message to Bob’s mail server. Why the two-step
procedure? Primarily because without relaying through Alice’s mail server,
Alice’s user agent doesn’t have any recourse to an unreachable destination
mail server. By having Alice first deposit the e-mail in her own mail server,
Alice’s mail server can repeatedly try to send the message to Bob’s mail
server, say every 30 minutes, until Bob’s mail server becomes operational.

imagen 2.16

*How does a recipient like Bob, running a user agent on his local PC, obtain his
messages, which are sitting in a mail server within Bob’s ISP?* Note that Bob’s
*(recipient) user agent can’t use SMTP to obtain the messages because obtaining
the messages is a pull operation, whereas SMTP is a push protocol*. Special mail
access protocol that transfers messages from Bob’s mail server to his local PC.
There are currently a number of popular mail access protocols, including ~Post
Office Protocol—Version 3 (POP3)~, ~Internet Mail Access Protocol (IMAP)~, and
~HTTP~.

**** POP3 [RFC 1939]

arquitectura cliente servidor, donde el user-agent es el cliente.

POP3 comienza cuando el user-agent abre una conexion tcp con el mail
server en el puerto 110.

luego de establecer la conexion, pop3 inicia 3 fases:
1. Autorizacion: el user-agent envia un nombre de usuario y contraseña para
   autenticar al usuario. Comandos:  ~user <username>~, ~pass <password>~
2. Transaccion: el user-agent retira los mensajes; tambien puede
   marcar/desmarcar mensajes para eliminarlos y obtener estadisticas del
   mail. Comandos: ~list~, ~retr~, ~dele~, ~quit~. El user-agent puede ser
   configurado para:
   + =Download and delete=
     #+caption: interaccion entre cliente(C) y servidor (S)
     | Servidor                    | Cliente |
     |                             | list    |
     | 1 498                       |         |
     | 2 912                       |         |
     | .                           |         |
     |                             | retr 1  |
     | (blah blah ...              |         |
     | .................           |         |
     | ..........blah)             |         |
     | .                           |         |
     |                             | dele 1  |
     |                             | retr 2  |
     | (blah blah ...              |         |
     | .................           |         |
     | ..........blah)             |         |
     | .                           |         |
     |                             | dele 2  |
     |                             | quit    |
     | +OK POP3 server signing off |         |

     A problem with this download-and-delete mode the recipient may want to
     access his mail messages from multiple machines. The mode partitions the
     user’s mail messages over the machines; in particular, if the user first
     reads a message on his office PC, he will not be able to reread the
     message from his portable at home later in the evening. In the
     download-and-keep mode, the user agent leaves the messages on the mail
     server after downloading them. In this case, the user can reread
     messages from different machines;
   + =Download and keep=
3. =Update=: ocurre luego de que el cliente ejecuta el comando ~quit~,
   finalizando la sesion POP3. en este momento, el mail server elimina los
   mensajes que fueron marcados para borrarse.

El servidor responde a cada comando con una respuesta:
- ~+OK~, para indicar que el comando fue exitoso
- ~-ERR~, para indicar que hubo un error

  Durante la sesion, el server mantiene el estado de la sesion. lleva registro
  de los mensajes marcados para su eliminacion, etc. sin embargo no se
  mantiene un estado entre sesiones.

**** IMAP [RFC 3501]
# carpetas!
IMAP fue creado para proveer al usuario de herramientas para *crear carpetas
remotas y asignar mensajes a dichas carpetas*, lo que pop3 no puede hacer.

en imap, el servidor asocia cada mensaje con una carpeta. por defecto se
asocia a INBOX.

# ctrl+f
IMAP also provides commands that allow users to *search remote folders for
messages matching specific criteria*.

an _IMAP server_[fn:imapServer] *maintains user state information across IMAP
sessions*. for example, the names of the folders and which messages are
associated with which folders.

[fn:imapServer]
IMAP (MAIL) server, right?


# mensaje por partes - bueno para conexiones lentas, bajo bandwidth
Another important feature of IMAP is that it has commands that *permit a
user-agent to obtain components of messages*. For example, a user agent can
obtain just the message header of a message or just one part of a multipart
MIME message. This feature is useful when there is a low-bandwidth
connection (for example, a slow-speed modem link) between the user agent and
its mail server. With a low-bandwidth connection, the user may not want to
download all of the messages in its mailbox, particularly avoiding long
messages that might contain, for example, an audio or video clip.

**** Web-based E-Mail
With this service, *the user agent is an ordinary Web browser*, and the user
communicates with its remote mailbox via HTTP. When a recipient, such as
Bob, wants to access a message in his mailbox, the e-mail message is sent
from Bob’s mail server to Bob’s browser using the HTTP protocol rather than
the POP3 or IMAP protocol.  When a sender, such as Alice, wants to send an
e-mail message, the e-mail message is sent from her browser to her mail
server over HTTP rather than over SMTP. Alice’s mail server, however, still
sends messages to, and receives messages from, other mail servers using
SMTP.

** DNS - The Internet's Directory Service [RFC 1034] [RFC 1035]
# hostnames son mas faciles
Internet hosts can be identified in many ways. One identifier for a host is
its ~hostname~ . Hostnames — such as www.facebook.com, www.google.com ,
gaia.cs.umass.edu —are mnemonic and are therefore appreciated by humans.

# direcciones ip
However, hostnames provide little, if any, information about the location
within the Internet of the host. (A hostname such as www.eurecom.fr , which
ends with the country code .fr , tells us that the host is probably in France,
but doesn’t say much more.) Furthermore, because hostnames can consist of
variable-length alphanumeric characters, they would be difficult to process by
routers. For these reasons, hosts are also identified by so-called ~IP
addresses~.

An IP address consists of four bytes and has a rigid hierarchical structure.
the bytes expressed in decimal notation from 0 to 255. An IP address is
hierarchical because as we scan the address from left to right, we obtain more
and more specific information about where the host is located in the Internet
(that is, within which network, in the network of networks).

*** TODO Services provided by DNS
# traduccion entre hostname y direccion IP
People prefer the more mnemonic hostname identifier, while routers prefer
fixed-length, hierarchically structured IP addresses. In order to reconcile
these preferences, we need a directory service that translates hostnames to
IP addresses. This is the main task of the Internet’s ~domain name system
(DNS)~.

The DNS is
1. a *distributed database* implemented in a hierarchy of DNS servers, and
2. an *application-layer protocol that allows hosts to query the distributed
   database*. The DNS servers are often UNIX machines running the Berkeley
   Internet Name Domain (BIND) software [BIND 2016]. The DNS protocol runs over
   UDP and uses port 53.

# ej
DNS is commonly employed by other application-layer protocols (HTTP, SMTP, ...)
to translate user-supplied hostnames to IP addresses. As an example, consider
what happens when a browser (that is, an HTTP client), running on some user’s
host, requests the URL www.someschool.edu/index.html. In order for the user’s
host to be able to send an HTTP request message to the Web server
www.someschool.edu , the user’s host must first obtain the IP address of
www.someschool.edu .

This is done as follows.
1. The same user machine runs the client side of the DNS application.
2. The browser extracts the hostname, www.someschool.edu , from the URL and
   passes the hostname to the client side of the DNS application.
3. The DNS client sends a query containing the hostname to a DNS server.
4. The DNS client eventually receives a reply, which includes the IP address
   for the hostname.
5. Once the browser receives the IP address from DNS, it can initiate a TCP
   connection to the HTTP server process located at port 80 at that IP
   address.

# otros servicios
DNS provides a few other important services in addition to translating
hostnames to IP addresses:
- Host aliasing :: A host with a complicated hostname can have one or more alias
  names. For example, a hostname such as relay1.west-coast.enterprise.com could
  have, say, two aliases such as enterprise.com and www.enterprise.com . In this
  case, the hostname relay1.west-coast.enterprise.com is said to be a ~canonical
  hostname~. ~Alias hostnames~, when present, are typically more mnemonic than
  canonical hostnames. *DNS can be invoked by an application to obtain the canonical hostname for a supplied alias hostname as well as the IP address of the host*.
- Mail server aliasing :: it is desirable that e-mail addresses be mnemonic. For
  example, if Bob has an account with Yahoo Mail, Bob’s e-mail address might be
  as simple as bob@yahoo.mail. However, the hostname of the Yahoo mail server is
  more complicated and much less mnemonic than simply yahoo.com (for example,
  the canonical hostname might be something like relay1.west-coast.yahoo.com ).
  DNS can be invoked by a mail application to obtain the canonical hostname for
  a supplied alias hostname as well as the IP address of the host. In fact, the
  MX record (see below) permits a company’s mail server and Web server to have
  identical (aliased) hostnames; for example, a company’s Web server and mail
  server can both be called enterprise.com.
- Load distribution :: DNS is also used to perform load distribution among
  replicated servers, such as replicated *Web* servers. Busy sites, such as
  cnn.com, are replicated over multiple servers, with each server running on a
  different end system and each having a different IP address. For replicated
  Web servers, *a set of IP addresses is thus associated with one canonical
  hostname*. The DNS database contains this set of IP addresses. When clients
  make a DNS query for a name mapped to a set of addresses, the server responds
  with the entire set of IP addresses, but rotates the ordering of the addresses
  within each reply. Because a client typically sends its HTTP request message
  to the IP address that is listed first in the set, DNS rotation distributes
  the traffic among the replicated servers. DNS rotation is also used for e-mail
  so that multiple mail servers can have the same alias name. Also, content
  distribution companies such as Akamai have used DNS in more sophisticated ways
  to provide Web content distribution.

PRINCIPLES IN PRACTICE
DNS: CRITICAL NETWORK FUNCTIONS VIA THE CLIENT-SERVER PARADIGM
#+begin_quote
Like HTTP, FTP, and SMTP, the DNS protocol is an application-layer protocol
since it (1) runs between communicating end systems using the client-server
paradigm and (2) relies on an underlying end-to-end transport protocol to
transfer DNS messages between communicating end systems. In another sense,
however, the role of the DNS is quite different from Web, file transfer, and
e-mail applications. Unlike these applications, the DNS is not an application
with which a user directly interacts. Instead, the DNS provides a core Internet
function—namely, translating hostnames to their underlying IP addresses, for
user applications and other software in the Internet. We noted that much of the
complexity in the Internet architecture is located at the “edges” of the
network. The DNS, which implements the critical name-to-address translation
process using clients and servers located at the edge of the network, is yet
another example of that design philosophy.
#+end_quote

*** Overview of How DNS Works
hostname-to-IP-address translation service.

# procedimiento
Suppose that some application running in a user’s host needs to translate a
hostname to an IP address. The application will invoke the client side of DNS,
specifying the hostname that needs to be translated. DNS in the user’s host then
takes over, sending a query message into the network. All DNS query and reply
messages are sent within *UDP datagrams to port 53*. After a delay, ranging from
milliseconds to seconds, DNS in the user’s host receives a DNS reply message
that provides the desired mapping. This mapping is then passed to the invoking
application.

# mala idea
A simple design for DNS would have *one DNS server* that contains all the
mappings. Although the simplicity of this design is attractive, it is
inappropriate for today’s Internet, with its vast (and growing) number of hosts.
The problems with a centralized design include:
- A single point of failure :: If the DNS server crashes, so does the entire
  Internet.
- Traffic volume :: A single DNS server would have to handle all DNS queries
  (for all the HTTP requests and e-mail messages generated from hundreds of
  millions of hosts).
- Distant centralized database :: A single DNS server cannot be “close to” all
  the querying clients. If we put the single DNS server in New York City, then
  all queries from Australia must travel to the other side of the globe, perhaps
  over slow and congested links. This can lead to significant delays.
- Maintenance :: The single DNS server would have to keep records for all
  Internet hosts. Not only would this centralized database be huge, but it would
  have to be updated frequently to account for every new host.

  a single DNS server simply doesn’t scale

**** TODO A Distributed, Hierarchical Database
# bdd distribuida y jerarquica
In order to deal with the issue of scale, the DNS uses a large number of
servers, organized in a hierarchical fashion and distributed around the
world. No single DNS server has all of the mappings for all of the hosts in
the Internet. Instead, the mappings are distributed across the DNS
servers. To a first approximation, there are three classes of DNS servers—
~root~ DNS servers, ~top-level domain (TLD)~ DNS servers, and
~authoritative~ DNS servers.

imagen 2.17

# ej
suppose a DNS client wants to determine the IP address for the hostname
www.amazon.com.

The client first contacts one of the root servers, which returns IP addresses
for TLD servers for the top-level domain ~com~ . The client then contacts one of
these TLD servers, which returns the IP address of an authoritative server for
amazon.com . Finally, the client contacts one of the authoritative servers for
amazon.com , which returns the IP address for the hostname www.amazon.com .

A closer look at these three classes of DNS servers:
- Root DNS servers :: There are over 400 root name servers scattered all
  over the world. These root name servers are managed by 13 different
  organizations. Root name servers provide the IP addresses of the TLD
  servers.
- Top-level domain (TLD) servers :: For each of the top-level domains —
  top-level domains such as com, org, net, edu, and gov, and all of the country
  top-level domains such as uk, fr, ca, and jp — there is TLD server (or server
  cluster). The company Verisign Global Registry Services maintains the TLD
  servers for the com top-level domain, and the company Educause maintains the
  TLD servers for the edu top-level domain. TLD servers provide the IP addresses
  for authoritative DNS servers.
- Authoritative DNS servers :: Every organization with publicly accessible hosts
  (such as Web servers and mail servers) on the Internet must provide publicly
  accessible DNS records that map the names of those hosts to IP addresses. An
  organization’s authoritative DNS server houses these DNS records. An
  organization can choose to implement its own authoritative DNS server to hold
  these records; alternatively, the organization can pay to have these records
  stored in an authoritative DNS server of some service provider. Most
  universities and large companies implement and maintain their own primary and
  secondary (backup) authoritative DNS server.

# local dns server
existe otro tipo de servidores Dns llamado ~local DNS server~ o ~default name
server~. Este tipo de servidor no pertenece la jerarquia de servidores. Cada isp
(residencial o institucional) posee un local dns server. cuando un host se
conecta al isp, el isp provee al host con la ip de uno o mas de sus local dns
server. cuando un host realiza una consulta dns, esta sea hace al local dns
server, que actua como proxy, reenviando la consulta a la jerarquia de
servidores dns.

# ejemplo
Suppose the host cse.nyu.edu desires the IP address of gaia.cs.umass.edu . Also
suppose that NYU’s ocal DNS server for cse.nyu.edu is called dns.nyu.edu and
that an authoritative DNS server for gaia.cs.umass.edu is called dns.umass.edu .
The host cse.nyu.edu first sends a DNS query message to its local DNS server,
dns.nyu.edu . The query message contains the hostname to be translated, namely,
gaia.cs.umass.edu . The local DNS server forwards the query message to a root
DNS server. The root DNS server takes note of the edu suffix and returns to the
local DNS server a list of IP addresses for TLD servers responsible for edu .
The local DNS server then resends the query message to one of these TLD servers.
The TLD server takes note of the umass.edu suffix and responds with the IP
address of the authoritative DNS server for the University of Massachusetts,
namely, dns.umass.edu . Finally, the local DNS server resends the query message
directly to dns.umass.edu , which responds with the IP address of
gaia.cs.umass.edu . Note that in this example, in order to obtain the mapping
for one hostname, eight DNS messages were sent: four query messages and four
reply messages!

imagen 2.19

en el ejemplo anterior se asume que el servidor TLD conoce al servidor
autoritativo que a su vez reconoce al hostname. en general esto no es
cierto, si no que el TLD conoce a un servidor DNS intermediario que conoce
al servidor autoritativo.

#+begin_example
    For example, suppose again that the University of Massachusetts has a DNS
    server for the university, called dns.umass.edu . Also suppose that each of
    the departments at the University of Massachusetts has its own DNS server,
    and that each departmental DNS server is authoritative for all hosts in the
    department. In this case, when the intermediate DNS server, dns.umass.edu ,
    receives a query for a host with a hostname ending with cs.umass.edu , it
    returns to dns.nyu.edu the IP address of dns.cs.umass.edu , which is
    authoritative for all hostnames ending with cs.umass.edu .  The local DNS
    server dns.nyu.edu then sends the query to the authoritative DNS server,
    which returns the desired mapping to the local DNS server, which in turn
    returns the mapping to the requesting host. In this case, a total of 10 DNS
    messages are sent!
#+end_example

# tipos de consultas
El ejemplo de la imagen 2.19 hace uso de ~recursive queries~ y ~iterative
queries~. La consulta enviada de cse.nyu.edu a dns.nyu.edu es una consulta
recursiva, ya que se pide a dns.nyu.edu que se haga la consulta en su nombre.
las siguientes consultas son iterativas ya que todas las respuestas son
devuletas a dns.nyu.edu. En teoria cualquier consulta puede ser iterativa o
recursiva.

*For example, Figure 2.20 shows a DNS query chain for which all of the
queries are recursive*. *In practice, the queries typically follow the
pattern in Figure 2.19*. *The query from the requesting host to the local
DNS server is recursive, and the remaining queries are iterative*

imagen 2.20

**** DONE Cacheo DNS

El cacheo DNS *reduce el delay* y la *cantidad de mensajes (DNS) en
circulacion*.

La idea es: dada una cadena de consultas, cuando un server DNS recibe una
respuesta DNS (conteniendo, por ejemplo, un mapeo de un hostname a una direccion
IP), este puede cachear el mapeo a su memoria local. Si un par (hostname, IP)
esta cacheado en el servidor DNS y se realiza otra consulta por el mismo
hostname, el servidor puede proveer la direccion IP deseada. Dado que los mapeos
no son permanentes, los servidores DNS descartan la informacion cacheada luego
de un cierto periodo de tiempo.

Un servidor DNS local puede cachear tambien las direcciones IP de servidores
TLD, pudiendo saltar sobre el servidor DNS root en la consulta.

*** DONE Registros DNS y Mensajes

Los servidores DNS almacenan ~resource records (RRs)~. Estos proveen mapeos
(hostname, IP). Cada respuesta DNS lleva consigo uno o mas RRs.

Un RR es una 4-upla que contiene los campos
#+BEGIN_SRC
(Name, Value, Type, TTL)
#+END_SRC

~TTL~ es el tiempo de vida del RR. Determina cuándo un RR debería ser removido
del cache.

Los significados de ~Name~ y ~Value~ dependen de ~Type~:

| Type  | Name           | Value                                 |
|-------+----------------+---------------------------------------|
| A     | hostname       | IP address                            |
| NS    | domain         | hostname de servidor DNS autoritativo |
| CNAME | hostname alias | hostname canonico                     |
| MX    | hostname alias | hostname canonico de un mail server   |

- Tipo A, mapeo hostname-a-IP
  - Name es un hostname
  - Value es su IP
  - ~( relay1.bar.foo.com , 145.37.93.126, A)~
- Tipo NS, utilizado para direccionar a consultas DNS un paso siguiente en la
  cadena de consultas.
  - Name es un dominio
  - Value es el hostname de un servidor autoritativo que conoce como obtener
    IPs de hosts en el dominio
  - ~( foo.com , dns.foo.com , NS)~
- Tipo CNAME, proveen el nombre canonico para el hostname consultado
  - Name hostname alias
  - Value hostname canonico
  - ~( foo.com , relay1.bar.foo.com , CNAME)~
- Tipo MX, permiten que los mail-servers tengan un alias simple
  - Name hostname alias
  - Value hostname canonico de un mail-server
  - ~( foo.com , mail.bar.foo.com , MX)~
  - si una organizacion utiliza el mismo alias para web y mail, en la
    consulta se busca por el registro MX si se quiere el hostname canonico
    del mail-server.

    Si un servidor DNS (autoritativo o no) *es autoritativo para un hostname
    particular*, éste tendrá un registro de Tipo A para el hostname, es decir,
    puede contener un registro de Tipo A en su cache.

    Si el servidor *no es autoritativo para un hostname*, tendrá un registro Tipo
    NS para el dominio que incluye al hostname. También tendrá un registro Tipo A
    que provee la IP del servidor DNS en el campo Value del registro NS.

**** TODO Mensajes DNS

necesito imagenes

Existen dos tipos de mensajes DNS, =consulta= y =respuesta=. Ambos tipos tienen
el mismo formato.

- Los primeros 12 Bytes son la seccion de header:
  - Número de 16 bits que identifica el tipo de consulta :: Es copiado en el
    mensaje de respuesta de la consulta, permitiendo que el cliente pueda
    relacionar respuestas recibidas con búsquedas enviadas.
  - Varios flags :: Pueden indicar
    - si el mensaje es consulta o respuestas,
    - si la respuesta es enviada por un servidor autoritativo,
    - si se desea una query recursiva,
    - si el servidor DNS soporta recursion o no.
  - 4 campos que indican la cantidad de ocurrencias de los tipos de secciones de
    datos que le siguen al header.

- La seccion ~question section~ contiene informacion sobre la consulta que
  se esta realizando. Incluye:
  1) campo nombre que contiene el nombre consultado
  2) campo tipo que indica el tipo de la pregunta por el nombre. Ej:
     direccion de un host asociado a un nombre (tipo A) o el mail-server
     asociado a un alias (Tipo MX).

# respuesta
- En la respuesta del servidor DNS, la seccion ~answer section~ contiene los
  RRs para el hostname que se consulta.
- La seccion ~authority section~ contiene registros de otros servidores
  autoritativos.
- La seccion ~additional section~ contiene otros registros utiles. Por
  ejemplo, dada una consulta MX, la respuesta en el answer section contiene
  un RR con el hostname canonico del mail-server, y la additional section
  contienen un registro tipo A que provee la IP del hostname canonico del
  mail-server.

**** DONE Insertando Registros en la Base de Datos DNS
# ej
supongamos que se crea una compañia Network Utopia.

para registrar un dominio networkutopia.com, se debe hacerlo a traves de un
registrar. Un ~registrar~ es una entidad comercial que verifica la unicidad
de un nombre de dominio, ingresa el nombre de dominio en la BdD DNS y cobra
por el servicio. La =Internet Corporation for Assigned Names and Numbers
(ICANN)= acredita a los distintos registrars[fn:1].

[fn:1] http://www.internic.net.


cuando se registra el dominio networkutopia.com con algun registrar, tambien
se le debe proveer los nombres y direcciones IP de los servidores
autoritativos primario y secundario. Llamemoslos dns1.networkutopia.com ,
dns2.networkutopia.com con direcciones ~212.212.212.1~, y
~212.212.212.2~.

para cada uno de estos servidores, el registrar se asegura de ingresar sus
RR de tipo NS y tipo A a los TLD .com. Por ejemplo para el servidor
autoritativo primario, se ingresan los RR:

#+BEGIN_SRC
(networkutopia.com, dns1.networkutopia.com, NS)
(dns1.networkutopia.com, 212.212.212.1, A)
#+END_SRC

Tambien hay que asegurarse de que haya registros A y MX en los servidores
autoritativos para los servidores web y de mail en los dominios
www.networkutopia.com y mail.networkutopia.com

Una vez completados estos pasos, ya se puede visitar al sitio web y enviar
mails a la compañia.

#+begin_example
      Supongamos que Alice quiere visitar el sitio www.networkutopia.com.

      su host envia una consulta DNS a su local DNS server.

      este a su vez, contacta a un TLD .com (si lo tiene en cache, si no, contacta a
      un servidor root)

      el TLD responde al local DNS server con los registros NS y A.

      el local dns server consulta a las ips en dichos registros, preguntando por un
      registro tipo A con hostname www.networkutopia.com

      este ultimo registro provee la ip del sitio web deseado, por ejemplo 212.212.71.4
      y es devuelto al host de Alice.

      el navegador de Alice ahora puede comenzar una conexion TCP a 212.212.71.4
      y enviarle mensajes HTTP.
#+end_example

* KILL Capa de Transporte, UDP y Entrega confiable

La capa de transporte tiene el rol de proveer servicios de
comunicacion-a-la-aplicacion-ejecutandose-en-otro-host.

extiende el servicio-de-entrega entre dos *end-systems* de la capa de red, a un
servicio-de-entrega entre dos *procesos* de capa de aplicacion ejecutandose en
los end-systems.

** Introduccion y Servicios de Capa de Transporte

un protocolo de capa de transporte provee ~comunicacion logica~ entre procesos
ejecutandose en diferentes hosts. Desde el punto de vista del proceso, es como
si los hosts estuviesen conectados directamente, sin tener que preocuparse por
los detalles involucrados para enviar los mensajes.

imagen 3.1

los protocolos de capa de transporte estan implementados en los end-systems y no
en los conmutadores de paquetes. De el lado que envia, la capa de transporte
convierte mensajes de capa-de-aplicacion en paquetes de capa-de-transporte
llamados ~segmentos~.

*** Relacion entre las capas de transporte y capa de red

protocolos de capa de red proveen una comunicaicon logica entre *hosts*, los
protocolos de capa de transporte proveen una comunicacion logica entre
*procesos*

*** Panorama general de la capa de transporte en la internet

La internet tiene dos protocolos de capa de transporte:
- ~UDP (User Datagram Protocol)~ :: que provee un servicio sin conexion y no
  confiable.
- ~TCP (Transmission Control Protocol)~ :: que provee un servicio orientado a
  conexion y confiable.

# mux/demux
La responsabilidad fundamental de UDP y TCP es extender el servicio de IP entre
dos end-systems a un servicio entre dos procesos. Esto se llama ~multiplexacion~
y ~demultiplexacion~.

UDP y TCP tambien proveen verificacion de integridad al incluir campos en el
encabezado para la deteccion de errores.

# UDP
UDP solamente provee:
- entrega de datos proceso-a-proceso[fn:UDP1]
- verificacion de errores

[fn:UDP1] notar que es proc-a-proc y no host-a-host. proc-a-proc es mux/demux!


no garantiza que los mensajes enviado lleguen a su destino.

TCP ofrece otros servicios adicionales:
- ~Transferencia de datos confiable~ :: Al utilizar control de flujo, numeros de
  secuencia, acuse de recibo, y timers, se garantiza que los datos enviados
  lleguen a su (proceso) destino, sin errores y en orden.
- ~Control de Congestion~ :: evita congestionar enlaces y routers con demasiado
  trafico. Se logra regulando a la velocidad de transmicion.

** Multiplexacion y Demultiplexacion
# !
(extender servicio de delivery de host-a-host (de la capa inferior (red - IP)) a
proceso-a-proceso)

imagen 3.2

# sockets
Recordemos que las aplicaciones se comunican a traves de la red mediante
sockets.

Un host recibe segmentos de capa de transporte y los redirecciona al socket
apropiado, utilizando campos en el encabezado del segmento. el proceso de
entregar el segmento de capa de transporte al socket correcto se conoce como
demultiplexacion.

El proceso de recolectar datos de distintos sockets en el origen, encapsulando
cada uno, creando segmentos y pasarlos a la capa de red, se conoce como
multiplexacion.

La multiplexacion de capa de transporte requiere:
1. que los sockets tengan identificadores unicos
2. que cada segmento tenga campos especiales para indicar al socket al que deben
   ser entregados. estos campos son ~source port number field~ y ~destination
   port number field~.

imagen 3.3

cada numero de puerto es un numero de 16-bits, entre 0 y 65535.

Los numeros de puertos de 0 a 1023 son well-known y estan reservados para
aplicaciones de capa de aplicacion well-known (http:80, ftp:21).

RFC 1700

*** Multiplexacion y Demultiplexacion Sin Conexion

En general, el lado cliente de la aplicacion permite que la capa de transporte
asigne automaticamente el numero de puerto, mientras que el lado servidor de la
aplicacion asigna un numero de puerto especifico.

# ej
Supongamos que un proceso en el host A, con puerto UDP 19157, quiere enviar
datos a un proceso con puerto UDP 46428 en el host B. La capa de transporte en
el host A crea un segmento de capa de transporte que incluye datos de la
aplicacion, source port number (19157), destination port number (46428), y otros
dos valores. El segmento luego se pasa a la capa de red.

La capa de red encapsula el segmento en un datagrama IP y hace el mejor intento
de enviar el datagrama a su destino.

Si el segmento llega al host B, la capa de transporte examina el numero de
puerto destino (46428) y entrega el segmento al socket identificado en el
numero 46428.

# socket UDP
Un socket UDP es identificable por una tupla formada por la *IP y puerto de
destino*. Como consecuencia, si dos segmentos UDP tienen diferentes IP de origen
y/o puerto de origen, pero tienen la misma IP y puerto de destino, entonces
ambos segmentos son dirigidos al mismo proceso mediante el mismo socket.

El puerto de origen sirve para poder devolver segmentos al mismo proceso.

imagen 3.4

*** Multiplexacion y Demultiplexacion Orientado a Conexion

Un socket TCP es identificable por una 4-upla
#+begin_quote
(IP origen, puerto origen, IP destino, puerto destino)
#+end_quote

Cuando se recibe un segmento TCP, el host utiliza estos cuatro valores para
demultiplexar el segmento al socket apropiado.

# ej
Ejemplo:
- El servidor TCP tiene un socket aceptador, que espera por conexiones de
  clientes TCP en el puerto 12000.
- El cliente crea un socket y envia un pedido de conexion al puerto 12000.
  Tambien envia el puerto de origen del segmento elegido por el cliente.
- Cuando el servidor recibe el segmento de pedido de conexion, el OS ubica al
  proceso que espera por la conexion en el puerto 12000.
- La aplicacion acepta la conexion, anota los cuatro campos mencionados y se
  crea un socket nuevo asociado a estos campos. Los siguientes segmentos con los
  mismos cuatro valores son demultiplexados a este socket.

imagen 3.5

** Transporte sin conexion: UDP [RFC 768]

UDP hace lo minimo posible que un protocolo de capa de transporte puede hacer.

Ademas de multiplexar y demultiplexar segmentos y una simple verificacion de
errores, no agrega nada a IP.

A UDP se lo denomina como ~sin conexion~ porque no realiza ~handshaking~ entre
los hosts antes de enviar y recibir segmentos.

DNS es un ejemplo de un protocolo de capa de aplicacion que usualmente utiliza
UDP.

# usos de udp
Algunas aplicaciones utilizan UDP porque:
- Mayor control de la aplicacion sobre los datos enviados y cuando :: En UDP,
  apenas el proceso envia los datos por el socket UDP, el segmento se pasa
  directo a la capa de red. TCP por el otro lado, al tener el mecanismo de
  control de congestion, el trafico es regulado si el enlace o routers en el
  camino sufren de congestion. TCP tambien reenvia segmentos hasta que se
  reciba un acuse de recibo, lo que es un inconveniente para aplicaciones de
  tiempo real ya que requieren una velocidad de transmision minima y pueden
  tolerar perdida de datos.
- Sin establecimiento de conexion (handshake) (delay) :: UDP simplemente envia
  los datos hacia la siguiente capa, por lo que no tiene delay para establecer
  una conexion.
- Sin estado de conexion :: TCP mantiene el estado de conexion en los
  end-systems. Esto incluye buffers de envio y recepcion, parametros de control
  de congestion, numeros de secuencia y acuse de recibo. UDP, no mantiene estado
  de la conexion ni lleva registro de estos parametros (utilizados para
  garantizar una comunicacion confiable). == mas clientes que TCP
- Encabezado pequeño (overhead) :: El encabezado de TCP tiene 20 bytes mientras que el de
  UDP es solamente de 8 bytes.

  imagen 3.6

*** Segmento UDP [RFC 768]

--- 32 bits -----------------------------
|------------------+--------------------|
| Source Port #    | Destination Port # |
|------------------+--------------------|
| Length           | Checksum           |
|------------------+--------------------|
| Application Data |                    |
|------------------+--------------------|

# imagen 3.7

El mensaje de la aplicacion se encuentran en el campo de datos del segmento.

El encabezado UDP solo tienen cuatro campos, cada uno de dos bytes.

El campo length especifica el numero de bytes en el segmento UDP (encapsulado +
datos). Es necesario ya que el tamaño del campo de datos puede variar entre
segmentos UDP.

El checksum es utilizado por el host receptor para verificar si el segmento
tiene errores.

*** UDP Checksum

Del lado que envia, se realiza el complemento a 1 de la suma de todos los words
de 16 bits en el segmento.

** Principios de Transferencia de Datos Confiable (TDC)

Con un canal de comunicacion confiable, los bits transferidos *no se corrompen*
*ni se pierden*, y son *entregados en el orden en que fueron enviados*.

imagen 3.8

rdt : reliable data transfer

Consideramos el caso de la transferencia de datos unidireccional.

Ademas del intercambio de paquetes de datos, ambos extremos de la
comunicacion confiable tambien deben intercambiar paquetes de control. El
envio de paquetes se realiza mediante la funcion udt_send() (donde udt es
unreliable data transfer)

*** Armando un protocolo de TDC
**** RDT 1.0: TDC sobre un canal confiable
We first consider the simplest case, in which the underlying channel is
completely reliable.

dos ~maquinas de estado finitas (FSM)~ definen las operaciones de un
transmisor y emisor rdt1.0.

imagen 3.9

Cada FSM tiene un estado

el lado transmisor del rdt:
1. acepta datos de la capa superior via el evento ~rdt_send(data)~,
2. crea un paquete con los datos con la accion ~make_pkt(data)~) y
3. envia el paquete al canal.

el lado receptor:
1. recibe un paquete de la capa inferior via el evento ~rdt_rcv(packet)~,
2. quita los datos del paquete con la accion ~extract (packet, data)~ ) y
3. pasa los datos a la capa superior ~deliver_data(data)~.

en este protocolo simple, no hay distincion etnre una unidad de datos y un
paquete. tambien, todo el flujo de paquetes es del emisor al receptor, no
hay necesidad de que el receptor envie feedback. tambien se asume que el el
emisor es capaz de recibir datos a la misma velocidad en que el transmisor
los envia (por lo que no es necesario que el receptor indique al transmisor
a que reduzca la velocidad).

**** RDT 2.0: TDC sobre un canal con errores

ocurre corrupcion de bits en el canal de comunicacion.

***** Protocolos ARQ (Automatic Repeat reQuest)
Reliable data transfer protocols based on retransmissions depending on the
control messages sent.

The types of control messages are:
- positive acknowledgments
- negative acknowledgments

These control messages allow the *reciever* to let the sender know what has
been received correctly, and what has been received in error and thus
requires repeating.

three additional protocol capabilities are required in ARQ protocols to
handle the presence of bit errors:
- Error detection :: a mechanism that allows the receiver to detect when
  bit errors have occurred. Recall from the previous section that UDP uses
  the Internet checksum field for exactly this purpose. For now, we need
  only know that these techniques require that extra bits (beyond the bits
  of original data to be transferred) be sent from the sender to the
  receiver; these bits will be gathered into the packet checksum field of
  the rdt2.0 data packet.
- Receiver feedback :: the only way for the sender to know whether or not a
  packet was received correctly, is for the receiver to provide explicit
  feedback to the sender. The positive (~ACK~) and negative (~NAK~)
  acknowledgment replies in the message-dictation scenario are examples of
  such feedback. Our rdt2.0 protocol will similarly send ACK and NAK
  packets back from the receiver to the sender. In principle, these packets
  need only be one bit long; for example, a 0 value could indicate a NAK
  and a value of 1 could indicate an ACK.
- Retransmission :: A packet that is received in error at the receiver will
  be retransmitted by the sender.

imagen 3.10

The send side of rdt2.0 has two states.
- In the leftmost state, the send-side protocol is waiting for data to be
  passed down from the upper layer. When the ~rdt_send(data)~ event occurs,
  the sender will create a packet ( ~sndpkt~ ) containing the data to be
  sent, along with a packet checksum and then send the packet via the
  ~udt_send(sndpkt)~ operation.
- In the rightmost state, the sender protocol is waiting for an ACK or a
  NAK packet from the receiver.
  - If an ACK packet is received the sender knows that the most recently
    transmitted packet has been received correctly and thus the protocol
    returns to the state of waiting for data from the upper layer.
  - If a NAK is received, the protocol retransmits the last packet and
    waits for an ACK or NAK to be returned by the receiver in response to
    the retransmitted data packet.

when the sender is in the wait-for-ACK-or-NAK state, it cannot get more
data from the upper layer; that is, the ~rdt_send()~ event can not occur;
that will happen only after the sender receives an ACK and leaves this
state. Thus, the sender will not send a new piece of data until it is sure
that the receiver has correctly received the current packet. Because of
this behavior, protocols such as rdt2.0 are known as ~stop-and-wait~
protocols.

The receiver-side FSM for rdt2.0 still has a single state. On packet
arrival, the receiver replies with either an ACK or a NAK, depending on
whether or not the received packet is corrupted.

# acuse de recibo corrupto?
we haven’t accounted for the possibility that the ACK or NAK packet could
be corrupted. Minimally, we will need to add checksum bits to ACK/NAK
packets in order to detect such errors. The more difficult question is how
the protocol should recover from errors in ACK or NAK packets. The
difficulty here is that if an ACK or NAK is corrupted, the sender has no
way of knowing whether or not the receiver has correctly received the last
piece of transmitted data.

Consider three possibilities for handling corrupted ACKs or NAKs:
1. consider what a human might do in the message-dictation scenario. If the
   speaker didn’t understand the “OK” or “Please repeat that” reply from the
   receiver, the speaker would probably ask, “What did you say?” (thus
   introducing a new type of sender-to-receiver packet to our protocol). The
   receiver would then repeat the reply. But what if the speaker’s “What did you
   say?” is corrupted? The receiver, having no idea whether the garbled sentence
   was part of the dictation or a request to repeat the last reply, would
   probably then respond with “What did you say?” And then, of course, that
   response might be garbled. Clearly, we’re heading down a difficult path.
2. add enough checksum bits to allow the sender not only to detect, but also to
   recover from, bit errors. This solves the immediate problem for a channel
   that can corrupt packets but not lose them.
3. the sender simply resends the current data packet when it receives a garbled
   ACK or NAK packet. This approach, introduces duplicate packets into the
   sender-to-receiver channel. The fundamental difficulty with duplicate packets
   is that the receiver doesn’t know whether the ACK or NAK it last sent was
   received correctly at the sender. Thus, it cannot know a priori whether an
   arriving packet contains new data or is a retransmission!

A simple solution is to add a new field to the data packet and have the sender
number its data packets by putting a ~sequence number~ into this field. The
receiver then need only check this sequence number to determine whether or not
the received packet is a retransmission. For this simple case of a stop-and-wait
protocol, a 1-bit sequence number will suffice, since it will allow the receiver
to know whether the sender is resending the previously transmitted packet (the
sequence number of the received packet has the same sequence number as the most
recently received packet) or a new packet. Since we are currently assuming a
channel that does not lose packets, ACK and NAK packets do not themselves need
to indicate the sequence number of the packet they are acknowledging. The sender
knows that a received ACK or NAK packet (whether garbled or not) was generated
in response to its most recently transmitted data packet.

imagen 3.11
imagen 3.12

The rdt2.1 sender and receiver FSMs each now have twice as many states as
before. This is because the protocol state must now reflect whether the
packet currently being sent (by the sender) or expected (at the receiver)
should have a sequence number of 0 or 1. Note that the actions in those
states where a 0- numbered packet is being sent or expected are mirror
images of those where a 1-numbered packet is being sent or expected; the
only differences have to do with the handling of the sequence number.

Protocol rdt2.1 uses both positive and negative acknowledgments from the
receiver to the sender. When an out-of-order packet is received, the
receiver sends a positive acknowledgment for the packet it has
received. When a corrupted packet is received, the receiver sends a
negative acknowledgment. We can accomplish the same effect as a NAK if,
instead of sending a NAK, we send an ACK for the last correctly received
packet.

A sender that receives two ACKs for the same packet (that is, receives
duplicate ACKs) knows that the receiver did not correctly receive the
packet following the packet that is being ACKed twice.

One subtle change between rtdt2.1 and rdt2.2 is that the receiver must now
include the sequence number of the packet being acknowledged by an ACK
message (this is done by including the ACK , 0 or ACK , 1 argument in
~make_pkt()~ in the receiver FSM), and the sender must now check the
sequence number of the packet being acknowledged by a received ACK message
(this is done by including the 0 or 1 argument in ~isACK()~ in the sender
FSM).

**** RDT 3.0: TDC sobre un canal con perdidas y errores
Ahora tambien se asume que el canal de comunicacion tiene perdida de paquetes.

1. como detectar perdida de paquetes?
2. que hacer cuando ocurre?

el uso de checksums, numeros de secuencias, paquetes ACK, y retransmisiones
nos permiten responder la segunda pregunta.

supongamos que el emisor transmite un paquete y que este paquete, o su ACK, se
pierden. en cualquiera de los casos, el emisor no recibe una respuesta del
receptor. si el emisor esta dispuesto a esperar lo suficiente para asegurarse de
que el paquete se perdio, puede luego retransmitir el paquete.

pero cuanto tiempo debe esperar para asegurarse de que un paquete se perdio?

claramente debe esperar al menos el tiempo rtt + el tiempo requerido para
procesar el paquete en el receptor. esto es muy dificil de determinar. ademas el
protocolo deberia recuperarse de la perdida de paquete lo mas rapido posible.

si un ACK no se recibe dentro de este tiempo, el paquete es retransmitido. notar
que si el paquete sufre de mucho delay, el emisor puede retransmitir aunque no
se haya perdido el paquete o su ACK, y esto produce que haya paquetes duplicados
(solucionado en rdt 2.2 con numeros de secuencia).

imagen 3.14

el emisor no sabe si
- el paquete se perdio
- el ACK se perdio
- el paquete o el ACK tuvieron mucho delay.

en cualquiera de los casos, la accion el la misma, retransmitir.

implementar el mecanismo de retransmision basado en timers requiere de un
timer capaz de interrumpir al emisor.  el emisor debe luego:
1. empezar el timer cada vez que un paquete se envia (ya sea por primera vez
   o una retransmision).
2. responder a la interrupcion del timer, tomando acciones apropiadas.
3. frenar el timer

imagen 3.15

imagen 3.16
* Ventana deslizante y TCP
** Pipelined reliable Data Transfer Protocols
# problema de stop-and-wait
Protocol rdt3.0 is a functionally correct protocol, but its performance
problem is the fact that it is a stop-and-wait protocol.

imagen 3.16

This stop-and-wait protocol limits the sender ~utilization~ of the channel
(the amount of time the sender actually sends bits into the channel).

image 3.17

# solucion a stop-and-wait -> que no sea stop-and-wait
The solution to this particular performance problem is simple: Rather than
operate in a stop-and-wait manner, *the sender is allowed to send multiple
packets without waiting for acknowledgments*.

illustrated in Figure 3.17(b). Figure 3.18(b) shows that if the sender is
allowed to transmit three packets before having to wait for acknowledgments,
*the utilization of the sender is essentially tripled*.

# pipelining
this technique is known as ~pipelining~. Pipelining has the following
consequences for reliable data transfer protocols:
- The range of sequence numbers must be increased, since each in-transit
  packet (not counting retransmissions) must have a unique sequence number and
  there may be multiple, in-transit, unacknowledged packets.
- The sender and receiver sides of the protocols may have to buffer more than
  one packet. Minimally, the sender will have to buffer packets that have been
  transmitted but not yet acknowledged. Buffering of correctly received
  packets may also be needed at the receiver, as discussed below.
- The range of sequence numbers needed and the buffering requirements will
  depend on the manner in which a data transfer protocol responds to lost,
  corrupted, and overly delayed packets. Two basic approaches toward pipelined
  error recovery can be identified: ~Go-Back-N~ and ~selective repeat~.

  imagen 3.18

*** Go-Back-N (GBN)

In a Go-Back-N (GBN) protocol, the sender is allowed to transmit multiple
packets (when available) without waiting for an acknowledgment, but is
constrained to have no more than some maximum allowable number, N, of
unacknowledged packets in the pipeline.

imagen 3.19

Figure 3.19 shows the sender’s view of the range of sequence numbers in a GBN
protocol. If we define ~base~ to be the sequence number of the oldest
unacknowledged packet and ~nextseqnum~ to be the smallest unused sequence
number (that is, the sequence number of the next packet to be sent), then
four intervals in the range of sequence numbers can be identified.

Sequence numbers in the interval
- ~[ 0, base-1 ]~ correspond to packets that have already been transmitted
  and acknowledged.
- ~[base, nextseqnum-1]~ corresponds to packets that have been sent but not yet
  acknowledged. Sequence numbers in the interval
- ~[nextseqnum, base+N-1]~ can be used for packets that can be sent immediately,
  *should data arrive from the _upper layer_*.
- ~[base+N, ...]~ cannot be used until an unacknowledged packet currently in
  the pipeline (specifically, the packet with sequence number ~base~ ) has
  been acknowledged.

the range of permissible sequence numbers for transmitted but not yet
acknowledged packets can be viewed as a window of size ~N~ over the range of
sequence numbers.

As the protocol operates, this window slides forward over the sequence number
space. For this reason, ~N~ is often referred to as the ~window size~ and the
GBN protocol itself as a ~sliding-window protocol~.

You might be wondering *why we would even limit the number of outstanding,
unacknowledged packets to a value of N?*. *Why not allow an unlimited number
of such packets?*

flow control is one reason to impose a limit on the sender..

In practice, a packet’s sequence number is carried in a fixed-length field in
the packet header. If k is the number of bits in the packet sequence number
field, the range of sequence numbers is thus [0,2k-1].

Figures 3.20 and 3.21 give an extended FSM description of the sender and
receiver sides of an ACK-based, NAK-free, GBN protocol.

imagen 3.20

imagen 3.21

we have added variables (similar to programming-language variables) for base
and nextseqnum, and added operations on these variables and conditional
actions involving these variables.

The GBN sender must respond to three types of events:
- Invocation from above :: When rdt_send() is called from above, the sender
  first checks to see if the window is full, that is, whether there are N
  outstanding, unacknowledged packets.
  - If the window is not full, a packet is created and sent, and variables
    are appropriately updated.
  - If the window is full, the sender simply returns the data back to the
    upper layer, an implicit indication that the window is full.
  The upper layer would presumably then have to try again later. In a real
  implementation, the sender would more likely have either buffered (but not
  immediately sent) this data, or would have a synchronization mechanism (for
  example, a semaphore or a flag) that would allow the upper layer to call
  rdt_send() only when the window is not full.
- Receipt of an ACK :: In our GBN protocol, an acknowledgment for a packet
  with sequence number n will be taken to be a ~cumulative acknowledgment~,
  indicating that all packets with a sequence number up to and including n
  have been correctly received at the receiver.
- A timeout event :: The protocol’s name, “Go-Back-N,” is derived from the
  sender’s behavior in the presence of lost or overly delayed packets. As in
  the stop-and-wait protocol, a timer will again be used to recover from lost
  data or acknowledgment packets. If a timeout occurs, the sender resends all
  packets that have been previously sent but that have not yet been
  acknowledged. Our sender in Figure 3.20 uses only a single timer, which can
  be thought of as a timer for the oldest transmitted but not yet
  acknowledged packet. If an ACK is received but there are still additional
  transmitted but not yet acknowledged packets, the timer is restarted. If
  there are no outstanding, unacknowledged packets, the timer is stopped.

The receiver’s actions in GBN are:
- If a packet with sequence number n is received correctly and is in order
  (that is, the data last delivered to the upper layer came from a packet
  with sequence number), the receiver sends an ACK for packet n and delivers
  the data portion of the packet to the upper layer.
- In all other cases, the receiver discards the packet and resends an ACK for
  the most recently received in-order packet. Note that since packets are
  delivered one at a time to the upper layer, if packet k has been received
  and delivered, then all packets with a sequence number lower than k have
  also been delivered. Thus, the use of ~cumulative acknowledgments~ is a
  natural choice for GBN.

*In our GBN protocol, the receiver discards out-of-order packets.* Recall that
the receiver must deliver data in order to the upper layer.

Suppose now that packet n is expected, but packet n+1 arrives. Because data must
be delivered in order, the receiver could buffer packet n+1 and then deliver
this packet to the upper layer after it had later received and delivered
packet n. However, if packet n is lost, both it and packet n+1 will eventually
be retransmitted as a result of the GBN retransmission rule at the sender.
Thus, the receiver can simply discard packet.

The advantage is the simplicity of receiver buffering. the receiver need not
buffer any out-of-order packets. Thus, while the sender must maintain the
upper and lower bounds of its window and the position of ~nextseqnum~ within
this window, the only piece of information the receiver need maintain is the
sequence number of the next in-order packet held in the variable
~expectedseqnum~.

the disadvantage of throwing away a correctly received packet is that the
subsequent retransmission of that packet might be lost or garbled and thus
even more retransmissions would be required.

imagen 3.22

Figure 3.22 shows the operation of the GBN protocol for the case of a window
size of four packets. Because of this window size limitation, the sender sends
packets 0 through 3 but then must wait for one or more of these packets to be
acknowledged before proceeding. As each successive ACK (for example, ACK0 and
ACK1 ) is received, the window slides forward and the sender can transmit one
new packet (pkt4 and pkt5, respectively). On the receiver side, packet 2 is lost
and thus packets 3, 4, and 5 are found to be out of order and are discarded.

*** Selective Repeat (SR)

There are scenarios in which GBN itself suffers from performance problems. In
particular, when the window size and bandwidth-delay product are both large,
many packets can be in the pipeline. A single packet error can thus cause GBN
to retransmit a large number of packets.

As the name suggests, selective-repeat protocols avoid unnecessary
retransmissions by having the sender *retransmit only those packets that it
suspects were received in error* (that is, were lost or corrupted) at the
receiver. This individual, as-needed, retransmission will require that the
receiver individually acknowledge correctly received packets.

A window size of N will again be used to limit the number of outstanding,
unacknowledged packets in the pipeline. However, unlike GBN, the sender will
have already received ACKs for some of the packets in the window.

The SR receiver will acknowledge a correctly received packet whether or not
it is in order. Out-of-order packets are buffered until any missing packets
(that is, packets with lower sequence numbers) are received, at which point a
batch of packets can be delivered in order to the upper layer.

imagen 3.23

#+caption: SR sender events and actions
imagen 3.24

#+caption: SR receiver events and actions
imagen 3.25

It is important to note that in Step 2 in Figure 3.25, the receiver
reacknowledges (rather than ignores) already received packets with certain
sequence numbers below the current window base.

#+caption: SR operation
imagen 3.26

Given the sender and receiver sequence number spaces in Figure 3.23, for
example, if there is no ACK for packet send_base propagating from the
receiver to the sender, the sender will eventually retransmit packet
send_base, even though it is clear (to us, not the sender!) that the
receiver has already received that packet. If the receiver were not to
acknowledge this packet, the sender’s window would never move forward! This
example illustrates an important aspect of SR protocols (and many other
protocols as well). The sender and receiver will not always have an identical
view of what has been received correctly and what has not. For SR protocols,
this means that the sender and receiver windows will not always coincide.


The lack of synchronization between sender and receiver windows has important
consequences when we are faced with the reality of a finite range of sequence
numbers.

#+BEGIN_EXAMPLE
   Consider what could happen, for example, with a finite range of four packet
   sequence numbers, 0, 1, 2, 3, and a window size of three.

   Suppose packets 0 through 2 are transmitted and correctly received and
   acknowledged at the receiver. At this point, the receiver’s window is over the
   fourth, fifth, and sixth packets, which have sequence numbers 3, 0, and 1,
   respectively. Now consider two scenarios.

   In the first scenario, shown in Figure 3.27(a), the ACKs for the first three
   packets are lost and the sender retransmits these packets. The receiver thus
   next receives a packet with sequence number 0—a copy of the first packet sent.

   In the second scenario, shown in Figure 3.27(b), the ACKs for the first three
   packets are all delivered correctly. The sender thus moves its window forward
   and sends the fourth, fifth, and sixth packets, with sequence numbers 3, 0, and
   1, respectively. The packet with sequence number 3 is lost, but the packet with
   sequence number 0 arrives—a packet containing new data.

   Now consider the receiver’s viewpoint in Figure 3.27, which has a figurative
   curtain between the sender and the receiver, since the receiver cannot “see” the
   actions taken by the sender. All the receiver observes is the sequence of
   messages it receives from the channel and sends into the channel. As far as it
   is concerned, the two scenarios in Figure 3.27 are identical. There is no way of
   distinguishing the retransmission of the first packet from an original
   transmission of the fifth packet. Clearly, a window size that is 1 less than the
   size of the sequence number space won’t work. But how small must the window size
   be? A problem at the end of the chapter asks you to show that the window size
   must be less than or equal to half the size of the sequence number space for SR
   protocols.
#+END_EXAMPLE

imagen 3.27

Table 3.1 Summary of reliable data transfer mechanisms and their use

| Mechanism               | Use, Comments                                                                                                                                                                                                                                                                                                                                                                                                                                |   |
| Checksum                | Used to detect bit errors in a transmitted packet.                                                                                                                                                                                                                                                                                                                                                                                           |   |
| Timer                   | Used to timeout/retransmit a packet, possibly because the packet (or its ACK) was lost within the channel. Because timeouts can occur when a packet is delayed but not lost (premature timeout), or when a packet has been received by the receiver but the receiver-to-sender ACK has been lost, duplicate copies of a packet may be received by a receiver                                                                                 |   |
| Sequence number         | Used for sequential numbering of packets of data flowing from sender to receiver. Gaps in the sequence numbers of received packets allow the receiver to detect a lost packet. Packets with duplicate sequence numbers allow the receiver to detect duplicate copies of a packet.                                                                                                                                                            |   |
| Acknowledgment          | Used by the receiver to tell the sender that a packet or set of packets has been received correctly. Acknowledgments will typically carry the sequence number of the packet or packets being acknowledged. Acknowledgments may be individual or cumulative, depending on the protocol.                                                                                                                                                       |   |
| Negative acknowledgment | Used by the receiver to tell the sender that a packet has not been received correctly. Negative acknowledgments will typically carry the sequence number of the packet that was not received correctly.                                                                                                                                                                                                                                      |   |
| Window, pipelining      | The sender may be restricted to sending only packets with sequence numbers that fall within a given range. By allowing multiple packets to be transmitted but not yet acknowledged, sender utilization can be increased over a stop-and-wait mode of operation. We’ll see shortly that the window size may be set on the basis of the receiver’s ability to receive and buffer messages, or the level of congestion in the network, or both. |   |

** Transporte Orientado a Conexion: TCP

TCP esta definido en RFC 793, RFC 1122, RFC 1323, RFC 2018, RFC 2581.

*** The TCP Connection

Se dice que TCP es orientado a conexion porque antes que una aplicacion comienze
a tranferir datos, ambos procesos deben primero realizar un handshake con el
otro, es decir, deben enviarse segmentos preliminares entre si para establecer
parametros para la transferencia de datos.

esta connexion tcp es una conexion logica punto-a-punto (no se puede hacer
multicast). el estado de la conexion es mantenido por ambos extremos de la
misma y no por elementos intermedios de la red. se provee un servicio
full-duplex, es decir que ambos extremos pueden enviar y recibir datos al
mismo tiempo.

# handshake
si un host quiere comunicarse con otro, primero envia un paquete especial al
otro host; el otro host responde con un segundo paquete especial y por ultimo
el primer host responde con un tercer paquete especial. Los primeros dos
paquetes no contienen payload (datos de capa de aplicacion), el tercero puede
no tener.

este proceso para establecer la conexion se lo conoce como =three-way
handshake=.

Una vez establecida la conexion, los procesos pueden enviar y recibir datos
entre si. Cuando un proceso envia datos a traves del socket a TCP, TCP los
redirige al =buffer de salida= de la conexion, que fue creado en el proceso
del handshake. Cada cierto tiempo, TCP toma pedazos del buffer y los pasa a
la capa de red.

# que?
la cantidad maxima de datos que se pueden enviar a la vez esta limitada por
el =maximum segment size (MSS)=. En general se determina a partir del frame
mas largo que puede ser enviado por el host que envia (=maximum transmission
unit, MTU=).
# 1ro se determina el mtu y despues el mss

Un valor tipico del MSS es 1460B. *Observar que el MSS es la cantidad maxima de
datos que una aplicacion puede enviar en el segmento y no el tamaño maximo del
segmento TCP*.

TCP agrupa pares de datos-del-cliente con un encabezado TCP, formando
=segmentos TCP=, que son pasados a la capa de red.

Del otro lado de la conexion, cuando se recibe un segmento, este es colocado
en el buffer de recepcion de la conexion. La aplicacion lee datos de este
buffer. Cada lado de la conexion tiene su propio buffer de envio y buffer de
recepcion.

imagen 3.28

la conexion TCP consiste de buffers, variables y un socket asociado a un
proceso en un host y otro conjunto de buffers, variables y un socket asociado
a un proceso en otro host.

*** Estructura del segmento TCP

el segmento consiste de campos del encabezado y el campo de datos.

el campo de datos contiene datos de la aplicacion.  el mss limita el tamaño
de este campo.

los campos del encabezado incluyen
- puertos de origen y destino (multiplexacion y demultiplexacion),
- campo de checksum,
- numero de secuencia y numero de ACK, ambos numeros de 32 bits.
- el campo =receive window= de 16 bits, utilizado para control de
  flujo. indica la cantidad de bytes que el receptor puede aceptar.
- el campo =largo del encabezado= de 4 bits, especifica el largo del
  encabezado TCP en words de 32 bits, ya que puede ser de tamaño variable. en
  general el encabezado es de 20 bytes.
- campo de =opciones=, opcional y de tamaño variable, utilizado para negociar
  el mss o como factor de escala para el tamaño de la ventana.
- campo de =flags= de 6 bits. =Bit de ACK=, utilizado para indicar que el
  valor en el =campo de ACK= es valido.  Los bits de =RST=, =SYN=, y =FIN=
  son utilizados para establecer y abandonar la conexion.  Los bits =CWR= y
  =ECE= son utilizados para notificar congestion. El bit =PSH= indica que el
  receptor deberia enviar los datos a la capa de arriba inmediatamente. El
  bit =URG= es utilizado para indicar que el transmisor marca a los datos
  como urgentes.

  imagen 3.29

**** Numeros de secuencia y ACK

los dos campos mas importantes de TCP son:
- el campo de numero de secuencia
- el campo de numero de acuse de recibo

que son criticos para la TDC

imagen 3.30

tcp ve los datos como un stream de bytes, sin estructura pero ordenados. el
uso de numeros de secuencia refleja esta perspectiva porque los numeros de
secuencia son sobre los bytes transmitidos y no sobre los segmentos
transmitidos. el =numero de secuencia de un segmento= es entonces el numero
de byte del stream correspondiente al primer byte del segmento.

# ejemplo num secuencia
un proceso en el host a quiere enviar un stream de datos a un proceso en el
host b sobre tcp. tcp en el host a, numera a cada byte del stream de datos.
supongamos que:
- el stream es de 500000 B
- mss 1000 B
- el primer byte del stream tiene numero 0.

tcp contruye 500 segmentos a partir del stream. al primer segmento se le
asigna el numero de secuencia 0, al segundo el numero 1000, luego 2000, y
asi sucesivamente y se colocan en el campo de numero de secuencia del
segmento.
# fin ejemplo num secuencia

consideremos a los numeros de acuse de recibo. recordemos que tcp es full
duplex. el numero de ACK del segmento, es el proximo byte que el receptor
espera a recibir del emisor, es decir, es el numero de secuencia del ultimo
segmento recibido mas uno.

# ejemplo num ack
supongamos que el host a recibio todos los bytes numerados del 0 al 535 del host
b, y que el host a esta a punto de enviar un segmento a B. el host A esta
esperando a el byte 536 y el resto de los bytes del stream del host B. el host A
coloca 536 en el campo de numero ACK en el segmento que envia a B.

si host A recibe segmentos con bytes de 0 a 535 y 900 a 1000, en el proximo
segmento que envie el host A, tendra el numero de ack 536, para reconstruir
el stream de datos original.
# fin ejemplo num ack

al solamente hacer ack de bytes hasta el primer byte faltante, se dice que
tcp provee =cumulative acknowledgments=.

si tcp recibe segmentos fuera de orden (como el ejemplo anterior), tcp
puede:
- descartar segmentos fuera de orden
- mantener a los segmentos fuera de orden y esperar a que los bytes faltantes
  llenen los espacios. ya que en los rfc, no esta especificado este
  comportamiento.

ambos lados de la conexion eligen un numero de secuencia inicial aleatorio.
esto es para minimizar la probabilidad de que un segmento de una conexion
anterior, que este todavia presente en la red sea interpretado cono legitimo
para otra conexion entre los mismos hosts (y que tambien utilice a los
mismos puertos).

*** Estimacion de Timeout y RTT (Round-Trip Time)

claramente, el tiempo de timeout deberia ser mas largo que el tiempo de rtt
(es tiempo que tarda un paquete desde que es enviado hasta que se reciba su
ack). si no, se haran retransmisiones innecesarias.

**** Estimando el RTT

la mayoria de las implementaciones de tcp, toma una muestra del rtt a la
vez, es decir que se muestrea el rtt para uno de los segmentos transmitidos
y que todavia no fue confirmada su recepcion. esto quiere decir que se
muestrea un rtt cada rtt. nunca se calcula un rtt de un segmento que fue
retransmitido, solo segmentos que son transmitidos una vez.

claramente, los valores del muestreo fluctuan entre segmentos debido a
la congestion en la red. tcp mantiene una estimacion para mitigar
fluctuaciones entre muestras. la estimacion se actualiza con una nueva
muestra segun:

$$Estimated_{RTT} = (1-\alpha)Estimated_{RTT} + \alpha Sample_{RTT}$$

[RFC 6298] recomienda el valor \alpha = 0.125

tambien es util estimar la variabilidad del rtt. [RFC 6298] define a la
variacion del rtt como:

$$Dev_{RTT} = (1-\beta)Dev_{RTT} + \beta |Sample_{RTT}-Estimated_{RTT}|$$

con $\beta = 0.25$.

*** TDC

[RFC 6298] recomienda que se utilice un solo timer para retransmisiones,
incluso si hay multiples paquetes transmitidos pero todavia no confirmados.

**** Escenarios

imagen 3.34

Host A sends one segment to Host B. Suppose that this segment has sequence
number 92 and contains 8 bytes of data. After sending this segment, Host A waits
for a segment from B with acknowledgment number 100. Although the segment from A
is received at B, the acknowledgment from B to A gets lost. In this case, the
timeout event occurs, and Host A retransmits the same segment. Of course, when
Host B receives the retransmission, it observes from the sequence number that
the segment contains data that has already been received. Thus, TCP in Host B
will discard the bytes in the retransmitted segment.

imagen 3.35

Host A sends two segments back to back. The first segment has sequence
number 92 and 8 bytes of data, and the second segment has sequence number
100 and 20 bytes of data. Suppose that both segments arrive intact at B, and
B sends two separate acknowledgments for each of these segments. The first
of these acknowledgments has acknowledgment number 100; the second has
acknowledgment number 120. Suppose now that neither of the acknowledgments
arrives at Host A before the timeout.  When the timeout event occurs, Host A
resends the first segment with sequence number 92 and restarts the timer. As
long as the ACK for the second segment arrives before the new timeout, the
second segment will not be retransmitted.

imagen 3.36

Host A sends the two segments, exactly as in the second example. The
acknowledgment of the first segment is lost in the network, but before the
timeout event, Host A receives an acknowledgment with acknowledgement
number 120. Host A therefore knows that Host B has received everything up
through byte 119; so Host A does not resend either of the two segments.

**** Duplicando el intervalo de timeout

cuando ocurre un timeout, configura el proximo tiempo de timeout al doble del
anterior.

Esta moficicacion provee una forma limitada de control de congestion. La
expiracion del timer, probablemente es causada por congestion en la red.

**** Fast retransmit

timeout-triggered retransmissions can be relatively long. When a segment is
lost, this long timeout period forces the sender to delay resending the lost
packet, thereby increasing the end-to-end delay. Fortunately, the sender can
often detect packet loss well before the timeout event occurs by noting
so-called duplicate ACKs. A =duplicate ACK= is an ACK that reacknowledges a
segment for which the sender has already received an earlier acknowledgment.

To understand the sender’s response to a duplicate ACK, we must look at why
the receiver sends a duplicate ACK in the first place.

| Event                                                                                                                    | TCP Receiver Action                                                                                                                               |
| Arrival of in-order segment with expected sequence number. All data up to expected sequence number already acknowledged. | Delayed ACK. Wait up to 500 msec for arrival of another in-order segment. If next in-order segment does not arrive in this interval, send an ACK. |
| Arrival of in-order segment with expected sequence number. One other in-order segment waiting for ACK transmission.      | One Immediately send single cumulative ACK, ACKing both in-order segments.                                                                        |
| Arrival of out-of-order segment with higher-than-expected sequence number. Gap detected.                                 | Immediately send duplicate ACK, indicating sequence number of next expected byte (which is the lower end of the gap).                             |
| Arrival of segment that partially or completely fills in gap in received data.                                           | Immediately send ACK, provided that segment starts at the lower end of gap.                                                                       |

When a TCP receiver receives a segment with a sequence number that is larger
than the next, expected, in-order sequence number, it detects a gap in the
data stream—that is, a missing segment. This gap could be the result of lost
or reordered segments within the network. Since TCP does not use negative
acknowledgments, the receiver cannot send an explicit negative
acknowledgment back to the sender. Instead, it simply reacknowledges (that
is, generates a duplicate ACK for) the last in-order byte of data it has
received.

If the TCP sender receives three duplicate ACKs for the same data, it takes
this as an indication that the segment following the segment that has been
ACKed three times has been lost. In the case that three duplicate ACKs are
received, the TCP sender performs a =fast retransmit= [RFC 5681],
retransmitting the missing segment before that segment’s timer expires. This
is shown in Figure 3.37, where the second segment is lost, then
retransmitted before its timer expires.

imagen 3.37

**** Go-Back-N o Selective Repeat?

TCP es un protocolo Go-Back-N o Selective Repeat?

recordemos que los acks de tcp son cumulativos y que los segmentos
correctamente recibidos pero fuera de orden no son confirmados por el
receptor.  el emisor tcp solo mantiene el menor numero de secuencia
transmitido pero no confirmado (SendBase) y el numero de secuencia a
transmitir (NextSeqNum). en este sentido, tcp es un protocolo gbn.

aunque, hay diferencias entre gbn y tcp. muchas implementaciones de tcp, si
guardan segmentos recibidos fuera de orden. tambien consideremos el caso en
que se transmiten y reciben los paquetes de 1 a N, pero se pierde el ack
para el paquete n<N, pero el resto de los N-1 acuses de recibo llegan al
emisor antes de que expire el timer. en este caso gbn, retransmitiria el
paquete n y el resto n+1,n+2,...,N. tcp por el otro lado, solo retransmite
el segmento n en caso de que el timer de n expire antes de llegar el acuse
de n+1.

una modificacion propuesta en [RFC 2018] para TCP, permite a tcp confirmar
paquetes recibidos fuera de orden de forma selectiva en vez de confirmar de
forma cumulativa al utlimo paquete recibido correctamente en orden. esto
hace que se parezca al protocolo de selective repeat.
* Control de Congestion
** Transporte Orientado a Conexion
*** Control de flujo

recordemos que cada extremo de la conexion tcp, reserva un buffer para la
recepcion de datos. los datos se colocan en el buffer si son correctos y estan
en orden. la aplicacion puede tardar un tiempo en leer los datos y por lo tanto,
el emisor puede desbordar el buffer receptor.

tcp proveee un servicio de control de flujo para evitar que esto suceda. el
control de flujo es entonces un servicio para equilibrar las velocidades de
transmision del emisor y lectura de la aplicacion receptora.

(esto es diferente de control de congestion, en el sentido de que cc es debido a
congestion en la red. cf es debido a la sincronizacion entre emisor y receptor)

we suppose throughout this section that the TCP implementation is such that the
TCP receiver discards out-of-order segments.

se provee cf al hacer que el emisor mantenga una variable llamada ~receive
window~. le da al emisor una idea de cuanto espacio libre hay disponible en el
buffer receptor.

# ejemplo
Suppose that Host A is sending a large file to Host B over a TCP
connection. Host B allocates a receive buffer to this connection; denote its
size by RcvBuffer. From time to time, the application process in Host B reads
from the buffer.
# fin

Define the following variables (in receiver):
- LastByteRead: numero del ultimo byte leido en el stream de datos en el buffer
  por el proceso en el host B
- LastByteRcvd: numero del ultimo byte del stream que llego de la red y fue
  colocado en el buffer en B.

como no se puede desbordar el buffer alocado:

$$LastByteRcvd−LastByteRead \leq RcvBuffer$$

La ventana de recepcion ($rwnd$) se configura para que sea la cantidad de
espacio libre en el buffer:

$$rwnd=RcvBuffer−[LastByteRcvd−LastByteRead]$$

imagen 3.38

el host b notifica al host a cuanto espacio libre tiene en su buffer al
colocar el valor actual de rwnd en el campo recieve window del header tcp.

por el otro lado, el host A mantiene dos variables:
- LastByteSent
- LastByteAcked

La diferencia $LastByteSent - LastByteAcked$ es la cantidad de datos no
confirmados que A envio a la conexion.  si esta diferencia es menor a rwnd,
entonces el host A se asegura de que no esta desbordando el buffer en el host
B.

$$LastByteSent-LastByteAcked \leq rwnd$$

(el emisor conoce el rwnd del receptor porque le llega en el ACK)

Que pasa si el receptor notifica que rwnd=0 y no tiene mensajes para enviar?
El emisor nunca podria saber cuando el receptor tiene espacio el buffer para
seguir almacenando datos.  por este motivo, la especificacion de TCP requiere
que el host A continue enviado segmentos de un byte de datos cuando el rwnd
de B es 0. estos segmentos son confirmados por el receptor. Eventualmente el
buffer del receptor se liberara y el rwnd aumentara.

udp no realiza control de flujo.

*** Administracion de conexion de TCP

Como se establece y se termina una conexion TCP.

por que?
- puede incrementar delay percivido
- entender explotacion de vulnerabilidades

supongamos que un cliente quiere iniciar una connexion con un server:
1. TCP del lado cliente envia un segmento TCP especial al lado servidor. No
   contiene datos de la capa de aplicacion, pero un bit del campo de flags
   del encabezado del segmento, bit SYN, se setea a 1. Segmento SYN. Se elige
   un numero de secuencia inicial de forma aleatoria (client_isn). El
   segmento se encapsula en un datagrama IP y se envia al servidor.
2. una vez que llega el datagrama al servidor, se extrae el segmento TCP, se
   reserva espacio para los buffers y variables de la conexion, y se envia un
   segmento de conexion-aprobada al cliente. Este segmento tampoco contiene
   datos de capa de aplicacion, pero el encabezado TCP si contiene el bit SYN
   en 1, el campo de ACK en client_isn+1, y el numero-de-secuencia, inicial,
   aleatorio, del servidor (server_isn). Al este segmento se lo llama SYNACK
3. cuando el cliente recibe este segmento SYNACK, este reserva buffers y
   variables para la conexion, y se envia al servidor un ultimo segmento para
   indicar al servidor que se recibio el segmento de conexion-aprobada. Este
   segmento tiene el bit de SYN en 0, numero de ACK server_isn+1 y puede
   tener datos de capa de aplicacion en el payload del segmento.

una vez realizados estos pasos, el cliente y servidor pueden enviarse
segmentos con datos entre si. (SYN = 0).

imagen 3.39

a este proceso (de establecimiento de conexion) se lo llama ~three way
handshake~.

una vez que se quiera finalizar la conexion, los recursos reservados para
esta, deben liberarse.

imagen 3.40

supongamos que el cliente desea cerrar la conexion. este envia un comando
close. TCP del lado del cliente envia un segmento especial con el bit FIN
en 1.  Cuando el servidor recibe este segmento, responde con un ACK. Luego el
servidor envia otro segmento, esta vez con el bit FIN en 1, a lo que el
cliente responde con un ACK. En este punto es cuando se pueden liberar los
recursos.

imagen 3.41

se muestra en la imagen los posibles estados de TCP en el lado del cliente.

imagen 3.42

se muestra en la imagen los posibles estados de TCP en el lado del servidor.

Que pasa si el host receptor no tiene ningun proceso escuchando en un puerto,
al que otro host quiere conecarse?

El host receptor envia un segmento con el bit RST en 1 de vuelta a la fuente,
indicando que no hay un socket para el segmento recibido.

**** SYN FLOOD attack

Al establecer la conexion TCP, un servidor reserva recursos para la misma y
luego envia un segmento SYNACK. Una forma de atacar al servidor es un Denial
of Service (DoS) mediante un SYN flood.

Consiste de enviar una gran cantidad de segmentos SYN, sin intencion de
completar el tercer paso del handshake. El servidor reserva recursos para
cada conexion falsa, lo que denega el servicio a clientes legitimos.

para contrarrestar esto se utilizan SYN cookies [RFC 4987] desplegadas en la
mayoria de los sistemas operativos

- Cuando el servidor recibe un segmento SYN, el servidor crea un numero de
  secuencia inicial a partir de un hash de: las ip de origen y destino;
  numeros de puertos de origen y destino; y un numero secreto que solo
  conoce el servidor. Este numero especial se llama cookie. El servidor
  envia un SYNACK con el cookie como numero de secuencia. El servidor no
  guarda informacion del estado de esta conexion (ni recursos ni cookie,
  nada).
- Un cliente legitimo devuleve el ACK. Cuando el servidor recive el
  segmento, recalcula el hash y verifica que el numero del ACK sea el cookie
  (recalculado) mas 1. Si es el caso, el servidor crea una conexion
  completa.
- Por el otro lado, si el cliente no responde con ACK, entonces el SYN
  original no hace daño al servidor, ya que el servidor no reservo recursos.

** Principios de Control de Congestion

Analizamos Control de congestion en un contexto generico:

congestion:
- por que es malo
- como se manifiesta (como es percivida en capas superiores)
- como evitarla

*** Causas y costos de la congestion
**** escenario 1 - dos emisores, un router con buffers infinitos

imagen 3.43

el host a transmite datos con un promedio de $\lambda_{in}$ bytes/seg. estos datos
son originales en el sentido en que cada unidad se envia al socket solo una vez.

el protocolo de capa de transporte es simple, solo encapsula y envia. sin
recuperacion de errores, sin control de flujo o control de congestion.

ignorando el delay de otras capas, la velocidad de transmision el $\lambda_{in}$

el host b opera de forma similar. asumimos tambien que transmite a una velocidad
de $\lambda_{in}$.

a y b comparten un enlace de capacidad R al router. el router tiene buffers en
dicho enlace para que no haya overflow.

imagen 3.44

la imagen muestra el throughput en funcion de la velocidad de transmision. si la
velocidad de transmision va entre 0 a R/2, todo va bien, todo lo que se
transmite, se recibe. Cuando la velocidad de transmision pasa R/2, el throughput
se limita a R/2. el enlace no puede entregar paquetes que exceden la capacidad
del enlace. maximizar el throughput puede parecer bueno, porque se aprovecha la
capacidad del enlace, pero el otro grafico de la imagen 3.44, muestra las
consecuencias de operar al limite de la capacidad del enlace. cuando se excede
la transmision de R/2, la cantidad de paquetes encolados en el router incrementa
sin limite, por lo que el delay tambien incrementa.

# conclusion
se tienen delays de encolado a medida que la velocidad de arrivos de paquetes
(al router) se acerca a la capacidad del enlace.

**** escenario 2 - dos emisores, un router con buffers finitos

en este escenario, puede ocurrir perdida de paquetes. la conexion es confiable.
si ocurre una perdida, el protocolo de capa-de-transporte se encarga de
retransmitir.

ahora hay una distincion entre transmision de datos-originales y transmision de
datos-originales-y-retransmitidos, $\lambda_{in}$ y $\lambda_{in}^{'}$
respectivamente.

a $\lambda_{in}^{'}$ tambien se lo llama ~offered load~ a la red.

...

el transmisor debe retransmitir para compensar por perdida de paquetes debido a
buffer overflow.

# conclusion
retransmisiones innecesarias ante largos delays pueden causar que un router
utilice el bandwidth del enlace para enviar copias de paquetes inncesarias.

**** escenario 3 - cuatro emisores, routers con buffers finitos, multiples saltos

imagen 3.47

...

# conclusion
cuando un paquete se pierde en la ruta, la capacidad de transmision que fue
utilizada para enviar el paquete hasta el punto en el que se perdio, fue
desperdiciada.

...

*** Abordando el control de congestion

la capa-de-red puede asistir o no, a la capa de transporte para el control de
congestion:
- End-to-end congestion control :: la capa-de-red no provee apoyo explicito a la
  capa-de-transporte para el control de congestion. Los end-systems deben
  inferir la presencia de congestion basados solamente en obsevaciones de la
  red. La perdida de paquetes en TCP (por timeout o 3 ACK consecutivos) se toma
  como congestion de la red.
- Network-assisted congestion control :: los routers proveen feedback explicito
  al emisor sobre el estado de la red. puede consistir de un solo bit indicando
  la congestion en el enlace. IBM SNA, DEC DECnet, ATM.  El control de
  congestion puede ser mas sofisticado, por ejemplo el router informa al emisor
  sobre la capacidad maxima de transmision que tiene en un enlace saliente
  (visto en ATM Available Bite Rate (ABR))

  otra forma: cuando un paquete pasa por un router con congestion, el router marca
  al encabezado indicando esto. cuando el receptor responde al emisor, lo hace con
  la misma marca en el encabezado, de esta forma indicando al emisor de la
  congestion. (toma un RTT)

imagen 3.49

** Control de Congestion de TCP

tcp debe usar control de congestion end-to-end ya que ip no provee asistencia.

en tcp cada transmisor limita la velocidad a la cual transmite trafico a la
conexion en funcion de la congestion percivida en la red.

- como limita la velocidad de transmision?
- como percive la congestion en la red?
- que algoritmo deberia utilizar para cambiar la velocidad de transmision?

# velocidad de transmision, cwnd
el emisor en tcp mantiene:
- buffer de entrada y salida
- LastByteRead
- rwnd
- ~congestion windows (cwnd)~ : *limita a la velocidad de transmision. La
  cantidad de datos sin ACK en el emisor, no puede superar al minimo entre cwnd
  y rwnd $$LastByteSent - LastByteAcked \leq \min\{cwnd,rwnd\}$$*

# percepcion de congestion, 3 ack duplicados, timeout
*se considera a un paquete perdido cuando ocurre un timeout o cuando el emisor
recibe tres ACK duplicados*.

por el otro lado, suponiendo que no hay perdidas de paquetes, el arrivo de
paquetes indica al emisor que todo anda bien y que se puede incrementar la
ventana de congestion. si los ACKs llegan despacio, la ventana de congestion
incrementa despacio. por el uso de este mecanismo, se dice que TCP es
~self-clocking~.

# algoritmo para determinar la velocidad de transmision
- un segmento perdido implica congestion y por lo tanto, el emisor debe bajar la
  velocidad de transmision
- un segmento confirmado indica que la red entrega los segmentos al receptor y
  por lo tanto el emisor puede incrementar la velocidad de transmision cuando
  llegue el ACK de un paquete enviado.

El algoritmo de control de congestion definido en [RFC 5681] tiene 3
componentes:
1. slow start
2. congestion avoidance
3. fast recovery (opcional)

*** Slow Start (SS)

cuando la conexion TCP empieza , en general cwnd = 1MSS[fn:cwnd]

[fn:cwnd]
[RFC 3390]


por lo que la velocidad de transmision es MSS/RTT aprox. *Se incrementa en 1 MSS
cada vez que se recibe un ACK*.

imagen 3.50

*crecimiento exponencial*.

como termina ss?
- si hay una perdida por timeout, cwnd=1, y se mantiene en modo SS
  se establece una variable ssthresh (slow start threshold) = cwnd/2 , cuando se
  detecta congestion
- si cwnd=ssthresh, se ingresa en modo (CA congestion avoidance)
- si se reciben 3 ACKs duplicados (4 ACKs iguales), se realiza fast retransmit y
  se ingresa en modo fast recovery

imagen 3.51

*** TODO Congestion Avoidance (CA)

[RFC 5681]

se setea a cwnd = cwnd/2 cuando se detecta congestion

por cada RTT, se incrementa cwnd en 1 MSS.  en realidad se incrementa por
cada ACK recibido, pero al enviarse cwnd/MSS (cantidad de MSSs/segmentos de
tamaño MSS) y cada ACK incrementa cwnd en MSS/cwnd; si se reciben todos los
ACKs, se termina incrementando cwnd en 1 MSS = cwnd/MSS (ACKs recibidos) *
MSS/cwnd (incremento por ACK)


como termina CA?
se entra en modo FR

*** TODO Fast Recovery (FR)

[RFC 5681]

el valor de cwnd se incrementa en 1 MSS por cada ACK duplicado recibido para el
segmento que causo que TCP entrara en modo FR.

utilizado en TCP Reno

*** en retrospectiva

# aimd (additive increase multiplicative decrease)
asumiendo que las perdidas ocurren por ACKs duplicados y no por timeout, el control de congestion de TCP consiste de incrementos aditivos y decrementos multiplicativos de la ventana de congestion (cwnd).

imagen 3.53

*** TODO TCP Splitting

** Resumen

servicios que un protocolo de capa de transporte provee a aplicaciones de red.

- mux/demux
- entrega de datos confiable
- garantia de delay
- garantia de ancho de banda

los servicios que se proveen estan restringidos por el protocolo de capa de red
(que esta abajo) si no se proveen garantias de delay y ancho de banda a
segmentos de capa de transporte, no se puede proveer estas garantias a
mensajes de aplicaciones

se puede proveer TDC a traves de acuse de recibo, timers, retransmisiones y
numeros de secuencia.

TCP provee:
- administracion de conexion
- control de flujo
- estimacion de tiempo de round-trip
- TDC
- control de congestion end-to-end que incrementa la velocidad de transmision de forma aditica y la decrementa de forma multiplicativa cuando se detecta perdida de paquetes.

La complejidad de TCP esta oculta para la aplicacion de red.

# otros protocolos de capa-de-transporte
The Datagram Congestion Control Protocol (DCCP) [RFC 4340] provides a
low-overhead, message-oriented, UDP-like unreliable service, but with an
application-selected form of congestion control that is compatible with TCP. If
reliable or semi-reliable data transfer is needed by an application, then this
would be performed within the application itself. DCCP is envisioned for use in
applications such as streaming media that can exploit the tradeoff between
timeliness and reliability of data delivery, but that want to be responsive to
network congestion.

Google’s QUIC (Quick UDP Internet Connections) protocol [Iyengar 2016],
implemented in Google’s Chromium browser, provides reliability via
retransmission as well as error correction, fast-connection setup, and a
rate-based congestion control algorithm that aims to be TCP friendly—all
implemented as an application-level protocol on top of UDP.

DCTCP (Data Center TCP) [Alizadeh 2010] is a version of TCP designed
specifically for data center networks, and uses ECN to better support the mix of
short- and long-lived flows that characterize data center workloads.

The Stream Control Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is a
reliable, message-oriented protocol that allows several different
application-level “streams” to be multiplexed through a single SCTP connection
(an approach known as “multi-streaming”). From a reliability standpoint, the
different streams within the connection are handled separately, so that packet
loss in one stream does not affect the delivery of data in other streams.

QUIC provides similar multi-stream semantics.

SCTP also allows data to be transferred over two outgoing paths when a host is
connected to two or more networks, optional delivery of out-of-order data, and a
number of other features.

SCTP’s flow- and congestion-control algorithms are essentially the same as in
TCP.

The TCP-Friendly Rate Control (TFRC) protocol [RFC 5348] is a
congestion-control protocol rather than a full-fledged transport-layer
protocol. It specifies a congestion-control mechanism that could be used
in another transport protocol such as DCCP (indeed one of the two
application-selectable protocols available in DCCP is TFRC). The goal of TFRC
is to smooth out the “saw tooth” behavior (see Figure3.53) in TCP congestion
control, while maintaining a long-term sending rate that is “reasonably” close
to that of TCP. With a smoother sending rate than TCP, TFRC is well-suited for
multimedia applications such as IP telephony or streaming media where such a
smooth rate is important. TFRC is an “equation-based” protocol that uses the
measured packet loss rate as input to an equation [Padhye 2000] that estimates
what TCP’s throughput would be if a TCP session experiences that loss rate.
This rate is then taken as TFRC’s target sending rate.

* Arquitectura de routers, IP y fragmentacion

la capa de red se puede descomponer en dos partes que interactuan entre si, el
~data plane~ y ~control plane~.

las funciones del ~data plane~ de la capa de red son las funciones por-enrutador
que determinan cómo un datagrama (paquete de capa de red) que llega a uno de los
enlaces de entrada de un enrutador se reenvía a uno de los enlaces de salida.

las funciones del ~control plane~ de la capa de red es la lógica de toda la red
que controla cómo se enruta un datagrama entre enrutadores a lo largo de una
ruta de extremo a extremo desde el host de origen al host de destino.

Tradicionalmente, estos protocolos de enrutamiento del plano de control y el
forwarding se han implementado juntas, monolíticamente, dentro de un enrutador.

en ~Software-defined networking (SDN)~, se separan explícitamente el ~data
plane~ y ~control plane~ mediante la implementación de las funciones del
~control plane~ como un servicio separado, normalmente en un "controlador"
remoto.

** Overview of the Network Layer

# ej
Figure 4.1 shows a simple network with two hosts, H1 and H2, and several routers
on the path between H1 and H2. Let’s suppose that H1 is sending information to
H2, and consider the role of the network layer in these hosts and in the
intervening routers. The network layer in H1 takes segments from the transport
layer in H1, encapsulates each segment into a datagram, and then sends the
datagrams to its nearby router, R1. At the receiving host, H2, the network layer
receives the datagrams from its nearby router R2, extracts the transport-layer
segments, and delivers the segments up to the transport layer at H2.


la funcion principal del data plane de cada router es enviar datagramas desde
los enlaces de entrada a los de salida.

la funcion principal del control plane es coordinar las acciones de envio
por-router para que los datagramas lleguen a su destino.

*los routers no ejecutan protocoloes de capa de aplicacion o capa de transporte*

imagen 4.1

*** Forwarding y Routing: The Data and Control Planes

el rol principal de la capa de red es mover paquetes del host emisor al host
receptor.

**** Forwarding
cuando un paquete llega al router por el enlace de entrada, el router debe
moverlo al enlace de salida apropiado.

For example, a packet arriving from Host H1 to Router R1 in Figure 4.1 must be
forwarded to the next router on a path to H2.


forwarding es solo una funcion (de muchas) implementada en el data plane.

en otro caso, un paquete puede tener la *salida bloqueada de un router* (por ej,
si el paquete se origina de host malicioso conocido o si tiene un host destino
prohibido), o puede ser *duplicado y enviado por multiples enlaces de salida*.

para lograr el forwarding, un router tiene una =forwarding table=. en router
envia un paquete al examinar los valores de algunos campos del paquete, y los
consulta en la tabla que termina indicando el enlace de salida apropiado para
dicho paquete.

**** Routing
la capa de red debe determinar la ruta o el camino que deben seguir los paquetes
a medida que fluyen desde el origen a su destino. Los algortimos de enrutamiento
calculan estos caminos. El enrutamiento es implementado en el control plane.

imagen 4.2

**** Control Plane: The Traditional Approach
como se configuran las forwading tables?

el algoritmo de enrutamiento determina el contenido de las tablas. este se
ejecuta en cada router y ambas funciones de forwarding y ruteo estan contenidas
en el router.

el algoritmo de ruteo de un router se comunica con otro de otro router para
calcular los valores para la forwarding table.

**** Control Plane: The SDN Approach

imagen 4.3

un metodo alternativo consiste de un controlador remoto, fisicamente separado,
que computa y distribuye las forwarding tables a cada router.

el enrutamiento es separado fisicamente del router. Este solamente realiza
forwarding.

el controlador remoto puede estar implementado en un data center, con alta
redundancia y confiabilidad (ante fallas)

*** Modelo de Servicios de Red
El =network service model= define las caracteristicas de la entrega de paquetes
end-to-end.

Algunos posibles servicios que puede ofrecer la capa de red son:
- Entrega garantizada :: Se garantiza que un paquete enviado de un host
  eventualmente llegara a su destino
- Entrega garantizada con demora limitada :: Ademas de garantizar la entrega del
  paquete, tambien se garantiza sera completada dentro de una cantidad de
  tiempo.
- Entrega de paquete en orden :: se garantiza que los paquetes arrivan al
  destino en el orden en que se enviaron.
- Ancho de banda minimo garantizado :: se emula el comportamiento de un enlace
  de transmision de una velocidad garantizada. ?
- Seguridad :: encriptacion de los datagramas en el origen y decriptacion en el
  destino.

la capa de red de la internet provee un solo servicio, *=best-effort service=*
en donde no hay garantias de entrega, entrega eventual u orden de los paquetes.

** Qué hay adentro de un router?

imagen 4.4

A high-level view of a generic router architecture is shown in Figure 4.4.

Se pueden identificar cuatro componentes de un router:
- Puertos de entrada :: se realiza una funcion de lookup en el puerto de
  entrada. La tabla de forwarding es consultada para determinar el puerto de
  salida. Los paquetes de control (por ej paquetes con informacion de protocolos
  de ruteo) son enviados del puerto de entrada al procesador.
- Switching fabric :: Conecta a los puertos de entrada con los de salida.
- Puertos de salida :: almacena paquetes recibidos de la switching fabric y los
  transmite al enlace saliente.
- Procesador :: realiza las funciones del control plane. en routers
  tradicionales, ejecuta los protocolos de ruteo, administra las tablas de ruteo
  y el estado de los enlaces, calcula la forwarding table. En routers SDN, el
  procesador se comunica con el controlador remoto para recibir entradas de la
  forwarding table.

*** Input Port Processing and Destination-Based Forwarding

imagen 4.5
#+caption: vista detallada del procesamiento de entrada.

La forwarding table es copiada del procesador a las line cards sobre un bus
aparte. Con una copia en cada puerto, las decisiones de forwarding pueden
hacerse de forma local sin invocar al procesador por cada paquete entrante y asi
evitar cuellos de botella.

#+caption: forwarding table con 4 interfaces
| Destination Address Range           | Link Interface |
|-------------------------------------+----------------|
| 11001000 00010111 00010000 00000000 |              0 |
| through                             |                |
| 11001000 00010111 00010111 11111111 |                |
|-------------------------------------+----------------|
| 11001000 00010111 00011000 00000000 |              1 |
| through                             |                |
| 11001000 00010111 00011000 11111111 |                |
|-------------------------------------+----------------|
| 11001000 00010111 00011001 00000000 |              2 |
| through                             |                |
| 11001000 00010111 00011111 11111111 |                |
|-------------------------------------+----------------|
| Otherwise                           |              3 |
|-------------------------------------+----------------|

#+caption: forwarding table utilizando prefijos
| Prefix                     | Link Interface |
|----------------------------+----------------|
| 11001000 00010111 00010    |              0 |
| 11001000 00010111 00011000 |              1 |
| 11001000 00010111 00011    |              2 |
| Otherwise                  |              3 |

Con el ultimo estile de la tabla de forwarding, el router encuentra un prefijo
de la direccion de destino entre las entradas de la table; si hay alguna
conincidencia, el router envia el paquete al enlace asociado.

si hay multiples conincidencias, el router utiliza el =longest prefix matching
rule=; esto es, encuentra el prefijo mas largo que coincide y envia el paquete a
la interfaz asociada.

el paquete es enviado al puerto de salida mediante la switching fabric.

otras acciones ocurren en la etapa de procesamiento en el puerto de entrada
1) procesamiento de capa fisica y capa de enlace
2) los campos de version de numero del paquete, checksum y time-to-live son
   verificados y los ultimos dos campos deben ser reescritos
3) se actualizan los contadores utilizados para la administracion de la red (ej
   la cantidad de datagramas IP recibidos)

*** Switching (fabric)

los paquetes son enviados de los puertos de entrada a los de salida mediante la switching fabric

- Switching via memoria :: los primeros routers eran computadoras tradicionales. El switcheo de paquetes lo realizaba la CPU. Los puertos de entrada y salida funcionaban como dispostivos de I\O tradicionales en un sistema operativo. Se indicaba el arribo de un paquete con una interrupcion, se copiaba el paquete en memoria, se procesaba y copiaba al puerto de salida correspondiente.
- Switching via bus :: un puerto de entrada tranfiere el paquete directamente al puerto de salida por sobre un bus compartido, sin intervencion del procesador. esto se logra haciendo que el puerto de entrada anteponga un encabezado al paquete indicando el puerto de salida y enviandolo por el bus. todos los puertos de salida reciben el paquete pero solo el que coincida con el encabezado lo guarda y le quita el encabezado. solo un paquete se puede transferir por el bus a la vez. el ancho de banda esta limitado por la velocidad del bus.
- Switching via red de interconexiones :: un crossbar switch es una red de interconexion que consiste de 2N buses que conectan N puertos de entrada a N puertos de salida. Cada bus vertical intersecta con uno horizontal formando una malla que puede ser controlada. Multiples paquetes pueden ser transferidos en simultaneo mientras no se dirijan al mismo puerto. Un crossbar switch es no bloqueante: un paquete enviado a un puerto de salida no sera bloqueado siempre y cuando no haya otro paquete siendo enviado al puerto de salida.

imagen 4.6

*** Procesamiento del puerto de salida

se toman paquetes que fueron almacenados en la memoria del puerto y se transmiten por el enlace. esto incluye seleccion, desencolado de paquetes y la funciones de capa fisica y capa de enlace necesarias.

imagen 4.7

*** Where Does Queuing Occur?
Las colas de paquetes se pueden formar en los puertos de entrada y de salida. La
ubicacion y tamaño del encolado va a depender de la carga de trafico, la
velocidad de la switching fabric y la velocidad de line card.

**** Encolado en la entrada

Si la velocidad de switcheo es mas rapida que la de transmision, el encolado sera despreciable.

=head-of-the-line (HOL) blocking= un paquete en la cola de entrada debe esperar
a ser transferido por la switching fabric porque el primer paquete de la misma
cola (head of line) esta bloqueado.

imagen 4.8

cuando no hay suficiente memoria para guardar el paquete entrante, se debe
decidir entre descartar el paquete (politica ~drop-tail~) o remover paquetes
encolados. En algunos casos puede ser conveniente descartar (o marcar) un
paquete antes de que se llene la cola y luego enviar una señal al amisor
(algortimos ~active queue management~).

**** Encolado en la salida

Se pueden formar colas en los puertos de salida cuando la switching fabric es
mas rapida que la velocidad de transmision.

imagen 4.9

how much buffering is required? [RFC 3439] indica que la cantidad de buffering
(B) deberia ser igual a el (RTT) promedio multiplicado por la capacidad del
enlace (C).

$$ B = RTT * C$$

*** Packet Scheduling

Decide el puerto de entrada a atender.

**** FIFO
Esta politica selecciona paquetes para su transmision segun su orden de llagada a la cola del puerto de salida

imagen 4.10

imagen 4.11

**** Cola de prioridad
Los paquetes entrantes al puerto de salida son clasificados en clases de prioridades apenas llegan a la cola.

En la practica, se configura la cola para que los paquetes con informacion para la administracion de la red tengan prioridad sobre trafico de usuarios.

imagen 4.12

Cada clase tiene su propia cola. Se deben transmitir todos los paquetes de una cola de mayor prioridad antes continuar con las demas.

imagen 4.13

**** Round Robin (RR) y Weighted Fair Queuing (WFQ)
En Round Robin, los paquetes son ordenados en clases como en la politica de
cola-de-prioridad. La diferencia es que el servicio a las colas se alterna sin
prioridad.

La politica =work-conserving queuing= (encolado de conservacion de trabajo) nunca permite que el enlace permanezca desocupado cuando haya paquetes (de cualquier clase) encolados para ser transmitidos. Una politica
work-conserving round robin que busca paquetes de una clase pero no encuentra ninguno, pasa a la siguiente clase.

imagen 4.14

En la politica =weighted fair queuing (WFQ)=, los paquetes entrantes son clasificados y encolados en al area de espera de la clase apropiada. Como en RR, las clases se van sirviendo en orden; cuando no hay mas paquetes en una, se pasa a la siguiente. La diferencia entre WFQ y RR es que cada clase puede recibir una cantidad de servicio mas que otra clase. A cada clase $i$ se le asigna un peso, $w_{i}$. Durante cualquier intervalo de tiempo donde hay que enviar clase_{i} paquetes, la clase i tiene garantizada recibir ($w_{i}/\sum_{j}w_{j}$) de servicio, donde la suma en el denominador es tomada sobre todas las clases que tambien tienen paquetes para transmitir. En el peor caso, la clase i tiene garantizada una fraccion del ancho de banda. Para un enlace con velocidad de transmision R, la clase i siempre tendra un throughput de al menos $R*w_{i}/\sum_{j}w_{j}$.

** IP

Existen dos versiones de IP en  uso hoy en dia.

*** IPv4 [RFC 791]

imagen 4.16

**** Formato de Datagrama

campos clave del datagrama IPv4:

- Version number :: 4 bits la version del protocolo del datagrama. El router lo puede utilizar para determinar como interpretar el resto del datagrama.
- Header length :: un datagrama IPv4 puede contener una cantidad variable de opciones. Cuatro bits determinan donde en el datagrama comienza el payload. La mayoria de los datagramas no contiene opciones por lo que tienen un encabezado de 20 Bytes.
- Type of service :: Los bits de =TOS= fueron incluidos para permitir que
  diferentes tipos de datagramas sean distinguibles unos de otros. Por ej,
  datagramas de tiempo real y de no tiempo real. Es configurado por el
  administrador de red del router.
- Datagram length :: tamaño total del datagrama (encabezado + datos) en bytes. Numero de 16 bits, por lo que el maximo teorico es 65535 bytes. Rara vez son mayores a 1500 bytes.
- Identifier, flags, fragmentation offset :: Tienen que ver con la framentacion IP.
- Time-to-live :: El campo =TTL= se incluye para asegurarse de que los datagramas no circulen para siempre por la red. Se decrementa en uno cada vez que es procesado por un router. Si llega a cero, es descartado.
- Protocol :: Tipicamente utilizado cuando el datagrama llega a su destino. El valor indica el protocolo especifico de capa-de-transporte (IANA Protocol Numbers 2016). El numero de protocolo es el pegamento que une a la capa-de-red con la capa-de-transporte. (como los puertos unen transpote con aplicacion)
- Header checksum :: Asiste al router para detectar errores de bits en el datagrama. Si el checksum no coincide con el computado, se descarta. Debe ser reescrito ya que cambian algunos campos del encabezado (TTL, opciones).
- Source and destination IP addresses :: Cuando el host origen crea un
  datagrama, inserta su propia direccion IP y la del host destino en estos
  campos.
- Options :: permiten extender el encabezado IP.
- Data (payload) :: contiene el segmento de capa de transporte para ser entregado.

Asumiendo que no se utiliza el campo de opciones, el encabezado es de 20 Bytes.

Si el segmento que se transporta es de TCP, el datagrama tiene 40 bytes de encabezado (20 bytes de TCP)

**** Fragmentacion de Datagramas

La cantidad maxima de datos que un frame de capa de enlace puede llevar es
llamado =maximum transmission unit (MTU)=. Debido a que cada datagrama es
encapsulado dentro de un frame, el MTU limita el largo de un datagrama IP. La
limitacion no es un problema en si, lo que lo es, es que cada enlace de la ruta
entre emisor y receptor pueden utilizar distintos protocolos de capa de enlace,
y que estos protocolos tengan diferentes MTUs.

Ante el problema de "apretar" un datagrama IP en el campo de payload de un frame de capa de enlace, la solucion es fragmentar el payload en el datagrama IP en dos o mas datagramas pequeños, encapsulando cada uno de estos en un frame separado; y enviar estos frames por el enlace de salida. A cada uno de estos datagramas se los llama =fragmentos=.

Los fragmentos deben ser reensamblados antes llegar a la *capa-de-transporte* en el destino. Los protocolos de capa-de-transporte esperan recibir un datagrama completo de la capa-de-red. El reensamblado ocurre en el destino.

Para permitir al host destino realizar la tarea de reensamblado, se hace uso de los campos de identificacion, flag, y ~framentation offset~ en el encabezado del datagrama IP.

Tipicamente, el host emisor incrementa el numero de identificacion para datagrama que envia. Cuando un router debe fragmentar un datagrama, cada framento es marcado con las ip de origen y destino y el numero de indentificacion del datagrama original.

Para que el host destino se asegure de haber recibido todos los fragmentos, el ultimo fragmento es marcado con un bit en 0, mientras que el resto de los fragmentos tienen un 1. El campo de ~offset~ tambien es utilizado para verificar si falta algun fragmento.

imagen 4.17
* Data Plane - Direccionamiento e IPv6
** Direccionamiento IPv4
# interfaces
A host typically has only a single link into the network; when IP in the host
wants to send a datagram, it does so over this link. The boundary between the
host and the physical link is called an =interface=.

Now consider a router and its interfaces. Because a router’s job is to receive a
datagram on one link and forward the datagram on some other link, a router
necessarily has two or more links to which it is connected. The boundary between
the router and any one of its links is also called an interface. A router thus
has multiple interfaces, one for each of its links.

# cada interfaz tiene una IP
Because every host and router is capable of sending and receiving IP datagrams,
*IP requires each host and router interface to have its own IP address*. Thus, an
IP address is technically associated with an interface, rather than with the
host or router containing that interface.

Each IP address is 32 bits long (4 bytes), and there are thus a total of
$2^{32}$ (or approximately 4 billion) possible IP addresses.

Each interface on every host and router in the global Internet must have an IP
address that is globally unique. These addresses cannot be chosen in a
willy-nilly manner, however. A portion of an interface’s IP address will be
determined by the subnet to which it is connected.

# dominio de broadcast, subnet
--- A group of interfaces interconnected to each other by a network that
contains no routers forms a =subnet=. [RFC 950] Interfaces in the same subnet
share a number of bits in their IP address. This is called =subnet mask=, which
are the (N) leftmost bits of the address. ---

imagen 4.18

imagen 4.19

For a general interconnected system of routers and hosts, we can use the
following recipe to define the subnets in the system:

#+BEGIN_QUOTE
To determine the subnets, detach each interface from its host or router,
creating islands of isolated networks, with interfaces terminating the end
points of the isolated networks. Each of these isolated networks is called a
subnet.
#+END_QUOTE

imagen 4.20

# cidr
The Internet’s address assignment strategy is known as =Classless Interdomain
Routing (CIDR)= [RFC 4632]. CIDR generalizes the notion of subnet addressing. As
with subnet addressing, the 32-bit IP address is divided into two parts and
again has the dotted-decimal form a.b.c.d/x, where x indicates the number of
bits in the first part of the address.

# network prefix of ip adress
The x most significant bits of an address of the form a.b.c.d/x constitute the
*network portion* of the IP address, and are often referred to as the =prefix= (or
network prefix) of the address. An organization is typically assigned a block of
contiguous addresses, that is, a range of addresses with a common prefix.

# host number in prefix, may be another subnet
The remaining $32-x$ bits of an address can be thought of as distinguishing
among the devices within the organization, all of which have the same network
prefix. These are the bits that will be considered when forwarding packets at
routers within the organization. These lower-order bits may (or may not) have an
additional subnetting structure, such as that discussed above.

# deprecated adressing
Before CIDR was adopted, the network portions of an IP address were constrained
to be 8, 16, or 24 bits in length, an addressing scheme known as =classful
addressing=, since subnets with 8-, 16-, and 24-bit subnet addresses were known
as class A, B, and C networks, respectively.

The IP broadcast address 255.255.255.255. When a host sends a datagram with
destination address 255.255.255.255, the message is delivered to all hosts on
the same subnet. Routers optionally forward the message into neighboring subnets
as well (although they usually don’t).

*** Obtaining a Block of Addresses

*how an organization gets a block of addresses for its devices?*

how a device (such as a host) is assigned an address from within the
organization’s block of addresses?

In order to obtain a block of IP addresses for use within an organization’s
subnet, a network administrator might first contact its ISP, which would provide
addresses from a larger block of addresses that had already been allocated to
the ISP.

| ISP’s block    | 200.23.16.0/20 | =11001000 00010111 0001=0000 00000000 |
| Organization 0 | 200.23.16.0/23 | =11001000 00010111 0001000=0 00000000 |
| Organization 1 | 200.23.18.0/23 | =11001000 00010111 0001001=0 00000000 |
| Organization 2 | 200.23.20.0/23 | =11001000 00010111 0001010=0 00000000 |
| ...            | ...            | ...                                   |
| Organization 7 | 200.23.30.0/23 | =11001000 00010111 0001111=0 00000000 |

Is there a global authority that has ultimate responsibility for managing the IP
address space and allocating address blocks to ISPs and other organizations?

Indeed there is! IP addresses are managed under the authority of the Internet
Corporation for Assigned Names and Numbers (ICANN) [ICANN 2016], based on
guidelines set forth in [RFC 7020].

*** Obtaining a Host Address: The Dynamic Host Configuration Protocol (DHCP)

Once an organization has obtained a block of addresses, it can assign individual
IP addresses to the host and router interfaces in its organization. A system
administrator will typically manually configure the IP addresses into the
router. Host addresses can also be configured manually, but typically this is
done using the =Dynamic Host Configuration Protocol (DHCP)= [RFC 2131]. DHCP
allows a host to obtain an IP address automatically.

A network administrator can configure DHCP so that a given host receives the
same IP address each time it connects to the network, or a host may be assigned
a =temporary IP address= that will be different each time the host connects to
the network. In addition to host IP address assignment, DHCP also allows a host
to learn additional information, such as its subnet mask, the address of its
first-hop router (often called the default gateway), and the address of its
local DNS server.

DHCP is a client-server protocol. A client is typically a newly arriving host
wanting to obtain network configuration information, including an IP address for
itself. In the simplest case, each subnet will have a DHCP server. If no server
is present on the subnet, a DHCP relay agent (typically a router) that knows the
address of a DHCP server for that network is needed.

Figure 4.23 shows a DHCP server attached to subnet 223.1.2/24, with the router
serving as the relay agent for arriving clients attached to subnets 223.1.1/24
and 223.1.3/24.

imagen 4.23

For a newly arriving host, the DHCP protocol is a four-step process for the
network setting shown in Figure 4.23:

1) DHCP server discovery. The first task of a newly arriving host is to find a
   DHCP server with which to interact. This is done using a DHCP discover
   message, which a client sends within a UDP packet to port 67. The UDP packet
   is encapsulated in an IP datagram. the IP datagram contains a DHCP discover
   message along with the broadcast destination IP address of 255.255.255.255
   and a “this host” source IP address of 0.0.0.0. The DHCP client passes the IP
   datagram to the link layer, which then broadcasts this frame to all nodes
   attached to the subnet.
2) DHCP server offer(s). A DHCP server receiving a DHCP discover message
   responds to the client with a DHCP offer message that is broadcast to all
   nodes on the subnet, again using the IP broadcast address of 255.255.255.255.
   Since several DHCP servers can be present on the subnet, the client may find
   itself in the enviable position of being able to choose from among several
   offers. Each *server offer message contains the transaction ID of the
   received discover message*, the *proposed IP address for the client*, the
   *network mask*, and an *IP address lease time, the amount of time for which
   the IP address will be valid*. It is common for the server to set the lease
   time to several hours or days.
3) DHCP request. The newly arriving client will choose from among one or more
   server offers and respond to its selected offer with a DHCP request message,
   echoing back the configuration parameters.
4) DHCP ACK. The server responds to the DHCP request message with a DHCP ACK
   message, confirming the requested parameters.

Once the client receives the DHCP ACK, the interaction is complete and the
client can use the DHCP-allocated IP address for the lease duration. Since a
client may want to use its address beyond the lease’s expiration, DHCP also
provides a mechanism that allows a client to renew its lease on an IP address.

imagen 4.24

In this figure, yiaddr (as in “your Internet address”) indicates the address
being allocated to the newly arriving client

*** Network Address Translation (NAT) [RFC 2663; RFC 3022]

Figure 4.25 shows the operation of a NAT-enabled router. The NAT-enabled router,
residing in the home, has an interface that is part of the home network on the
right of Figure 4.25. Addressing within the home network is exactly as we have
seen above; all four interfaces in the home network have the same subnet address
of 10.0.0/24.

# redes privadas
The address space 10.0.0.0/8 is one of three portions of the IP address space
that is reserved in =[RFC 1918]= for a =private network= or a =realm with
private addresses=, such as the home network in Figure 4.25. A realm with
private addresses refers to a network whose addresses only have meaning to
devices within that network. To see why this is important, consider the fact
that there are hundreds of thousands of home networks, many using the same
address space, 10.0.0.0/24. Devices within a given home network can send packets
to each other using 10.0.0.0/24 addressing. However, packets forwarded beyond
the home network into the larger global Internet clearly cannot use these
addresses (as either a source or a destination address) because there are
hundreds of thousands of networks using this block of addresses. That is, the
10.0.0.0/24 addresses can only have meaning within the given home network. But
if private addresses only have meaning within a given network, how is addressing
handled when packets are sent to or received from the global Internet, where
addresses are necessarily unique?

The NAT-enabled router does not look like a router to the outside world. Instead
the NAT router behaves to the outside world as a single device with a single IP
address. In Figure 4.25, all traffic leaving the home router for the larger
Internet has a source IP address of 138.76.29.7, and all traffic entering the
home router must have a destination address of 138.76.29.7. In essence, the
NAT-enabled router is hiding the details of the home network from the outside
world.

# tabla NAT
A =NAT translation table= is used at the NAT-enabled router to know which
internal host it should forward a datagram. The table includes port numbers and
IP addresses.

Suppose a user sitting in a home network behind host 10.0.0.1requests a Web page
on some Web server (port 80) with IP address 128.119.40.186. The host
10.0.0.1assigns the (arbitrary) source port number 3345 and sends the datagram
into the LAN. The NAT routerreceives the datagram, generates a new source port
number 5001 for the datagram, replaces the source IP address with its WAN-side
IP address 138.76.29.7, and replaces the original source portnumber 3345 with
the new source port number 5001. When generating a new source port number,
theNAT router can select any source port number that is not currently in the NAT
translation table. (Notethat because a port number field is 16 bits long, the
NAT protocol can support over 60,000 simultaneous connections with a single
WAN-side IP address for the router!) NAT in the router also adds an entry toits
NAT translation table. The Web server, blissfully unaware that the arriving
datagram containing theHTTP request has been manipulated by the NAT router,
responds with a datagram whose destinationaddress is the IP address of the NAT
router, and whose destination port number is 5001. When thisdatagram arrives at
the NAT router, the router indexes the NAT translation table using the
destination IPaddress and destination port number to obtain the appropriate IP
address (10.0.0.1) and destination portnumber (3345) for the browser in the home
network. The router then rewrites the datagram’s destinationaddress and
destination port number, and forwards the datagram into the home network.

** IPv6
*** Datagram Format

imagen 4.26

**** Changes introduced in the datagram format

- Expanded addressing capabilities :: IPv6 increases the size of the IP address
  from 32 to 128 bits. This ensures that the world won’t run out of IP
  addresses. In addition to unicast and multicast addresses, IPv6 has introduced
  a new type of address, called an *anycast address*, that allows a datagram to
  be delivered to any one of a group of hosts. (This feature could be used, for
  example, to send an HTTP GET to the nearest of a number of mirror sites that
  contain a given document.)
- A streamlined 40-byte header :: a number of IPv4 fields have been dropped or
  made optional. The resulting 40-byte fixed-length header (of IPv6) allows for
  faster processing of the IP datagram by a router. A new encoding of options
  allows for more flexible options processing.
- Flow labeling :: IPv6 has an elusive definition of a flow. [RFC 2460] states
  that this allows “labeling of packets belonging to particular flows for which
  the sender requests special handling, such as a non-default quality of service
  or real-time service.” For example, audio and video transmission might likely
  be treated as a flow. On the other hand, the more traditional applications,
  such as file transfer and e-mail, might not be treated as flows. It is
  possible that the traffic carried by a high-priority user (for example,
  someone paying for better service for their traffic) might also be treated as
  a flow. What is clear, however, is that the designers of IPv6 foresaw the
  eventual need to be able to differentiate among the flows, even if the exact
  meaning of a flow had yet to be determined.

**** Fields
- Version :: This 4-bit field identifies the IP version number
- Traffic class :: The 8-bit traffic class field, like the TOS field in IPv4, can
  be used to give priority to certain datagrams within a flow, or it can be used
  to give priority to datagrams from certain applications over datagrams from
  other applications (for example, VOIP over SMTP e-mail).
- Flow label :: this 20-bit field is used to identify a flow of datagrams.
- Payload length :: This 16-bit value is treated as an unsigned integer giving the
  number of bytes in the IPv6 datagram following the fixed-length, 40-byte
  datagram header.
- Next header :: This field identifies the protocol to which the payload of the
  datagram will be delivered (for example, to TCP or UDP). The field uses the
  same values as the protocol field in the IPv4 header.
- Hop limit :: The contents of this field are decremented by one by each router
  that forwards the datagram. If the hop limit count reaches zero, the datagram
  is discarded.
- Source and destination addresses :: The various formats of the IPv6 128-bit
  address are described in [RFC 4291].
- Data :: This is the payload portion of the IPv6 datagram. When the datagram
  reaches its destination, the payload will be removed from the IP datagram and
  passed on to the protocol specified in the next header field.

**** Some fields are removed from IPv4 to IPv6

- Fragmentation/reassembly :: IPv6 does not allow for fragmentation and reassembly
  at intermediate routers; these operations can be performed only by the source
  and destination. If an IPv6 datagram received by a router is too large to be
  forwarded over the outgoing link, the router simply drops the datagram and
  sends a “Packet Too Big” ICMP error message back to the sender. The sender can
  then resend the data, using a smaller IP datagram size. Fragmentation and
  reassembly is a time-consuming operation; removing this functionality from the
  routers and placing it squarely in the end systems considerably speeds up IP
  forwarding within the network.
- Header checksum :: Because the transport-layer (for example, TCP and UDP) and
  link-layer (for example, Ethernet) protocols in the Internet layers perform
  checksumming, the designers of IP probably felt that this functionality was
  sufficiently redundant in the network layer that it could be removed. Once
  again, fast processing of IP packets was a central concern. Recall that the
  header checksum needed to be recomputed at every router. As with fragmentation
  and reassembly, this too was a costly operation in IPv4.
- Options :: An options field is no longer a part of the standard IP header.
  However, it has not gone away. Instead, the options field is one of the
  possible next headers pointed to from within the IPv6 header. That is, just as
  TCP or UDP protocol headers can be the next header within an IP packet, so too
  can an options field. The removal of the options field results in a
  fixed-length, 40-byte IP header.

*** Transitioning from IPv4 to IPv6

imagen 4.27

The approach to IPv4-to-IPv6 transition that has been most widely adopted in
practice involves =tunneling [RFC 4213]=.

Suppose two IPv6 nodes (in this example, B and E in Figure 4.27) want to
interoperate using IPv6 datagrams but are connected to each other by intervening
IPv4 routers.

We refer to the intervening set of IPv4 routers between two IPv6 routers as a
=tunnel=, as illustrated in Figure 4.27. With tunneling, the IPv6 node on the
sending side of the tunnel (in this example, B) takes the entire IPv6 datagram
and puts it in the data (payload) field of an IPv4 datagram. This IPv4 datagram
is then addressed to the IPv6 node on the receiving side of the tunnel (in this
example, E) and sent to the first node in the tunnel (in this example, C).

The intervening IPv4 routers in the tunnel route this IPv4 datagram among
themselves, just as they would any other datagram, blissfully unaware that the
IPv4 datagram itself contains a complete IPv6 datagram. The IPv6 node on the
receiving side of the tunnel eventually receives the IPv4 datagram (it is the
destination of the IPv4 datagram!), determines that the IPv4 datagram contains
an IPv6 datagram (by observing that the protocol number field in the IPv4
datagram is 41 =[RFC 4213]=, indicating that the IPv4 payload is a IPv6
datagram), extracts the IPv6 datagram, and then routes the IPv6 datagram exactly
as it would if it had received the IPv6 datagram from a directly connected IPv6
neighbor.

** Generalized Forwarding and SDN

Recall from earlier that destination-based forwarding as the two steps of
looking up a destination IP address (“match”), then sending the packet into the
switching fabric to the specified output port (“action”).

now consider a significantly more general “match-plus-action” paradigm, where
the “match” can be made over multiple header fields associated with different
protocols at different layers in the protocol stack.

The “action” can include
- forwarding the packet to one or more output ports (as in destination-based
  forwarding),
- load balancing packets across multiple outgoing interfaces that lead to a
  service (as in load balancing),
- rewriting header values (as in NAT),
- purposefully blocking/dropping a packet (as in a firewall),
- sending a packet to a special server for further processing and action (as in
  DPI),
- and more.

In generalized forwarding, a match-plus-action table generalizes the notion of
the destination-based forwarding table. Because forwarding decisions may be made
using network-layer and/or link-layer source and destination addresses, the
forwarding devices are more accurately described as “packet switches” rather
than layer 3 “routers” or layer 2 to these devices as packet switches.

imagen 4.28

Figure 4.28
#+caption: Generalized forwarding: Each packet switch contains a match-plus-action table that is computed and distributed by a remote controller

Figure 4.28 shows a match-plus-action table in each packet switch, with the
table being computed, installed, and updated by a remote controller. We note
that while it is possible for the control components at the individual packet
switch to interact with each other, in practice generalized match-plus-action
capabilities are implemented via a remote controller that computes, installs,
and updates these tables.

OpenFlow is a highly visible and successful standard that has pioneered the notion
of the match-plus-action forwarding abstraction and controllers, as well as the
SDN revolution more generally.

Each entry in the match-plus-action forwarding table, known as a =flow table= in
OpenFlow, includes:
- A set of header field values to which an incoming packet will be matched. As
  in the case of destination-based forwarding, hardware-based matching is most
  rapidly performed in TCAM memory, with more than a million destination address
  entries being possible. A packet that matches no flow table entry can be
  dropped or sent to the remote controller for more processing. In practice, a
  flow table may be implemented by multiple flow tables for performance or cost
  reasons.
- A set of counters that are updated as packets are matched to flow table
  entries. These counters might include the number of packets that have been
  matched by that table entry, and the time since the table entry was last
  updated.
- A set of actions to be taken when a packet matches a flow table entry. These
  actions might be to forward the packet to a given output port, to drop the
  packet, makes copies of the packet and sent them to multiple output ports,
  and/or to rewrite selected header fields.

*** Match

imagen 4.29

Recall that a link-layer frame arriving to a packet switch will contain a
network-layer datagram as its payload, which in turn will typically contain a
transport-layer segment.

OpenFlow’s match abstraction *allows for a match to be made on selected fields
from three layers of protocol headers* (thus rather brazenly *defying the
layering principle*). Since we’ve not yet covered the link layer, suffice it to
say that the source and destination MAC addresses are the link-layer addresses
associated with the frame’s sending and receiving interfaces; by forwarding on
the basis of Ethernet addresses rather than IP addresses, we can see that an
OpenFlow-enabled device can equally perform as a router (layer-3 device)
forwarding datagrams as well as a switch (layer-2 device) forwarding frames.

# ?
The Ethernet type field corresponds to the upper layer protocol (e.g., IP) to
which the frame’s payload will be de-multiplexed, and the VLAN fields are
concerned with so-called virtual LANs.


Flow table entries may also have wildcards. For example, an IP address of
128.119.*.* in a flow table will match the corresponding address field of any
datagram that has 128.119 as the first 16 bits of its address.

Each flow table entry also has an associated priority. If a packet matches
multiple flow table entries, the selected match and corresponding action will be
that of the highest priority entry with which the packet matches.

*** Action

each flow table entry has a list of zero or more actions that determine the
processing that is to be applied to a packet that matches a flow table entry. If
there are multiple actions, they are performed in the order specified in the
list.

Among the most important possible actions are:

- Forwarding :: An incoming packet may be
  - forwarded to a particular physical output port,
  - broadcast over all ports (except the port on which it arrived) or
  - multicast over a selected set of ports.
  The packet may be encapsulated and sent to the remote controller for this
  device. That controller then may (or may not) take some action on that packet,
  including installing new flow table entries, and may return the packet to the
  device for forwarding under the updated set of flow table rules.
- Dropping :: A flow table entry with no action indicates that a matched packet
  should be dropped.
- Modify-field :: The values in ten packet header fields (all layer 2, 3, and 4
  fields except the IP Protocol field) may be re-written before the packet is
  forwarded to the chosen output port.

*** OpenFlow Examples of Match-plus-action in Action

imagen 4.30

The network has 6 hosts (h1 to h6) and three packet switches (s1 to s3), each
with four local interfaces (1 through 4). We’ll consider a number of
network-wide behaviors that we’d like to implement, and the flow table entries
in s1, s2 and s3 needed to implement this behavior.

**** A First Example: Simple Forwarding
As a very simple example, suppose that the desired forwarding behavior is that
packets from h5 or h6 destined to h3 or h4 are to be forwarded from s3 to s1,
and then from s1 to s2 (thus completely avoiding the use of the link between s3
and s2). The flow table entry in s1 would be:

| s1 Flow Table (Example 1)                                |            |
| Match                                                    | Action     |
| Ingress Port = 1 ; IP Src = 10.3.*.* ; IP Dst = 10.2.*.* | Forward(4) |
| ...                                                      | ...        |

Of course, we’ll also need a flow table entry in s3 so that datagrams sent from
h5 or h6 are forwarded to s1 over outgoing interface 3:

| s3 Flow Table (Example 1)             |            |
| Match                                 | Action     |
| IP Src = 10.3.*.* ; IP Dst = 10.2.*.* | Forward(3) |
| ...                                   | ...        |

Lastly, we’ll also need a flow table entry in s2 to complete this first example,
so that datagrams arriving from s1 are forwarded to their destination, either
host h3 or h4:

| s2 Flow Table (Example 1)            |            |
| Match                                | Action     |
| Ingress port = 2 ; IP Dst = 10.2.0.3 | Forward(3) |
| Ingress port = 2 ; IP Dst = 10.2.0.4 | Forward(4) |
| ...                                  | ...        |

**** A Second Example: Load Balancing
As a second example, let’s consider a load-balancing scenario, where datagrams
from h3 destined to 10.1.*.* are to be forwarded over the direct link between s2
and s1, while datagrams from h4 destined to 10.1.*.* are to be forwarded over
the link between s2 and s3 (and then from s3 to s1). Note that this behavior
couldn’t be achieved with IP’s destination-based forwarding. In this case, the
flow table in s2 would be:

| s2 Flow Table (Example 2)           |            |
| Match                               | Action     |
| Ingress port = 3; IP Dst = 10.1.*.* | Forward(2) |
| Ingress port = 4; IP Dst = 10.1.*.* | Forward(1) |
| ...                                 | ...        |

Flow table entries are also needed at s1 to forward the datagrams received from
s2 to either h1 or h2; and flow table entries are needed at s3 to forward
datagrams received on interface 4 from s2 over interface 3 towards s1.

See if you can figure out these flow table entries at s1 and s3.

**** A Third Example: Firewalling

As a third example, let’s consider a firewall scenario in which s2 wants only to
receive (on any of its interfaces) traffic sent from hosts attached to s3.

| s2 Flow Table (Example 3)           |            |
| Match                               | Action     |
| IP Src = 10.3.*.* IP Dst = 10.2.0.3 | Forward(3) |
| IP Src = 10.3.*.* IP Dst = 10.2.0.4 | Forward(4) |
| ...                                 | ...        |

If there were no other entries in s2’s flow table, then only traffic from
10.3.*.* would be forwarded to the hosts attached to s2.

* TODO Control Plane
** Protocolos de ruteo
*** Algoritmos de ruteo

determine good paths (equivalently,routes), from senders to receivers, through
the network of routers. Typically, a “good” path is one that has the least cost.

whether the network control plane adopts a per-router control approach or a
logically centralized approach, there must always be a well-defined sequence of
routers that a packet will cross in traveling from sending to receiving host.

A graph is used to formulate routing problems

Consideraremos a la red como un grafo G=(N,E)
+ N: Nodos del grafo (routers)
+ E: aristas que unen los nodos del grafo (links, subredes)

El grafo G, puede ser considerado como un grafo pesado (para la internet)

Qué significan los pesos de las aristas?
+ La distancia de los enlaces (mayor latencia)
+ La capacidad
+ El costo de la latencia

Actualización de la notación
+ c(x,y): costo de la arista entre el par de nodos x e y
+ Si no hay arista entre x e y —> c(x,y): infinito

Vocabulario
+ Si c(x,y) < infinito: x e y son vecinos


G=(N,E) grafo de la red

c(x,y) \forall x,y \in N costo entre dos nodos cualesquiera

\{x_1,x_2,\dots,x_k\} camino hasta el nodo x_k

\displaystyle^{k} c(x_i,x_j) costo de un camino


Objetivos
+ Hallar el camino de costo mínimo entre dos pares de nodos
+ También llamado el camino más corto (shortest path)

+ Únicamente basados en aspectos topológicos
+ No determinan los caminos en función de parámetros dinámicos (latencia,
  congestión)

¿Por qué?
+ Problemas de estabilidad, afectando todo internet

**** Algoritmo Centralizado (Link-state)
- El algoritmo cuenta con toda la información de la red (nodos, enlaces, status)
- No importa como consiguió esta información
- IMPORTANTE: Independiente si el CP es lógicamente centralizado o opera en cada
  router

A centralized routing algorithm computes the least-cost path between a source
and destination using complete, global knowledge about the network. That is,
the algorithm takes the connectivity between all nodes and all link costs as
inputs. This then requires that the algorithm somehow obtain this information
before actually performing the calculation. The calculation itself can be run
at one site or could be replicated in the routing component of each and every
router. The key distinguishing feature here, however, is that the algorithm has
complete information about connectivity and link costs. Algorithms with global
state information are often referred to as link-state (LS) algorithms, since
the algorithm must be aware of the cost of each link in the network.

¿Cómo se lleva a cabo?
- Cada router hace broadcast el estado de sus enlaces
- Cada router recibe el broadcast de todos los otros routers

Cada nodo (router) corre el algoritmo de ruteo

this is accomplished by having each node broadcast link-state packets to all
other nodes in the network, with each link-state packet containing the
identities and costs of its attached links.

shortest path algorithm [fn:1]

***** Oscilaciones

imagen 5.5

Before completing our discussion of the LS algorithm, let us consider a
pathology that can arise. Figure 5.5 shows a simple network topology where link
costs are equal to the load carried on the link, for example, reflecting the
delay that would be experienced. In this example, link costs are not symmetric;
that is, c(u, v) equals c(v, u) only if the load carried on both directions on
the link (u, v) is the same. In this example, node z originates a unit of
traffic destined for w, node x also originates a unit of traffic destined for w,
and node y injects an amount of traffic equal to e, also destined for w. The
initial routing is shown in Figure 5.5(a) with the link costs corresponding to
the amount of traffic carried.

When the LS algorithm is next run, node y determines (based on the link costs
shown in Figure 5.5(a)) that the clockwise path to w has a cost of 1, while the
counterclockwise path to w (which it had been using) has a cost of 1+e. Hence
y’s least-cost path to w is now clockwise. Similarly, x determines that its new
least-cost path to w is also clockwise, resulting in costs shown in Figure
5.5(b). When the LS algorithm is run next, nodes x, y, and z all detect a
zero-cost path to w in the counterclockwise direction, and all route their
traffic to the counterclockwise routes. The next time the LS algorithm is run,
x, y, and z all then route their traffic to the clockwise routes.

Another solution is to ensure that not all routers run the LS algorithm at the
same time.  This seems a more reasonable solution, since we would hope that even
if routers ran the LS algorithm with the same periodicity, the execution
instance of the algorithm would not be the same at each node.

Escenario
+ Topología rombo
+ c(x,y) := f(tráfico)
+ Trafico asimétrico (upstream != downstream)
+ Ramas con diferentes valores

Desync
Forzar que los routers no efectúen Dijkstra en simultáneo
- Distintos routers van a tener diferentes caminos mínimos instante a instante

**** Algoritmo Distribuido (Distance-vector)
- Calculado de manera iterativa y distribuida por los routers
- Información parcial (sólo se conoce a los vecinos)
- Entre vecinos se intercambia información (iterativamente) para reconstruir
  información global

+ Distribuido
each node receives some information from one or more of its directly attached
neighbors, performs a calculation, and then distributes the results of its
calculation back to its neighbors.

+ Iterativo
the process continues on until no more information is exchanged between
neighbors. (Interestingly, the algorithm is also self-terminating - there is no
signal that the computation should stop; it just stops.)

+ Asincrónico
- it does not require all of the nodes to operate in lockstep with each other.
- No necesita coordinación para envíos o cálculos
Bellman-ford equation
dx(y) = min_{v}\{c(x,v),dv(y)\} , \exists (x,v) \in E (v es vecino de x)
dx(y) : cost of least-cost path from x to y

The basic idea is as follows. Each node x begins with D (y), an estimate of the
cost of the least-cost path from itself to node y, for all nodes, y, in N. Let
Dx=[Dx(y):y \in N] be node x’s distance vector, which is the vector of cost
estimates from x to all other nodes, y, in N. With the DV algorithm, each node x
maintains the following routing information:
- For each neighbor v, the cost c(x, v) from x to directly attached neighbor, v
- Node x’s distance vector, that is, Dx=[Dx(y):y \in N], containing x’s estimate
  of its cost to all destinations, y, in N
- The distance vectors of each of its neighbors, that is, Dv=[Dv(y):y \in N] for
  each neighbor v of x

In the distributed, asynchronous algorithm, from time to time, each node sends a
copy of its distance vector to each of its neighbors. When a node x receives a
new distance vector from any of its neighbors w, it saves w’s distance vector,
and then uses the Bellman-Ford equation to update its own distance vector as
follows:

Dx(y) = min_{v}\{c(x,v)+Dv(y)\} for each node y in N

If node x’s distance vector has changed as a result of this update step, node x
will then send its updated distance vector to each of its neighbors, which can
in turn update their own distance vectors. Miraculously enough, as long as all
the nodes continue to exchange their distance vectors in an asynchronous
fashion, each cost estimate D (y) converges to d (y), the actual cost of the
least-cost path from node x to node y.

***** problemas
- Loops
- Convergencia lenta hasta alcanzar el equilibrio

Solucion:
- Si dz(x) utiliza a y, entonces z anuncia a y dz(x)=inf

**** LS vs DV
Velocidad de convergencia
+ LS ~inmediata (mensajes vía broadcast)
+ DV (muy) lento. (Cambios vía intermediarios + loops)

**** Open Shortest Path First (OSPF)
***** Autonomous System (AS)
a form of organization of routers that solves =scale= and =administrative
autonomy=, issues observed in practice when routers are viewed individually.

- Scale :: As the number of routers becomes large, the overhead involved in
  communicating, computing, and storing routing information becomes
  prohibitive. Today’s Internet consists of hundreds of millions of
  routers. Storing routing information for possible destinations at each of
  these routers would clearly require enormous amounts of memory. The overhead
  required to broadcast connectivity and link cost updates among all of the
  routers would be huge! A distance-vector algorithm that iterated among such a
  large number of routers would surely never converge. Clearly, something must
  be done to reduce the complexity of route computation in a network as large as
  the Internet.

- Administrative autonomy :: the Internet is a network of ISPs, with each ISP
  consisting of its own network of routers. An ISP generally desires to operate
  its network as it pleases (for example, to run whatever routing algorithm it
  chooses within its network) or to hide aspects of its network’s internal
  organization from the outside. Ideally, an organization should be able to
  operate and administer its network as it wishes, while still being able to
  connect its network to other outside networks.

a group of routers that are under the same administrative control.

An autonomous system is identified by its globally unique autonomous system
number (ASN) [RFC 1930].

AS numbers, like IP addresses, are assigned by ICANN regional registries.

Cada AS permite libertad para tomar decisiones de ruteo
- Protocolo de ruteo
- Broadcast

***** OSPF [RFC 2328]

Protocolo de ruteo interno (intra-AS)

OSPF is a link-state protocol that uses flooding of link-state information and a
Dijkstra’s least-cost path algorithm.

With OSPF, each router constructs a complete topological map of the entire
autonomous system (AS). Each router then locally runs Dijkstra’s shortest-path
algorithm to determine a shortest-path tree to all subnets, with itself as the
root node.

OSPF does not mandate a policy for how link weights are set.

With OSPF, a router broadcasts routing information to all other routers in the
autonomous system, not just to its neighboring routers. A router broadcasts
link-state information whenever there is a change in a link’s state (for
example, a change in cost or a change in up/down status). It also broadcasts a
link’s state periodically (at least once every 30 minutes), even if the link’s
state has not changed.

OSPF advertisements are contained in OSPF messages that are carried directly by
IP, with an upper-layer protocol of 89 for OSPF. Thus, the OSPF protocol must
itself implement functionality such as reliable message transfer and link-state
broadcast. The OSPF protocol also checks that links are operational (via a HELLO
message that is sent to an attached neighbor) and allows an OSPF router to
obtain a neighboring router's database of network-wide link state.

****** advantages

- Security :: Exchanges between OSPF routers (for example, link-state updates)
  can be authenticated. With authentication, only trusted routers can
  participate in the OSPF protocol within an AS, thus preventing malicious
  intruders from injecting incorrect information into router tables. By default,
  OSPF packets between routers are not authenticated and could be forged. Two
  types of authentication can be configured:
  - =simple authentication=, the same password is configured on each
    router. When a router sends an OSPF packet, it includes the password in
    plaintext. Clearly not very secure.
  - =MD5 authentication= is based on shared secret keys that are configured in
    all the routers. For each OSPF packet that it sends, the router computes
    the MD5 hash of the content of the OSPF packet appended with the secret
    key. Then the router includes the resulting hash value in the OSPF
    packet. The receiving router, using the preconfigured secret key, will
    compute an MD5 hash of the packet and compare it with the hash value that
    the packet carries, thus verifying the packet’s authenticity. Sequence
    numbers are also used with MD5 authentication to protect against replay
    attacks.

- Multiple same-cost paths :: When multiple paths to a destination have the same
  cost, OSPF allows multiple paths to be used (that is, a single path need not
  be chosen for carrying all traffic when multiple equal-cost paths exist).

- Integrated support for unicast and multicast routing :: Multicast OSPF (MOSPF)
  [RFC 1584] provides simple extensions to OSPF to provide for multicast
  routing. MOSPF uses the existing OSPF link database and adds a new type of
  link-state advertisement to the existing OSPF link-state broadcast mechanism.

- Support for hierarchy within a single AS :: An OSPF autonomous system can be
  configured hierarchically into areas. Each area runs its own OSPF link-state
  routing algorithm, with each router in an area broadcasting its link state to
  all other routers in that area. Within each area, one or more area border
  routers are responsible for routing packets outside the area. Lastly, exactly
  one OSPF-area in the AS is configured to be the backbone area. The primary
  role of the backbone area is to route traffic between the other areas in the
  AS. The backbone always contains all area border routers in the AS and may
  contain non-border routers as well. Inter-area routing within the AS requires
  that the packet be first routed to an area border router (intra-area routing),
  then routed through the backbone to the area border router that is in the
  destination area, and then routed to the final destination.

*** clasificacion de protocolos
**** estático vs dinámico
***** estático

+ Paths determinados por humanos
+ ¿Cuándo es útil?
  - Topologías que cambian poco frecuentemente
  - Topologías pequeñas

***** dinámico
+ Reconfiguración de path automático ante
  - Cambios de topología
  - Variables de calidad de la red
+ Problemas: pueden generar loops y oscilaciones

** Control Plane en SDNs
*** Elementos de una arq SDN
**** Flow table
Basada en múltiples headers de múltiples capas (L4, IP, Link)

**** Separacion de DP y CP
- DP: Memorias de acceso rápido donde se ejecuta m+a
- CP: Servers y SW remoto donde se calculan las FT

**** Funciones de red externas
- SDN controller
  - Control del estado de SW y enlaces
  - Envió de estados a las apps
  - comunicacion y escritura de SW
- Aplicaciones de red

**** APIs
Medios para que Apps puedan actuar sobre el funcionamiento de la red



[fn:1]

*** Cambios de arquitectura del Control Plane
**** pre sdn
1. Monolítica: CP y DP en el mismo dispositivo
2. Integración vertical: HW + OS@CP desarrollados por el fabricante
3. Inflexibilidad: Imposibilidad de alterar funciones preestablecidas por el
   fabricante en el CP

**** sdn
1. Diversificación de los servicios
2. SW & HW provistos por diferentes proveedores
3. Ecosistema similar al OSes @ PCs

*** Elementos del CP
**** Controlador SDN
***** Capa de comunicacion
Entre el controlador y los SW

Necesidad:
- Transmitir información entre ambos
- SW notifiquen eventos (links levantados o caidos)

Openflow

***** Control de estado de la red
- Acá se toman las decisiones (ej. Configurar las tablas)
- Necesita información en casi tiempo real del estado de toda la red
- Configura Forwarding Table de cada uno de los Switches
- Se mantiene un copia local de cada una de las Forwarding Tables distribuidas

***** Interfaz con las applicaciones
interaccion via apis

Consumidores
- Apps: Intervienen en la toma decisiones

Prestaciones
- Permite que =rw=? los estados de la red y/o las FTs

****

Implementación
- Lógicamente centralizado
- Hardware distribuido

Ejemplos
- SDN controller puede correr en un cluster de un datacenter
- Las App de control de red pueden ser remotas

Implementaciones propietarias
+ ONIX
+ Juniper -> Juniper Contrail
+ Google -> La usada en B4

Implementaciones de código abierto
+ OpenDaylight
+ ONOS
+ POX

**** Openflow

Opera entre el controlador SDN y un Switch SDN

OpenFlow: TCP:6653

***** Mensajes OpenFlow:
****** Controlador -> Switch
1. Configuración: Consultar o fijar parámetros en el SW
2. Modificar estado: Agregar o remover entradas en la FT
3. Leer estado: Acceder a las estadísticas del SW
4. Enviar paquete: Función: enviar un pckt al SW y fwd por un puerto específico

****** Switch -> controlador
1. Entrada removida: Notifica entrada de la FT fue removida (ACK de modificar
   entrada o expiró la entrada)
2. Port Status: Reporte de up/down de puertos o enlaces
3. Packet-in if match(pckt) == NULL: Se podría enviar al SW para que decida que
   hacer

**** Aplicaciones de control de red
***** Google B4
Google utiliza un SDNs para el manejo de su red

1.¿Dónde?
- Manejo de la red entre sus datacenters y sus PoPs @ (IXPs & ISPs)
2.¿Qué logra?
- Enlaces utilizados > 70%
3.¿Por qué es posible?
- Google controla toda la infra de punta a punta (DC, SW, enlaces y svrs)
- La cantidad de Datacenters en acción es reducida (~10)
4.¿Donde corre el Data Plane?
- En algún Datacenter de Google
5.¿Cómo se comunica el controlador con los SW?
- Opción out of band: Una red aparte

** asd
*** camino minimo
**** dijkstra

Paso #1: Elegimos un nodoSería el router desde donde se va a calcular el
algoritmo

Paso #2: Costo a los nodos vecinos

Paso #3:  Costo  de  los  vecinos  a sus vecinos

O(n^{2})

ver libro!




*** bellman-ford
ver libro

* Capa de Enlace - DOCSIS y ARP
6-6.1, 6.3-6.4.1

we naturally wonder how packets are sent across the individual links that make
up the end-to-end communication path.

- How are the network-layer datagrams encapsulated in the link-layer frames for
  transmission over a single link?
- Are different link-layer protocols used in the different links along the
  communication path?
- How are transmission conflicts in broadcast links resolved?
- Is there addressing at the link layer and, if so,
  - how does the link-layer addressing operate with the network-layer addressing?
- And what exactly is the difference between a switch and a router?

there are two fundamentally different types of link-layer channels.
1. broadcast channels, which connect multiple hosts in wireless LANs, satellite
   networks, and hybrid fiber-coaxial cable (HFC) access networks. Since many
   hosts are connected to the same broadcast communication channel, a so-called
   *medium access protocol* is needed to coordinate frame transmission. In some
   cases, a *central controller* may be used to coordinate transmissions; in
   other cases, the *hosts themselves* coordinate transmissions.
2. point-to-point communication link, such as that often found between two
   routers connected by a long-distance link, or between a user’s office
   computer and the nearby Ethernet switch to which it is connected.
   Coordinating access to a point-to-point link is simpler; the Point-to-Point
   Protocol (PPP) is used in settings ranging from dial-up service over a
   telephone line to high-speed point-to-point frame transport over fiber-optic
   links.

** Intro to the link layer

node: any device that runs a link-layer protocol

Nodes include hosts, routers, switches, and WiFi access points. We will also
refer to the communication channels that connect adjacent nodes along the
communication path as links

imagen 6.1

*** The Services Provided by the Link Layer

Although the basic service of any link layer is to move a datagram from one node
to an adjacent nodeover a single communication link, the details of the provided
service can vary from one link-layer protocol to the next.

**** Framing
Almost all link-layer protocols encapsulate each network-layer datagram within a
link-layer frame before transmission over the link. A frame consists of a data
field, in which the network-layer datagram is inserted, and a number of header
fields. The structure of the frame is specified by the link-layer protocol.
We’ll see several different frame formats when we examine specific link-layer
protocols in the second half of this chapter.

**** Link access
A medium access control (MAC) protocol specifies the rules by which a frame is
transmitted onto the link. For point-to-point links that have a single sender at
one end of the link and a single receiver at the other end of the link, the MAC
protocol is simple (or nonexistent)—the sender can send a frame whenever the
link is idle. The more interesting case is when multiple nodes share a single
broadcast link—the so-called multiple access problem. Here, the MAC protocol
serves to coordinate the frame transmissions of the many nodes.

**** Reliable delivery
When a link-layer protocol provides reliable delivery service, it guarantees to
move each network-layer datagram across the link without error. Recall that
certain transport-layer protocols (such as TCP) also provide a reliable delivery
service. Similar to a transport-layer reliable delivery service, a link-layer
reliable delivery service can be achieved with acknowledgments and
retransmissions. *A link-layer reliable delivery service is often used for links
that are prone to high error rates, such as a wireless link, with the goal of
correcting an error locally—on the link where the error occurs—rather than
forcing an end-to-end retransmission of the data by a transport- or
application-layer protocol*. However, link-layer reliable delivery can be
considered an unnecessary overhead for low bit-error links, including fiber,
coax, and many twisted-pair copper links. For this reason, many wired link-layer
protocols do not provide a reliable delivery service.

**** Error detection and correction
The link-layer hardware in a receiving node can incorrectly decide that a bit in
a frame is zero when it was transmitted as a one, and vice versa. Such bit
errors are introduced by signal attenuation and electromagnetic noise. Because
there is no need to forward a datagram that has an error, many link-layer
protocols provide a mechanism to detect such bit errors.

This is done by having the transmitting node include error-detection bits in the
frame, and having the receiving node perform an error check. Recall from
Chapters 3 and 4 that the Internet’s transport layer and network layer also
provide a limited form of error detection—the Internet checksum. Error detection
in the link layer is usually more sophisticated and is implemented in
hardware. Error correction is similar to error detection, except that a receiver
not only detects when bit errors have occurred in the frame but also determines
exactly where in the frame the errors have occurred (and then corrects these
errors).

*** Where Is the Link Layer Implemented?
for the most part, the link layer is implemented *in a network adapter*, also
sometimes known as a =network interface card (NIC)=. At the heart of the network
adapter is the link-layer controller, usually a single, special-purpose chip
that implements many of the link-layer services (framing, link access, error
detection, and so on). Thus, much of a link-layer controller’s functionality is
implemented in hardware.

imagen 6.2

On the sending side, the controller takes a datagram that has been created and
stored in host memory by the higher layers of the protocol stack, encapsulates
the datagram in a link-layer frame (filling in the frame’s various fields), and
then transmits the frame into the communication link, following the link-access
protocol.

On the receiving side, a controller receives the entire frame, and extracts the
network-layer datagram.

If the link layer performs error detection, then it is the sending controller
that sets the error-detection bits in the frame header and it is the receiving
controller that performs error detection

while most of the link layer is implemented in hardware, part of the link layer
is implemented in software that runs on the host’s CPU. The software components
of the link layer implement higher-level link-layer functionality such as
assembling link-layer addressing information and activating the controller
hardware.

On the receiving side, link-layer software responds to controller
interrupts (e.g., due to the receipt of one or more frames), handling error
conditions and passing a datagram up to the network layer. Thus, the link layer
is a combination of hardware and software—the place in the protocol stack where
software meets hardware.

** Multiple Access Links and Protocols

A =point-to-point link= consists of a single sender at one end of the link and
a single receiver at the other end of the link. Many link-layer protocols have
been designed for point-to-point links; the point-to-point protocol (PPP) and
high-level data link control (HDLC) are two such protocols.

a =broadcast link=, can have multiple sending and receiving nodes all connected
to the same, single, shared broadcast channel. The term broadcast is used here
because when any one node transmits a frame, the channel broadcasts the frame
and each of the other nodes receives a copy. Ethernet and wireless LANs are
examples of broadcast link-layer technologies.

- the =multiple access problem= :: how to coordinate the access of multiple
  sending and receiving nodes to a shared broadcast channel

  Computer networks have protocols — so-called multiple access protocols — by
  which nodes regulate their transmission into the shared broadcast channel.

  When multiple nodes transmit frames on a channel at the same time, all of the
  nodes receive multiple frames at the same time; that is, the transmitted frames
  =collide= at all of the receivers. Typically, when there is a collision, none of
  the receiving nodes can make any sense of any of the frames that were
  transmitted. Thus, all the frames involved in the collision are lost, and
  the broadcast channel is wasted during the collision interval.

  It is the multiple access protocol's job to coordinate the transmission of the
  active nodes.

  we can classify just about any multiple access protocol as belonging to one of
  three categories:
  - channel partitioning protocols,
  - random access protocols, and
  - taking-turns protocols.

  desirable characteristics of a multiple-access-protocol for a broadcast channel
  of rate $R$ bits per second:
  1. When only one node has data to send, that node has a throughput of R bps.
  2. When M nodes have data to send, each of these nodes has a throughput of R/M
     bps. This need not necessarily imply that each of the M nodes always has an
     instantaneous rate of R/M, but rather that each node should have an average
     transmission rate of R/M over some suitably defined interval of time.
  3. The protocol is decentralized; that is, there is no master node that
     represents a single point of failure for the network.
  4. The protocol is simple, so that it is inexpensive to implement.

*** Channel Partitioning Protocols

N: number of nodes on the channel.

R: transmission rate of the channel in bps.

**** Time Division Multiplexing (TDM)
TDM divides time into =time frames= and further divides each time frame into
N =time slots=.

Each time slot is then assigned to one of the N nodes.

Whenever a node has a packet to send, it transmits the packet’s bits during its
assigned time slot in the revolving TDM frame.

- Pros
  - Avoids collisions
  - Fair division of the bandwidth among the nodes
  - Each node gets a dedicated transmission rate of R/N bps during each frame
    time.
- Cons
  - a node is limited to an average rate of R/N bps even when it is the only
    node with packets to send
  - a node must always wait for its turn in the transmission sequence even when
    it is the only node with packets to send

**** Frequency Division Multiplexing (FDM)
FDM divides the R bps channel into different frequencies (each with a bandwidth
of R/N) and assigns each frequency to one of the N nodes.

FDM thus creates N smaller channels of R/N bps out of the single, larger R bps
channel.

- Pros
  - Avoids collisions
  - Fair division of the bandwidth among the nodes
  - Each node gets a dedicated transmission rate of R/N bps during each frame
    time.
- Cons
  - a node is limited to an average rate of R/N bps even when it is the only
    node with packets to send

**** Code Division Multiple Access (CDMA)
CDMA assigns a different code to each node.

Each node then uses its unique code to encode the data bits it sends. If the
codes are chosen carefully, CDMA networks have the property that different nodes
can transmit simultaneously and yet have their respective receivers correctly
receive a sender’s encoded data bits (assuming the receiver knows the sender’s
code) in spite of interfering transmissions by other nodes.

*** Random Access Protocols

a transmitting node always transmits at the full rate of the channel

When there is a collision, each node involved in the collision repeatedly
retransmits its frame (that is, packet) until its frame gets through without a
collision.

The node retransmits the frame after waiting for a random delay independently
from the other nodes and hopefully it will be able to send the frame into the
channel without a collision.

**** Slotted ALOHA

Assume the following:
- All frames consist of exactly L bits.
- Time is divided into slots of size L/R seconds (that is, a slot equals the
  time to transmit one frame).
- Nodes start to transmit frames only at the beginnings of slots.
- The nodes are synchronized so that each node knows when the slots begin.
- If two or more frames collide in a slot, then all the nodes detect the
  collision event before the slot ends.

Operation:
- When the node has a fresh frame to send, it waits until the beginning of the
  next slot and transmits the entire frame in the slot.
- If there isn’t a collision, the node has successfully transmitted its frame
  and thus need not consider retransmitting the frame. (The node can prepare a
  new frame for transmission, if it has one.)
- If there is a collision, the node detects the collision before the end of the
  slot. The node retransmits its frame in each subsequent slot with probability
  $p$ until the frame is transmitted without a collision.

allows a node to transmit continuously at the full rate, R, when that node is
the only active node. (A node is said to be active if it has frames to send.)

highly decentralized, because each node detects collisions and independently
decides when to retransmit.

however,it requires the slots to be synchronized in the nodes

Slotted ALOHA works well when there is only one active node, but how ­efficient
is it when there are multiple active nodes?

imagen 6.10

1. when there are multiple active nodes, a certain fraction of the slots will
   have collisions and will therefore be “wasted.”
2. another fraction of the slots will be empty because all active nodes refrain
   from transmitting as a result of the probabilistic transmission policy.

the only “unwasted” slots will be those in which exactly one node transmits. A
slot in which exactly one node transmits is said to be a =successful slot=. The
efficiency of a slotted multiple access protocol is defined to be the long-run
fraction of successful slots in the case when there are a large number of active
nodes, each always having a large number of frames to send.

the maximum efficiency of the protocol is given by p=1/e=0,37.

when a large number of nodes have many frames to transmit,then (at best) only 37
percent of the slots do useful work. Thus the effective transmission rate of
the channel is not R bps but only 0.37 R bps!

**** ALOHA

The first ALOHA protocol was unslotted and fully decentralized.

when a frame first arrives (that is, a network-layer datagram is passed down
from the network layer at the sending node), the node immediately transmits the
frame in its entirety into the broadcast channel.

Si se detecta una colision, inmediatamente luego de transmitir el frame
colisionado, se retransmite el frame con una probabilidad $p$. Si no se detecta
colision, se transmite el proximo frame con probabilidad $p$.

siempre se espera a que se termine de transmitir el frame.

imagen 6.11

the maximum efficiency of the protocol is given by p=1/(2e).

La mitad de eficiencia que el protocolo slotted ALOHA. Esto es porque es
totalmente descentralizado.

**** Carrier Sense Multiple Access (CSMA)

- Listen before speaking :: If someone else is speaking, wait until they are
  finished. In the networking world, this is called =carrier sensing= —a node
  listens to the channel before transmitting. If a frame from another node is
  currently being transmitted into the channel, a node then waits until it
  detects no transmissions for a short amount of time and then begins
  transmission.

- If someone else begins talking at the same time, stop talking :: In the
  networking world, this is called =collision detection= —a transmitting node
  listens to the channel while it is transmitting. If it detects that another
  node is transmitting an interfering frame, it stops transmitting and waits a
  random amount of time before repeating the sense-and-transmit-when-idle
  cycle.

  imagen 6.12

From Figure 6.12, it is evident that the end-to-end =channel propagation delay=
of a broadcast channel—the time it takes for asignal to propagate from one of
the nodes to another—will play a crucial role in determining its
performance. The longer this propagation delay, the larger the chance that a
carrier-sensing node is not yet able to sense a transmission that has already
begun at another node in the network.

**** Carrier Sense Multiple Access with Collision Detection (CSMA/CD)

When a node performs collision detection, it ceases transmission as soon as it
detects a collision.

imagen 6.13

1. The adapter obtains a datagram from the network layer, prepares a link-layer
   frame, and puts the frame adapter buffer.
2. If the adapter senses that the channel is idle (that is, there is no signal
   energy entering the adapter from the channel), it starts to transmit the
   frame. If, on the other hand, the adapter senses that the channel is busy, it
   waits until it senses no signal energy and then starts to transmit the frame.
3. While transmitting, the adapter monitors for the presence of signal energy
   coming from other adapters using the broadcast channel.
4. If the adapter transmits the entire frame without detecting signal energy
   from other adapters, the adapter is finished with the frame. If, on the other
   hand, the adapter detects signal energy from other adapters while
   transmitting, it aborts the transmission (that is, it stops transmitting its
   frame).
5. After aborting, the adapter waits a random amount of time and then returns to
   step 2.

Como se elije el tiempo de espera aleatorio?

- =binary exponential backoff= algorithm :: when transmitting a frame that has
  already experienced $n$ collisions, a node chooses the value of $K$ at random
  from $\{0,1,2,...,2^{n-1}\}$

example

We define the =efficiency of CSMA/CD= to be the long-run fraction of time during
which frames are being transmitted on the channel without collisions when there
is a large number of active nodes, with each node having a large number of
frames to send.

let $d_{prop}$ denote the maximum time it takes signal energy to propagate
between any two adapters.

Let $d_{trans}$ be the time to transmit a maximum-size frame

$$Efficiency = 11 + 5 * \frac{d_{prop}}{d_{trans}}$$

*** Taking Turn Protocols
**** Polling Protocol

The polling protocol requires one of the nodes to be designated as a master
node. The master node polls each of the nodes in a round-robin fashion.

In particular, the master node first sends a message to node 1, saying that it
can transmit up to some maximum number of frames. After node 1 transmits some
frames, the master node tells node 2 it can transmit up to the maximum number of
frames. (The master node can determine when a node has finished sending its
frames by observing the lack of a signal on the channel.)

The procedure continues in this manner, with the master node polling each of the
nodes in a cyclic manner.

- Cons:
  - polling delay: amount of time required to notify a node that it can
    transmit.
  - if the master node fails, the channel becomes inioperative.

**** Token-passing protocol

A small, special-purpose frame known as a =token= is exchanged among the
nodes in some fixed order. When a node receives a token, it holds onto the
token only if it has some frames to transmit; otherwise, it immediately
forwards the token to the next node. If a node does have frames to transmit
when it receives the token, it sends up to a maximum number of frames and
then forwards the token to the next node. Token passing is decentralized and
highly efficient. But for example, the failure of one node can crash the
entire channel. Or if a node accidentally neglects to release the token,
then some recovery procedure must be invoked to get the token back in
circulation.

*** Data-Over-Cable Service Interface Specifications (DOCSIS)

Recall that a cable access network typically connects several thousand
residential cable modems to a cable modem termination system (CMTS) at the
cable network headend.

DOCSIS uses FDM to divide the downstream (CMTS to modem) and upstream (modem
to CMTS) network segments into multiple frequency channels.

Each downstream channel is 6 MHz wide, with a maximum throughput of approximately
40 Mbps per channel;

Each upstream channel has a maximum channel width of 6.4 MHz, and a
maximum upstream throughput of approximately 30 Mbps.

Each channel (upstream and downstream) is a broadcast channel.

Frames transmitted on the downstream channel by the CMTS are received by all
cable modems receiving that channel; since there is just a single CMTS
transmitting into the downstream channel, there is no multiple access
problem.

In the upstream direction, since multiple cable modems share the same
upstream channel (frequency) to the CMTS, collisions can potentially occur.

imagen 6.14

each upstream channel is divided into intervals of time (TDM-like), each
containing a sequence of mini-slots during which cable modems can transmit to
the CMTS. The CMTS explicitly grants permission to individual cable modems to
transmit during specific mini-slots by sending a
control message known as a =MAP message= on a downstream channel to specify
which cable modem (with data to send) can transmit during which mini-slot for
the interval of time specified in the control message.

mini-slots are allocated to cable modems, so there are no colliding
transmissions during a mini-slot.

to know which cable modems have data to send (and therefore, be assigned a
mini-slot), the cable modems send mini-slot-request frames to the CMTS during
a special set of interval mini-slots that are dedicated for this purpose.

These mini-slot-request frames are transmitted in a random access manner and
so may collide with each other. A cable modem can neither sense whether the
upstream channel is busy nor detect collisions. Instead, the cable modem
infers that its mini-slot-request frame experienced a collision if it does
not receive a response to the requested allocation in the next downstream
control message. When a collision is inferred, a cable modem uses binary
exponential backoff to defer the retransmission of its mini-slot-request
frame to a future time slot. When there is little traffic on the upstream
channel, a cable modem may actually transmit data frames during slots
nominally assigned for mini-slot-request frames (and thus avoid having to
wait for a mini-slot assignment).

** Switched Local Area Networks

imagen 6.15

Figure 6.15 shows a switched local network connecting three departments, two
servers and a router with four switches. Because these switches operate at the
link layer, they switch link-layer frames (rather than network-layer
datagrams), don’t recognize network-layer addresses, and don’t use routing
algorithms to determine paths through the network of layer-2 switches. Instead
of using IP addresses, we will soon see that they use link-layer addresses to
forward link-layer frames through the network of switches.

*** Link-Layer Addressing and ARP
**** Mac Addresses

hosts and routers have adapters (network interfaces) which have a link-layer
address.

host or router with multiple network interfaces will thus have multiple
link-layer addresses associated with it, just as it would also have multiple
IP addresses associated with it. however, link-layer switches do not have
link-layer addresses associated with their interfaces that connect to hosts
and routers. This is because the job of the link-layer switch is to carry
datagrams between hosts and routers; a switch does this job transparently,
that is, without the host or router having to explicitly address the frame
to the intervening switch.

these addresses are called =LAN addresses=, =physical addresses= or =MAC
addresses=, and are of 6 bytes long. They can be changed via software.

no two adapters have the same address, because the ieee manages the mac
address space. an adapter manufacturer can buy an address space of 2^{24}
addresses, having an available space to manufacture 2^{24} adapters.

An adapter’s MAC address has a flat structure (as opposed to a hierarchical
structure) and doesn’t change no matter where the adapter goes.

When an adapter wants to send a frame to some destination adapter, the
sending adapter inserts the destination adapter’s MAC address into the frame
and then sends the frame into the LAN.  As we will soon see, a switch
occasionally broadcasts an incoming frame onto all of its interfaces.
because frames are broadcasted to all adapters, an adapter has to check if
a frame is addressed to it.  However, sometimes a sending adapter does want
all the other adapters on the LAN to receive and process the frame it is
about to send. In this case, the sending adapter inserts a special =MAC
broadcast address= into the destination address field of the frame.

**** Address Resolution Protocol (ARP)

imagen 6.17

each host and router has a single IP address and single MAC address

assume in this section that the switch broadcasts all frames; that is,
whenever a switch receives a frame on one interface, it forwards the frame
on all of its other interfaces.

uppose that the host with IP address 222.222.222.220 wants to send an IP
datagram to host 222.222.222.222. In this example, both the source and
destination are in the same subnet.

To send a datagram, the source must give its adapter not only the IP
datagram but also the MAC address for destination 222.222.222.222. The
sending adapter will then construct a link-layer frame containing the
destination’s MAC address and send the frame into the LAN.

But how does the sending host, know the MAC address of destination host?

The solution is ARP. An ARP module in the sending host takes any IP address
on the same LAN as input, and returns the corresponding MAC address.

ARP resolves an IP address to a MAC address for hosts and router interfaces
in the same subnet.

Each host and router has an =ARP table= in its memory, which contains
mappings of IP addresses to MAC addresses.

imagen 6.18
|      IP Address | MAC Address       |      TTL |
| 222.222.222.221 | 88-B2-2F-54-1A-0F | 13:45:00 |
| 222.222.222.223 | 5C-66-AB-90-75-B1 | 13:52:00 |

The ARP table also contains a time-to-live (TTL) value, which indicates when
each mapping will be deleted from the table. Note that a table does not
necessarily contain an entry for every host and router on the subnet; some
may have never been entered into the table, and others may have expired.

if the ARP table doesn’t currently have an entry for the destination, the
sender uses the ARP protocol to resolve the address. First, the sender
constructs a special packet called an =ARP packet=. An ARP packet has
several fields, including the sending and receiving IP and MAC
addresses. Both ARP query and response packets have the same format. The
purpose of the ARP query packet is to query all the other hosts and routers
on the subnet to determine the MAC address corresponding to the IP address
that is being resolved.

the query ARP message is sent within a broadcast frame, whereas the response
ARP message is sent within a standard frame.

ARP is plug-and-play; that is, an ARP table gets built automatically—it
doesn’t have to be configured by a system administrator. And if a host
becomes disconnected from the subnet, its entry is eventually deleted from
the other ARP tables in the subnet.

ARP is a protocol that arguably lies in between the link layer and the
network layer.

**** Sending a datagram off the subnet

imagen 6.19

Each host has exactly one IP address and one adapter.

Each router has an IP address for each of its interfaces. For each router
interface there is also an ARP module (in the router) and an adapter.

Because the router in Figure 6.19 has two interfaces, it has two IP addresses,
two ARP modules, and two adapters. Of course, each adapter in the network has
its own MAC address

El adaptador del nodo que envia, debe completar en el frame de capa de enlace,
la direccion MAC de el router que conecta a las subredes. Esto se conoce por
ARP. Una vez que el datagrama llega al router, este lo acepta porque esta
destinado para el mismo. El router inspecciona el paquete y observa que el
paquete (de capa de red) IP esta destinado a una IP de la otra subred. El router
inspecciona en la tabla ARP para obtener la direccion MAC del nodo receptor, y
+como direccion MAC de origen del nuevo datagrama+ reescribe la
direccion-MAC-origen, colocando la direccion MAC del adaptador conectado a la
subred destino
* Capa de Enlace - LAN Ethernet
** Switched LANs 524
*** Ethernet
**** Frame Ethernet
imagen 6.20

| Preamble | Destination Address | Source Address | Type | Data | CRC |

- Data field (46 to 1,500 bytes) :: This field carries the IP datagram. The
  maximum transmission unit (MTU) of Ethernet is 1,500 bytes. This means that if
  the IP datagram exceeds 1,500 bytes, then the host has to fragment the
  datagram. The minimum size of the data field is
  46 bytes. This means that if the IP datagram is less than 46 bytes, the data
  field has to be “stuffed” to fill it out to 46 bytes. When stuffing is used,
  the data passed to the network layer contains the stuffing as well as an IP
  datagram. The network layer uses the length field in the IP datagram header to
  remove the stuffing.

- Destination address (6 bytes) :: This field contains the MAC address of the
  destination adapter. When the destination adapter receives an Ethernet frame
  whose destination address is either their own address or the MAC broadcast
  address, it passes the contents of the frame’s data field to the network
  layer; if it receives a frame with any other MAC address, it discards the
  frame.

- Source address (6 bytes) :: This field contains the MAC address of the adapter
  that transmits the frame onto the LAN.

- Type field (2 bytes) :: The type field permits Ethernet to multiplex
  network-layer protocols. To understand this, we need to keep in mind that
  hosts can use other network-layer protocols besides IP. In fact, a given host
  may support multiple network-layer protocols using different protocols for
  different applications. For this reason, when the Ethernet frame arrives at
  destination adapter, the dest adapter needs to know to which network-layer
  protocol it should pass (demultiplex) the contents of the data field. IP and
  other network-layer protocols (for example, Novell IPX or AppleTalk) each have
  their own, standardized type number. Furthermore, the ARP protocol has its own
  type number, and if the arriving frame contains an ARP packet , the ARP packet
  will be demultiplexed up to the ARP protocol. Note that the type field is
  analogous to the protocol field in the network-layer datagram and the
  port-number fields in the transport-layer segment; all of these fields serve
  to glue a protocol at one layer to aprotocol at the layer above.

- Cyclic redundancy check (CRC) (4 bytes) :: the purpose of the CRC field is to
  allow the receiving adapter, to detect bit errors in the frame. When a frame
  fails the CRC check, the receiving adapter simply discards the frame.

- Preamble (8 bytes) :: The Ethernet frame begins with an 8-byte preamble
  field. Each of the first 7 bytes of the preamble has a value of 10101010; the
  last byte is 10101011. The first 7 bytes of the preamble serve to “wake up”
  the receiving adapters and to synchronize their clocks to that of the sender’s
  clock. Why should the clocks be out of synchronization? Keep in mind that the
  sender adapter aims to transmit the frame at 10 Mbps, 100 Mbps, or 1 Gbps,
  depending on the type of Ethernet LAN. However, because nothing is absolutely
  perfect, the sender adapter will not transmit the frame at exactly the target
  rate; there will always be some drift from the target rate, a drift which is
  not known a priori by the other adapters on the LAN. A receiving adapter can
  lock onto the sender adapter’s clock simply by locking onto the bits in the
  first 7 bytes of the preamble. The last 2 bits of the eighth byte of the
  preamble (the first two consecutive 1s) alert the receiving adapter that the
  “important stuff” is about to come.

All of the Ethernet technologies provide connectionless service to the network
layer. (no handshake)

**** Tecnologias Ethernet 535

The IEEE 802.3 CSMA/CD (Ethernet) working group has standardized multiple
Ethernet technologies.

For example: 10BASE-T, 10BASE-2, 100BASE-T, 1000BASE-LX, 10GBASE-T and 40GBASE-T

The first part refers to the speed of the standard.

“BASE” refers to baseband Ethernet, meaning that the physical media only carries
Ethernet traffic;

The final part of the acronym refers to the physical media itself; Ethernet is
both a link-layer and a physical-layer specification and is carried over a
variety of physical media including coaxial cable, copper wire, and
fiber. Generally, a “T” refers to twisted-pair copper wires.

```
habla de Mb ehter y Gb ether
```

Con el uso de switches en ethernet (topologia estrella), +debido al accionar de
los switches (store and forward)+, la necesidad de tener CSMA/CD fue dejando de
tener importancia.

*** Switches de capa de enlace

el rol del switch es recibir frames de capa de enlace entrantes y
redireccionarlos en enlaces salientes.

el switch en si es =transparente= para los hosts y routers en la subred, esto
es, los hosts y routers envian frames a otros hosts y routers, en vez de
hacerlo a un switch.

la tasa de arribos a una interfaz de salida de un switch pueden exceder a la
capacidad del enlace de dicha interfaz. Para resolver esto, se utilizan
buffers en las interfaces de salida de los switches, justo como los routers
tienen buffers para los datagramas.

**** Forwarding y Filtering

=Filtering= is the switch function that determines whether a frame should be
forwarded to some interface or should just be dropped.

=Forwarding= is the switch function that determines the interfaces to which a
frame should be directed, and then moves the frame to those interfaces.

Switch filtering and forwarding are done with a switch table. The switch
table contains entries for some, but not necessarily all, of the hosts and
routers on a LAN.

An entry in the switch table contains:
1) a MAC address,
2) the switch interface that leads toward that MAC address, and
3) the time at which the entry was placed in the table.

imagen 6.22

modern packet switches can be configured to forward on the basis of layer-2
destination MAC addresses (i.e., function as a layer-2 switch) or layer-3 IP
destination addresses (i.e., function as a layer-3 router).

Traditional switch tables (non-SDN context) are constructed differently from
a routers forwarding tables

when a frame arrives at the switch, its destination MAC address is indexed in
the switch table where:
- There is no entry in the table for the destination MAC. In this case, the
  switch forwards copies of the frame to the output buffers preceding /all/
  interfaces address, the switch broadcasts the frame.
- There is an entry but the frame is coming from a LAN segment that contains
  adapter with that MAC address. There being no need to forward the frame to
  any of the other interfaces, the switch performs the filtering function by
  discarding the frame.
- There is an entry in the table but this time the asociated interface to the
  MAC is different from the LAN segment where the MAC came from. In this
  case, the frame needs to be forwarded to the LAN segment attached to said
  interface. The switch performs its forwarding function by putting the frame
  in an output buffer that precedes that interface.

**** self learning

the switch table is built automatically, dynamically, and autonomously — without
any intervention from a network administrator or from a configuration protocol.

1. The switch table is initially empty.
2. For each incoming frame received on an interface, the switch stores in
   its table
   1) the MAC address in the frame’s source address field,
   2) the interface from which the frame arrived, and
   3) the current time.
   In this manner the switch records in its table the LAN segment on which
   the sender resides. If every host in the LAN eventually sends a frame,
   then every host will eventually get recorded in the table.
3. The switch deletes an address in the table if no frames are received with
   that address as the source address after some period of time (the aging
   time).

Switches are =plug-and-play= devices because they require no intervention
from a network administrator or user. A network administrator wanting to
install a switch need do nothing more than connect the LAN segments to the
switch interfaces. The administrator need not configure the switch tables at
the time of installation or when a host is removed from one of the LAN
segments. Switches are also full-duplex, meaning any switch interface can
send and receive at the same time.

**** Propiedades

Ventajas por sobre enlaces de broadcast (buses o hubs):
- Elimination of collisions :: In a LAN built from switches (and without
  hubs), there is no wasted bandwidth due to collisions! The switches buffer
  frames and never transmit more than one frame on a segment at any one
  time. As with a router, the maximum aggregate throughput of a switch is the
  sum of all the switch interface rates.
- Heterogeneous links :: Because a switch isolates one link from another, the
  different links in the LAN can operate at different speeds and can run over
  different media. A switch is ideal for mixing legacy equipment with new
  equipment.
- Management :: In addition to providing enhanced security, a switch also
  eases network management. For example, if an adapter malfunctions and
  continually sends Ethernet frames (called a jabbering adapter), a switch
  can detect the problem and internally disconnect the malfunctioning
  adapter. Similarly, a cable cut disconnects only that host that was using
  the cut cable to connect to the switch. In the days of coaxial cable, many
  a network manager spent hours “walking the line” (or more accurately,
  “crawling the floor”) to find the cable break that brought down the entire
  network. Switches also gather statistics on bandwidth usage, collision
  rates, and traffic types, and make this information available to the
  network manager. This information can be used to debug and correct problems,
  and to plan how the LAN should evolve in the future.

**** SNIFFING A SWITCHED LAN: SWITCH POISONING
When a host is connected to a switch, it typically only receives frames that are
intended for it. For example, consider a switched LAN in Figure 6.17. When host
A sends a frame to host B, and there is an entry for host B in the switch table,
then the switch will forward the frame only to host B. If host C happens to be
running a sniffer, host C will not be able to sniff this A-to-B frame. Thus, in
a switched-LAN environment (in contrast to a broadcast link environment such as
802.11 LANs or hub–based Ethernet LANs), it is more difficult for an attacker to
sniff frames. However, because the switch broadcasts frames that have
destination addresses that are not in the switch table, the sniffer at C can
still sniff some frames that are not intended for C. Furthermore, a sniffer will
be able sniff all Ethernet broadcast frames with broadcast destination address
FF–FF–FF–FF–FF–FF. A well-known attack against a switch, called switch
poisoning, is to send tons of packets to the switch with many different bogus
source MAC addresses, thereby filling the switch table with bogus entries and
leaving no room for the MAC addresses of the legitimate hosts. This causes the
switch to broadcast most frames, which can then be picked up by the sniffer
[Skoudis 2006]. As this attack is rather involved even for a sophisticated
attacker, switches are significantly less vulnerable to sniffing than are hubs
and wireless LANs.

**** Switches vs Routers 541

routers are store-and-forward packet switches that forward packets using
network-layer addresses. Although a switch is also a store-and-forward packet
switch, it is fundamentally different from a router in that it forwards packets
using MAC addresses

imagen 6.15 ... ?

imagen 6.24

***** Switches
****** Ventajas
- plug and play
- relatively high filtering and forwarding rates

****** Desventajas
- limits network topology to a tree, otherwise, broadcast frames could cycle
  infinitely. (frames do not have ttl)
- a large network requires a large ARP table in hosts and routers

***** Routers
****** Ventajas
- does not limit the topology. because of ttl of the packet and the addressing
  is hierarchical
- provide firewall protection against broadcast stomrs

****** Desventajas
- not plug and play. needs IP address to be configured
- packet-processing-time is larger for routers than for switches

***** .

|                   | Hubs | Routers | Switches |
| Traffic isolation | No   | Yes     | Yes      |
| Plug and play     | Yes  | No      | Yes      |
| Optimal routing   | No   | Yes     | No       |

*** VLAN

imagen 6.15

desventajas encontradas en la configuracion de la imagen
- Lack of traffic isolation :: Although the hierarchy localizes group traffic to
  within a single switch, broadcast traffic must still traverse the entire
  institutional network. Limiting the scope of such broadcast traffic would
  improve LAN performance. Perhaps more importantly, it also may be desirable to
  limit LAN broadcast traffic for security/privacy reasons. This type of
  isolation could be provided by replacing the center switch in Figure 6.15 with
  a router. We’ll see shortly that this isolation also can be achieved via a
  layer-2-switch solution.
- Inefficient use of switches :: If instead of three groups, the institution had
  10 groups, then 10 first-level switches would be required. If each group were
  small, say less than 10 people, then a single 96-port switch would likely be
  large enough to accommodate everyone, but this single switch would not provide
  traffic isolation.
- Managing users :: If an employee moves between groups, the physical cabling
  must be changed to connect the employee to a different switch in Figure
  6.15. Employees belonging to two groups make the problem even harder

Fortunately, each of these difficulties can be handled by a switch that supports
=virtual local area networks (VLANs)=.

a switch that supports VLANs allows multiple virtual lans to be defined over a
single physical lan infrastructure. Hosts within a VLAN communicate with each
other as if they (and no other hosts) were connected to the switch. In a
=port-based= VLAN, the switch’s ports (interfaces) are divided into groups by
the network manager. Each group constitutes a VLAN, with the ports in each VLAN
forming a broadcast domain (i.e., broadcast traffic from one port can only reach
other ports in the group).

Figure 6.25 shows a single switch with 16 ports. Ports 2 to 8 belong to the EE
VLAN, while ports 9 to 15 belong to the CS VLAN (ports 1 and 16 are unassigned).
This VLAN solves all of the difficulties noted above — EE and CS VLAN frames are
isolated from each other, the two switches in Figure 6.15 have been replaced by
a single switch, and if the user at switch port 8 joins the CS Department, the
network operator simply reconfigures the VLAN software so that port 8 is now
associated with the CS VLAN. One can easily imagine how the VLAN switch is
configured and operates — the network manager declares a port to belong to a
given VLAN (with undeclared ports belonging to a default VLAN) using switch
management software, a table of port-to-VLAN mappings is maintained within the
switch; and switch hardware only delivers frames between ports belonging to the
same VLAN.

imagen 6.25

How can traffic from the EE Department be sent to the CS Department? One way to
handle this would be to connect a VLAN switch port (e.g., port 1 in Figure 6.25)
to an external router and configure that port to belong both the EE and CS
VLANs. In this case, even though the EE and CS departments share the same
physical switch, the logical configuration would look as if the EE and CS
departments had separate switches connected via a router. An IP datagram going
from the EE to the CS department would first cross the EE VLAN to reach the
router and then be forwarded by the router back over the CS VLAN to the CS host.


Suppose now that members of EE VLAN and CS VLAN are in a different building, but
want to be connected to their respective department VLAN. A way to interconnect
VLAN switches is known as =VLAN Trunking= (imagen 6.26 b).

imagen 6.26

A special port on each switch (port 16 on the left switch and port 1 on the
right switch) is configured as a =trunk port= to interconnect the two VLAN
switches. The trunk port belongs to all VLANs, and frames sent to any VLAN are
forwarded over the =trunk link= to the other switch.

(Este puerto especial es para hacer broadcast a todos los switches. Luego el
switch que recibe se encarga de hacer broadcast a los puertos de la VLAN
correspondiente)

How does a switch know that a frame arriving on a trunk port belongs to a
particular VLAN? The IEEE has defined an extended Ethernet frame format,
802.1Q, for frames crossing a VLAN trunk.

imagen 6.27

the 802.1Q frame consists of the standard Ethernet frame with a four-byte =VLAN
tag= added into the header that carries the identity of the VLAN to which the
frame belongs. The VLAN tag is added into a frame by the switch at the sending
side of a VLAN trunk, parsed, and removed by the switch at the receiving side of
the trunk. The VLAN tag consists of
- a 2-byte Tag Protocol Identifier (TPID) field (with a fixed hexadecimal value
  of 81-00),
- a 2-byte Tag Control Information field that contains a 12-bit VLAN identifier
  field, and
- a 3-bit priority field that is similar in intent to the IP datagram TOS field.


Tambien existen la VLANs basadas en MAC. El administrador de red, configura un
conjunto de direcciones MAC que pertenecen a una VLAN.

Otra forma de definir VLANs es basada en protocolos de capa de red (IPv4, IPv6,
Appletalk) y otros criterios.

** Virtualizacion de enlaces 548
*** Multiprotocol Label Switching (MPLS) [RFC 3031, RFC 3032]

The goal was not to abandon the destination-based IP datagram-forwarding
infrastructure for one based on fixed-length labels and virtual circuits, but to
augment it by selectively labeling datagrams and allowing routers to forward
datagrams based on fixed-length labels (rather than destination IP addresses)
when possible. Importantly, these techniques work hand-in-hand with IP, using IP
addressing and routing. The IETF unified these efforts in the MPLS protocol,
effectively blending VC techniques into a routed datagram network.

imagen 6.28

La imagen muestra un frame que es transmitido entre dispositivos que soportan
MPLS. Contiene un header MPLS entre los headers de capa 2 y 3 (Ethernet e IP
respectivamente).

Dentro del header MPLS se encuetra:
- el label
- 3 bits para uso experimental
- 1 bit "S" para indicar el fin de una serie de header MPLS "apilados"
- campo ttl

Un router que soporta MPLS se lo llama =label-switched router= ya que envia
frames MPLS con solo observar el label MPLS y buscarlo en la forwarding table.
Es decir, no extrae la IP de destino y busca el LPM en la forwarding table.

Como sabe el router que su vecino soporta MPLS? Como asocia un label con una
direccion IP?

imagen 6.29

- routers R1 through R4 are MPLS capable.
- R5 and R6 are standard IP routers.
- R1 has advertised to R2 and R3 that it (R1) can route to destination A, with
  MPLS label 6.
- Router R3 has advertised to router R4 that it can route to destinations A and
  D, with labels 10 and 12 respectively.
- Router R2 has also advertised to router R4 that it (R2) can reach destination
  A, with a MPLS label 8.

Note that router R4 has two MPLS paths to reach A.

R5, R6, A y D, estan conectados mediante la infraestructura MPLS, tanto como una
switched LAN y red de ATMs que conectan dispositivos IP, y tanto como estos, los
routers que soportan MPLS lo hacen sin tocar el header IP de un paquete.

En el libro no se ve el protocolo utilizado por los routers MPLS para distribuir
los labels.

ver [RFC 3468] [RFC 3209].

tampoco se especifica como se calculan los caminos por los cuales los paquetes
deben fluir dentro de la red de routers MPLS, ni tampoco como se recolecta la
informacion de los enlaces en dichos caminos.

Lo mas importante de MPLS es la capacidad de manejo de trafico que permite
realizar. El protocolo IP especificaria un solo camino de costo minimo por el
cual un paquete debe seguir, pero MPLS provee varias rutas. Tambien se puede
usar MPLS para restaurar caminos de forma mas rapida en caso de haber fallas de
enlace.

tambien se usan para implementar VPNs y separar el trafico de clientes de VPN y
clientes regulares.

** Netwroking de Data Centers 552

Each data center has its own =data center network= that interconnects its hosts
with each other and interconnects the data center with the Internet.

The worker bees in a data center are the hosts: They serve content (e.g., Web
pages and videos), store e-mails and documents, and collectively perform
massively distributed computations (e.g., distributed index computations for
search engines). The hosts in data centers, called =blades=, are generally
commodity hosts that include CPU, memory, and disk storage. The hosts are
stacked in racks, with each rack typically having 20 to 40 blades.

At the top of each rack there is a switch, named the =Top of Rack (TOR) switch=,
that interconnects the hosts in the rack with each other and with other switches
in the data center.

Each host is also assigned its own data-center-internal IP address.

two types of traffic:
1. traffic flowing between external clients and internal hosts the data center
   network includes one or more =border routers= to handle this type of
   traffic. The data center network therefore interconnects the racks with each
   other and connects the racks to the border routers.
2. traffic flowing between internal hosts.

   imagen 6.30

*** Balance de Carga

Los data centers proveen varias aplicaciones de forma concurrente. para
soportar multiples pedidos de clientes externos, cada aplicacion esta
asociada a una IP publica. dentro del data center, las consultas se redirigen
a un =load balancer= para que distribuya las consultas entre los
hosts. Tambien se lo llama =layer 4 switch= porque puede tomar desiciones de
forwardeo basado en el puerto destino (capa 4) y tambien por la ip de destino
del paquete (capa 3). Luego de que el host procese la consulta, se la envia
de vuelta al load balancer, para que este se lo devuelva al cliente.

el load balancer tambien funciona como NAT, al traducir las ip publicas a
direcciones ip internas

*** Arquitectura Jerarquica

imagen 6.30

At the top of the hierarchy, the border router connects to access routers. Below
each access router there are three tiers of switches. Each access router
connects to a top-tier switch, and each top-tier switch connects to multiple
second-tier switches and a load balancer. Each second-tier switch in turn
connects to multiple racks via the racks’ TOR switches (third-tier
switches). All links typically use Ethernet for their link-layer and
physical-layer protocols, with a mix of copper and fiber cabling.

Como es necesario para aplicaciones en la nube proveer servicios de forma
continua, los data centers tambien incluyen equipo y enlaces redundantes. For
example, each TOR switch can connect to two tier-2 switches, and each access
router, tier-1 switch, and tier-2 switch can be duplicated and integrated into
the design.

observe that the hosts below each access router form a single subnet. In order
to localize ARP broadcast traffic, each of these subnets is further partitioned
into smaller VLAN subnets.

La arquitectura de la imagen resuelve el problema de escalabilidad, pero sufre
de limitacion en capacidad entre hosts (host-to-host capacity). Puede haber
multiples conexiones simultaneas entre hosts de un lado de la red hacia el otro,
pero la velocidad del enlace de los switches y routers es limitante. Una
solucion es obtener switches y routers de mayor velocidad, pero esto es mas
costoso.

*** Tendencias

Para reducir costos y mejorar el delay y throughput, se crean nuevas
arquitecturas y protocolos de red.

**** Fully Connected Topology
Un metodo es reemplazar la jerarquia de switches y routers por una =fully
connected topology=.

imagen 6.31

cada switch de tier 1 se conecta con todos los switches de tier 2 para:
1. que el trafico host-to-host no suba por mas arriba que el tier de switches
2. con $n$ switches de tier 1, hay $n$ caminos disjuntos entre 2 switches de
   tier 2 cualesquiera.

**** Modular Data Center (MDC)

Se crean pequeños data centers dentro de contenedores de 12 metros y se envian a
la ubicacion del data center, en la cual se interconectan los contenedores entre
si y la internet. Una vez desplegado, un contenedor es dificil de mantener por
lo que estan diseñados para operar con menor performance a medida que se van
degradando.

con este metodo, hay dos tipos de redes:
1. las redes dentro de los contenedores y
2. la red que conecta a cada contenedor

la dificultad se encuetra en el 2do tipo de red, ya que hay problemas para
proveer un alto ancho de banda para conexiones host-to-host.


en topologias altamente interconectadas, el diseño de algoritmos de ruteo entre
switches se vuelve un problema. soluciones: (1) random routing, (2) multiples
NICs en cada host y conectar cada interfaz a un switch de bajo costo y dejar que
los hosts resulevan el ruteo del trafico entre los switches.

* Redes Moviles
** DONE introduccion

imagen 7.1

Elementos en una red inalambrica:
- Hosts inalambricos :: son dispositivos end-system que ejecutan
  aplicaciones. puede ser una laptop, tablet, smartphone, pc. pueden no ser
  mobiles en si.
- Enlaces inalambricos :: un host se conecta a una =base station= o a otro host
  inalambrico mediante un =enlace de comunicacion inalambrico=. Different
  wireless link technologies have different transmission rates and can transmit
  over different distances.
- Base station :: no tiene equivalente en redes alambricas. es responsable de
  enviar y recibir datos (paquetes) desde y hacia un host inalambrico que esta
  asociado a esa base station. tambien es responsable de coordinar la
  transmicion de multiples hosts inalambricos a los cuales esta asociada, que
  significa que:
  1) el host esta dentro del rango de comunicacion de la base station
  2) el host utiliza dicha base station para entregar datos entre el host y el
     resto de la red.

  unos ejemplos de base stations son: torres de celulares y access points en
  802.11.

  los hosts asociados a una base station, se les dice que operan en =modo
  infraestructura=, ya que los servicios tradicionales (ej: asignacion de
  direccion y ruteo) son provistos por la red a la cual la base station esta
  conectada.

  En redes =ad hoc=, los hosts inalambricos no tienen tal infraestructura a la
  cual conectarse. en este caso, los hosts mismos son responsables de proveer
  estos servicios, tales como routeo, asignacion de direcciones, traduccion de
  nombre DNS, etc.

  cuando un host inalambrico se mueve del rango de una base station y entra en el
  rango de otra, cambia el punto de ingreso a la red, proceso conocido como
  =handoff=.
- Infraestructura de red :: red mas grande a la cual un host inalambrico desea
  conectarse.


PUBLIC WIFI ACCESS: COMING SOON TO A LAMP POST NEAR YOU?


clasificacion de redes inalambricas:
- si un paquete en la red realiza exactamente un solo salto o multiples saltos
- si existe infraestructura (como una base station) en la red

- un salto, con infraestructura :: redes con una base station conectada a una
  red alambrica mas grande (ej: internet). toda comunicacion es entre la base
  station y el host inalambrico (sin intermediarios). Ej: aula, cafe,
  biblioteca, red celular 4G LTE.
- un salto, sin infraestructura :: no hay base statinos, sin embargo un nodo
  puede coordinar las transmiciones de otros nodos. Ej: redes bluetooth y redes
  802.11 en modo ad hoc
- multiples saltos, con infraestructura :: la base station esta conectada al
  resto de la red, pero algunos nodos pueden llegar a depender de otros nodos
  inalambricos intermediarios para poder comunicarse a la base station. Ej:
  redes de sensores inalambricos, =wireless mesh networks=.
- multiples saltos, sin infraestructura :: no hay base station y los nodos deben
  depender de otros para poder llegar al destino. Los nodos pueden ser mobiles,
  por lo que la conectividad puede cambiar, un tipo de redes conocido como
  =mobile ad hoc networks (MANETs)=.

** TODO Enlaces inalambricos y caracteristicas de la red

Diferencias entre enlaces alambricos e inalambricos:
- Decresiente intensidad de señal :: la radiacion electromagnetica se atenua
  cuando pasa a traves de materia e incluso al solamente propagarse en el vacio.
- Interferencia con otras fuentes :: fuentes de transmicion en la misma
  frecuencia interfieren unas con otras.
- Propagacion Multipath :: ocurre cuando porciones de la onda electromagnetica
  se reflejan sobre ojetos y el suelo (tierra?), tomando caminos de distancias
  diferentes entre el que envia y el que recibe. esto provoca que se reciba una
  señal distorsionada.

Por estas caracteristicas mencionadas, los errores son mas comunes en enlaces
inalambricos que en los alambricos.

Nos enfocamos ahora en como un host recibe una señal electromagnetica. esta
señal es una combinacion de la señal original transmitida por el emisor y ruido
del ambiente. se utiliza la relacion =señal a ruido= (=signal-to-noise ratio
(SNR)=) para medir las intensidades relativas.

imagen 7.3

imagen 7.4

bit error rate (BER) , probabilidad de que un bit transmitido es recibido con
error

- For a given modulation scheme, the higher the SNR, the lower the BER :: Since
  a sender can increase the SNR by increasing its transmission power, a sender
  can decrease the probability that a frame is received in error by increasing
  its transmission power. Note, however, that there is arguably little practical
  gain in increasing the power beyond a certain threshold, say to decrease the
  BER from to . There are also disadvantages associated with increasing the
  transmission power: More energy must be expended by the sender (an important
  concern for battery-powered mobile users), and the sender’s transmissions are
  more likely to interfere with the transmissions of another sender (see Figure
  7.4(b)).
- For a given SNR, a modulation technique with a higher bit transmission rate (whether in error or not) will have a higher BER :: For
  example, in Figure 7.3, with an SNR of 10 dB, BPSK modulation with a
  transmission rate of 1 Mbps has a BER of less than , while with QAM16
  modulation with a transmission rate of 4 Mbps, the BER is , far too high to be
  practically useful. However, with an SNR of 20 dB, QAM16 modulation has a
  transmission rate of 4 Mbps and a BER of , while BPSK modulation has a
  transmission rate of only 1 Mbps and a BER that is so low as to be (literally)
  “off the charts.” If one can tolerate a BER of , the higher transmission rate
  offered by QAM16 would make it the preferred modulation technique in this
  situation. These considerations give rise to the final characteristic,
  described next.
- Dynamic selection of the physical-layer modulation technique can be used to adapt the modulation technique to channel conditions :: The
  SNR (and hence the BER) may change as a result of mobility or due to changes
  in the environment. Adaptive modulation and coding are used in cellular data
  systems and in the 802.11 WiFi and 4G cellular data networks . This allows,
  for example, the selection of a modulation technique that provides the highest
  transmission rate possible subject to a constraint on the BER, for given
  channel characteristics.

A higher and time-varying bit error rate is not the only difference between a
wired and wireless link.


en enlaces inalambricos, no todos los nodos pueden recibir transmiciones de
otros nodos.

Suppose that Station A is transmitting to Station B. Suppose also
that Station C is transmitting to Station B.

1. se llama =hidden terminal problem= cuando obstrucciones fisicas en el medio
   (montaña, eficio), no permiten que A y C escuchen las transmiciones del otro,
   a pesar de que sus señales se interfieren en el destino B. (7.4 a)
2. otro escenario que resulta en que el enlace no sea detectable ocurre cuando
   la señal que se progapa, se ve atenuada debido a la distancia entre los
   nodos. esto se conoce como =fading= (atenuacion) (7.4 b)

*** TODO Code Division Multiple Access (CDMA) :protocolo de acceso al medio:

pertenece a la familia de protocolos de particion de canal (entre acceso
aleatorio y por turnos).

en CDMA cada bit enviado esta codificado al multiplicar el bit por la señal (el
codigo) que cambia a una velocidad mucho mas grande (=chipping rate=) que la
secuencia de bits original

imagen 7.5

con la presencia de emisores que interfieren entre si, CDMA permite poder
discernir una señal codificada de otra.

CDMA asume que las señales transmitidas son aditivas, esto es, por ejemplo, si 3
emisores envian un 1, y cuarto emisor envia -1 durante el mismo mini-slot,
entoces la señal recibida en todos los receptores en dicho mini-slot es 2
(1+1+1-1=2).

...

** DONE WiFi: 802.11 Wireless LANs

existen diferentes estandares dentro de la familia del protocolo 802.11 (Wifi)

utilizan la misma estructura la el frame de capa de enlace, pueden reducir la
velocidad de transmicion para extender el alcance, son compatibles con versiones
anteriores.

sin embargo tienen diferencias en la capa fisica. operan en dos rangos de
frecuencias 2.4-2.485 GHz y 5.1-5.8 GHz.

| Standard | Frequency range   | data range      |
|----------+-------------------+-----------------|
| 802.11b  | 2.4 GHz           | up to 11 Mbps   |
| 802.11a  | 5 GHz             | up to 54 Mbps   |
| 802.11g  | 2.4 GHz           | up to 54 Mbps   |
| 802.11n  | 2.5 GHz and 5 GHz | up to 450 Mbps  |
| 802.11ac | 5 GHz             | up to 1300 Mbps |

*** Architecture

una pieza fundamental es el =basic service set (BSS)=. un bss contiene una o mas
estaciones inalambricas y una base station central, conocido como =access point
(AP)=.

imagen 7.7

una lan inalambrica con un AP se la llama tambien =infrastructure wireless lan=,
con infrastructure refiriendose al AP y su conexion con la internet.

la imagen 7.8 muestra una red ad hoc, una red si un control central ni
conexiones con el exterior.

**** Canales y Asociacion

en el protocolo 802.11 cada estacion inalambrica debe estar asociada a un AP
antes de enviar o recibir datos de capa de red.

cuando se instala un AP, se asigna un =Service Set Identifier (SSID)= al
AP. tambien se debe asignar un numero de canal al AP. dentro del rango de
frecuencia 2.4 GHz, se definen 11 canales parcialmente solapados. Dos canales no
estan solapados si y solo si estan separados por 4 o mas canales.


Una =Wifi jungle= es una ubicacion fisica en donde una estacion inalambrica
recibe señales suficientemente fuertes de dos o mas APs. para poder unirse una
subred del AP, una estacion debe =asociarse= al AP. se debe crear un enlace
virtual.


el protocolo 802.11 requiere que los AP envien =beacon frames= de forma
periodica que contienen el SSID y MAC del AP. El dispositivo escanea los 11
canales buscando beacon frames.

el proceso de escanear canales y escuchar por beacon frames se conoce como
=passive scanning=

imagen 7.9 a

Un dispositivo inalambrico tambien puede realizar =active scanning=, al hacer
broadcast de un =probe frame= que es recibido por todos los APs dentro del rango
del dispositivo. Los APs responden al frame y el dispositivo elige entre uno de
ellos.

imagen 7.9 b

Luego de seleccionar el AP al cual asociarse, el dispositivo envia al AP un
=asociation request frame= y el AP le responde. (parecido a DHCP)

Luego, Tipicamente para unirse a la subred del AP, el dispositivo envia un
mensaje DHCP.

En algunos casos se quiere que el dispositivo se autenticado por el AP. esto se
puede realizar mediante:
- validacion de MAC
- usuario y contraseña

en ambos casos el AP puede comunicarse con un servidor autenticador separado,
para poder proveer a varios APs simultaneamente, centralizando la informacion
sensible, matener la complejidad y costo del AP bajo.

*** Protocolo MAC 802.11

como protocolo de acceso al medio compartido, los diseñadores de 802.11 optaron
por uno de tipo de acceso aleatorio llamado =Carrier Sense Multiple Access
(CSMA) with Collision avoidance (CSMA/CA)=. Cada estacion escucha el enlace
antes de transmitir y espera mientras este ocupado.

diferencias entre protocolos MAC de Ethernet y 802.11:
1. ethernet utiliza =collision detection=, 802.11 utiliza =collision avoidance=
2. Wifi utiliza un esquema de =acknowledgement/retransmission (ARQ)= de capa de
   enlace.


ethernet utiliza =collision detection=, lo que significa que un dispositivo
transmite y escucha el canal al mismo tiempo. si escucha ruido, entonces para de
transmitir y espera un tiempo aleatorio para volver a hacerlo.
no se utiliza =collision detection= de ethernet porque:
- la señal recibida es en general mas atenuada con respecto a la señal de
  transmicion, por que hacer que el dispositivo haga ambos al mismo tiempo es
  mas costoso.
- se podria no detectar una colision debido a =hidden terminal problem= y
  =fading=.


en wifi, una vez que se empieza a transmitir un frame, se hace por completo.

**** esquema de acknowledgement de capa de enlace
debido a que un frame puede no llegar intacto a su destino, se utilizan
acknowledgement a nivel de capa de enlace. Cuando se recibe un frame que pasa
CRC (Cyclic Redundancy Check), se espera una cierta cantidad de tiempo =Short
Inter-frame Spacing (SIFS)= y luego responde con un acknowledgement

si la estacion que transmite no recibe un acuse de recibo dentro de una cierta
cantidad de tiempo, asume que ocurrio un error y retransmite el frame utilizando
CSMA/CA. Si no recibe acuse de recibo luego de retransmitir varias veces, se
descarta el frame.

**** CSMA/CA

1. si el canal esta libre, se transmite al frame luego de un periodo de tiempo
   corto conocido como =Distributed Inter-frame Space (DIFS)=.
2. en otro caso, se elige un tiempo aleatorio de espera utilizando binary
   exponential backoff. espera a que es canal este libre, espera DIFS, espera el
   tiempo aleatorio determinado. si se detecta señal en el canal, se congela el
   timer.
3. cuando el tiempo de espera llega a cero, se retransmite el frame por completo
   y espera por un acuse de recibo.
4. si se tiene otro frame para transmitir, empieza CSMA/CA desde el paso 2. si
   no se recibe acknowledgement, se vuelve al paso 2 con bin-exp-backoff mas
   grande.

por que se espera un tiempo mas luego de detectar a que el canal este libre?
si se transmite al momento de detectar al canal libre, se puede generar una
colision.
debido a que wifi no detecta colision y luego aborta la transmicion, un frame
que sufre de colision seria transmitido por completo.

*** Hidden terminals: RTS y CTS

existe un esquema opcional que ayuda a prevenir colisiones incluso con la presencia de
hidden terminals.

imagen 7.11

si ambos hosts de la imagen transmiten al mismo tiempo, ocurre una colision en
la cercania del AP y ninguno de los hosts se da cuenta.

para evitar el problema, se usan los frames de control =Request to Send (RTS)= y
=Clear to Send (CTS)= para reservar el acceso al canal. si se quiere transmitir,
se envia un RTS al AP indicando el tiempo requierido para transmitir sus datos y
un frame de acuse de recibo (?). Luego el AP recibe el RTS y responde con un CTS
explicitando que el dispositivo puede transmitir y los demas deben esperar por
la duracion que pidio el transmisor.

imagen 7.12

la performance puede mejorar en dos formas:
1. se mitiga el problema del hidden terminal, ya que los frames largos son
   transmitidos luego de que el canal sea reservado.
2. porque los frames de control son cortos, la duracion de la colision de frames
   RTS y CTS es corta

a pesar de ayudar a reducir colisiones, puede introducir delays y consume
recursos del canal. es por esto que en general se utiliza RTS/CTS para el envio
de frames largos.

*** 802.11 como enlace punto a punto

si dos nodos cuentan con antenas direccionales, pueden apuntarse entre si y
formar un enlace punto a punto.
  