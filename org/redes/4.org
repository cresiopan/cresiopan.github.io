#+TITLE: Capa de Transporte, UDP y Entrega confiable

It has the critical role of providing communication services directly to the
application processes running on different hosts.

extending the network layer’s delivery service between two end systems to a
delivery service between two application-layer processes running on the end
systems.

* Introduccion y Servicios de Capa de Transporte

A transport-layer protocol provides for ~logical communication~ between
application processes running on different hosts. By logical communication, we
mean that from an application’s perspective, it is as if the hosts running the
processes were directly connected


Application processes use the logical communication provided by the transport
layer to send messages to each other, free from the worry of the details of the
physical infrastructure used to carry these messages.

imagen 3.1

As shown in Figure 3.1, transport-layer protocols are implemented in the end
systems but not in network routers. On the sending side, the transport layer
converts the application-layer messages it receives from a sending application
process into transport-layer packets, known as transport-layer ~segments~

** Relacion entre las capas de transporte y red

network-layer protocol provides logical-communication between hosts. This
distinction is subtle but important.

** Overview of the Transport Layer in the Internet

Recall that the Internet makes two distinct transport-layer protocols available
to the application layer.

One of these protocols is ~UDP~ (User Datagram Protocol), which provides an
unreliable, connectionless service to the invoking application.

The second of these protocols is ~TCP~ (Transmission Control Protocol), which
provides a reliable, connection-oriented service to the invoking application.
When


The most fundamental responsibility of UDP and TCP is to extend
IP’s delivery service between two end systems to a delivery service between two
processes running on the end systems. Extending host-to-host delivery to
process-to-process delivery is called ~transport-layer multiplexing~ and
~demultiplexing~.

UDP and TCP also provide integrity checking by including error-detection fields
in their segments’ headers. These two minimal transport-layer
services—process-to-process data delivery and error checking—are the only two
services that UDP provides! In particular, like IP, UDP is an unreliable
service—it does not guarantee that data sent by one process will arrive intact
(or at all!) to the destination process.

TCP, offers several additional services to applications. First and foremost, it
provides ~reliable data transfer~. Using flow control, sequence numbers,
acknowledgments, and timers, TCP ensures that data is delivered from sending
process to receiving process, correctly and in order. TCP thus converts IP’s
unreliable service between end systems into a reliable data transport service
between processes.

TCP also provides ~congestion control~. ~Congestion control~ is not so much a
service provided to the invoking application as it is a service for the Internet
as a whole, a service for the general good. Loosely speaking, TCP congestion
control prevents any one TCP connection from swamping the links and routers
between communicating hosts with an excessive amount of traffic. TCP strives to
give each connection traversing a congested link an equal share of the link
bandwidth. This is done by regulating the rate at which the sending sides of TCP
connections can send traffic into the network. UDP traffic, on the other hand,
is unregulated. An application using UDP transport can send at any rate it
pleases, for as long as it pleases.

* Multiplexing and Demultiplexing

In this section, we discuss transport-layer multiplexing and demultiplexing,
that is, extending the host-to- host delivery service provided by the network
layer to a process-to-process delivery service for applications running on the
hosts.

imagen 3.2

Recall that application processes communite to the network through sockets.

a receiving host directs an incoming transport-layer segment to the appropriate
socket. Each transport-layer segment has a set of fields in the segment for this
purpose. At the receiving end, the transport layer examines these fields to
identify the receiving socket and then directs the segment to that socket. This
job of delivering the data in a transport-layer segment to the correct socket is
called ~demultiplexing~.

The job of gathering data chunks at the source host from different sockets,
encapsulating each data chunk with header information (that will later be used
in demultiplexing) to create segments, and passing the segments to the network
layer is called ~multiplexing~.

we know that transport-layer multiplexing requires (1) that sockets have unique
identifiers, and (2) that each segment have special fields that indicate the
socket to which the segment is to be delivered. These special fields, are the
~source port number field~ and the ~destination port number field~.

imagen 3.3

Each port number is a 16-bit number, ranging from 0 to 65535.

The port numbers ranging from 0 to 1023 are called well-known port numbers and
are restricted, which means that they are reserved for use by well-known
application protocols such as HTTP (which uses port number 80) and FTP (which
uses port number 21).

RFC 1700

***  Connectionless Multiplexing and Demultiplexing

Typically, the client side of the application lets the transport layer
automatically (and transparently) assign the port number, whereas the server
side of the application assigns a specific port number.

With port numbers assigned to UDP sockets, we can now precisely describe UDP
multiplexing/demultiplexing. Suppose a process in Host A, with UDP port 19157, wants to send a chunk
of application data to a process with UDP port 46428 in Host B. The transport layer in Host A creates a
transport-layer segment that includes the application data, the source port number (19157), the
destination port number (46428), and two other values (which will be discussed later, but are
unimportant for the current discussion). The transport layer then passes the resulting segment to the
network layer. The network layer encapsulates the segment in an IP datagram and makes a best-effort
attempt to deliver the segment to the receiving host. If the segment arrives at the receiving Host B, the
transport layer at the receiving host examines the destination port number in the segment (46428) and
delivers the segment to its socket identified by port 46428. Note that Host B could be running multiple
processes, each with its own UDP socket and associated port number. As UDP segments arrive from
the network, Host B directs (demultiplexes) each segment to the appropriate socket by examining the
segment’s destination port number.

It is important to note that a UDP socket is fully identified by a two-tuple consisting of a destination IP
address and a destination port number. As a consequence, if two UDP segments have different source
IP addresses and/or source port numbers, but have the same destination IP address and destination
port number, then the two segments will be directed to the same destination process via the same
destination socket.

You may be wondering now, what is the purpose of the source port number? As shown in Figure 3.4, in
the A-to-B segment the source port number serves as part of a “return address”—when B wants to send
a segment back to A, the destination port in the B-to-A segment will take its value from the source port
value of the A-to-B segment. (The complete return address is A’s IP address and the source port
number.)

imagen 3.4

*** Connection-Oriented Multiplexing and Demultiplexing

In order to understand TCP demultiplexing, we have to take a close look at TCP
sockets and TCP connection establishment. One subtle difference between a TCP
socket and a UDP socket is that a TCP socket is identified by a four-tuple:
(source IP address, source port number, destination IP address, destination port
number). Thus, when a TCP segment arrives from the network to a host, the host
uses all four values to direct (demultiplex) the segment to the appropriate
socket.

In particular, and in contrast with UDP, two arriving TCP segments with
different source IP addresses or source port numbers will (with the exception of
a TCP segment carrying the original connection- establishment request) be
directed to two different sockets. To gain further insight, let’s reconsider the
TCP client-server programming example in Section 2.7.2:

- The TCP server application has a “welcoming socket,” that waits for
  connection-establishment requests from TCP clients on port number 12000.
- The TCP client creates a socket and sends a connection establishment request
  segment.
- A connection-establishment request is nothing more than a TCP segment with
  destination port number 12000 and a special connection-establishment bit set
  in the TCP header. The segment also includes a source port number that was
  chosen by the client.
- When the host operating system of the computer running the server process
  receives the incoming connection-request segment with destination port 12000,
  it locates the server process that is waiting to accept a connection on port
  number 12000.
- Also, the transport layer at the server notes the following four values in the
  connection-request segment: (1) the source port number in the segment, (2) the
  IP address of the source host, (3) the destination port number in the segment,
  and (4) its own IP address. The newly created connection socket is identified
  by these four values; all subsequently arriving segments whose source port,
  source IP address, destination port, and destination IP address match these
  four values will be demultiplexed to this socket. With the TCP connection now
  in place, the client and server can now send data to each other.

  imagen 3.5

* Transporte sin conexion: UDP

suppose you were interested in designing a no-frills, bare-bones
transport protocol. How might you go about doing this?

At the very least, the transport layer has to provide a multiplexing/demultiplexing service in order to pass
data between the network layer and the correct application-level process.

UDP, defined in [RFC 768], does just about as little as a transport protocol can
do. Aside from the multiplexing/demultiplexing function and some light error
checking, it adds nothing to IP. In fact, if the application developer chooses
UDP instead of TCP, then the application is almost directly talking with IP. UDP
takes messages from the application process, attaches source and destination
port number fields for the multiplexing/demultiplexing service, adds two other
small fields, and passes the resulting segment to the network layer. The network
layer encapsulates the transport-layer segment into an IP datagram and then
makes a best-effort attempt to deliver the segment to the receiving host. If the
segment arrives at the receiving host, UDP uses the destination port number to
deliver the segment’s data to the correct application process. Note that with
UDP there is no handshaking between sending and receiving transport-layer
entities before sending a segment. For this reason, UDP is said to be
~connectionless~.

DNS is an example of an application-layer protocol that typically uses UDP.

some applications are better suited for UDP for the following reasons:

- Finer application-level control over what data is sent, and when :: Under UDP,
  as soon as an application process passes data to UDP, UDP will package the
  data inside a UDP segment and immediately pass the segment to the network
  layer. TCP, on the other hand, has a congestion- control mechanism that
  throttles the transport-layer TCP sender when one or more links between the
  source and destination hosts become excessively congested. TCP will also
  continue to resend a segment until the receipt of the segment has been
  acknowledged by the destination, regardless of how long reliable delivery
  takes. Since real-time applications often require a minimum sending rate, do
  not want to overly delay segment transmission, and can tolerate some data
  loss, TCP’s service model is not particularly well matched to these
  applications’ needs. As discussed below, these applications can use UDP and
  implement, as part of the application, any additional functionality that is
  needed beyond UDP’s no-frills segment-delivery service.
- No connection establishment :: As we’ll discuss later, TCP uses a three-way
  handshake before it starts to transfer data. UDP just blasts away without any
  formal preliminaries. Thus UDP does not introduce any delay to establish a
  connection. This is probably the principal reason why DNS runs over UDP rather
  than TCP—DNS would be much slower if it ran over TCP. HTTP uses TCP rather
  than UDP, since reliability is critical for Web pages with text. But, as we
  briefly discussed in Section 2.2, the TCP connection-establishment delay in
  HTTP is an important contributor to the delays associated with downloading Web
  documents. Indeed, the QUIC protocol (Quick UDP Internet Connection, [Iyengar
  2015]), used in Google’s Chrome browser, uses UDP as its underlying transport
  protocol and implements reliability in an application-layer protocol on top of
  UDP.
- No connection state :: TCP maintains connection state in the end systems. This
  connection state includes receive and send buffers, congestion-control
  parameters, and sequence and acknowledgment number parameters. We will see in
  Section 3.5 that this state information is needed to implement TCP’s reliable
  data transfer service and to provide congestion control. UDP, on the other
  hand, does not maintain connection state and does not track any of these
  parameters. For this reason, a server devoted to a particular application can
  typically support many more active clients when the application runs over UDP
  rather than TCP.
- Small packet header overhead :: The TCP segment has 20 bytes of header
  overhead in every segment, whereas UDP has only 8 bytes of overhead.

  imagen 3.6

** UDP Segment Structure

imagen 3.7

The UDP segment structure, shown in Figure 3.7, is defined in RFC 768. The
application data occupies the data field of the UDP segment.

For example, for DNS, the data field contains either a query message or a
response message. For a streaming audio application, audio samples fill the data
field. The UDP header has only four fields, each consisting of two bytes.

The length field specifies the number of bytes in the UDP segment (header plus
data). An explicit length value is needed since the size of the data field may
differ from one UDP segment to the next.

The checksum is used by the receiving host to check whether errors have been
introduced into the segment. In truth, the checksum is also calculated over a
few of the fields in the IP header in addition to the UDP segment.

The length field specifies the length of the UDP segment, including the header,
in bytes.

** UDP Checksum

The UDP checksum provides for error detection. That is, the checksum is used to
determine whether bits within the UDP segment have been altered (for example, by
noise in the links or while stored in a router) as it moved from source to destination.

UDP at the sender side performs the 1s complement of the sum of all the 16-bit
words in the segment, with any overflow encountered during the sum being wrapped
around. This result is put in the checksum field of the UDP segment.

You may wonder why UDP provides a checksum in the first place, as many
link-layer protocols (including the popular Ethernet protocol) also provide
error checking. The reason is that there is no guarantee that all the links
between source and destination provide error checking; that is, one of the links
may use a link-layer protocol that does not provide error checking. Furthermore,
even if segments are correctly transferred across a link, it’s possible that bit
errors could be introduced when a segment is stored in a router’s memory. Given
that neither link-by-link reliability nor in-memory error detection is
guaranteed, UDP must provide error detection at the transport layer, on an
end-end basis, if the end- end data transfer service is to provide error
detection.

Because IP is supposed to run over just about any layer-2 protocol, it is useful
for the transport layer to provide error checking as a safety measure. Although
UDP provides error checking, it does not do anything to recover from an error.

* Principles of Reliable Data Transfer
With a reliable channel, no transferred data bits are corrupted (flipped from 0
to 1, or vice versa) or lost, and all are delivered in the order in which they
were sent. This is precisely the service model offered by TCP to the Internet
applications that invoke it.

It is the responsibility of a ~reliable data transfer protocol~ to implement
this service abstraction. This task is made difficult by the fact that the layer
below the reliable data transfer protocol may be unreliable.

imagen 3.8

rdt : reliable data transfer

In this section we consider only the case of unidirectional data transfer, that
is, data transfer from the sending to the receiving side. The case of reliable
bidirectional (that is, full-duplex) data transfer is conceptually no more
difficult but considerably more tedious to explain.

In addition to exchanging packets containing the data to be transferred, the
sending and receiving sides of rdt will also need to exchange control packets
back and forth. Both the send and receive sides of rdt send packets to the other
side by a call to udt_send() (where udt stands for unerliable data transfer).

** Building a Reliable Data Transfer Protocol

*** Reliable Data Transfer over a Perfectly Reliable Channel: rdt1.0
We first consider the simplest case, in which the underlying channel is completely reliable.

Two ~finite-state machines (FSM)~ define the operations of an rdt1.0 sender and an rdt1.0 receiver.

imagen 3.9

Each FSM has one state

The sending side of rdt simply accepts data from the upper layer via the
~rdt_send(data)~ event, creates a packet containing the data (via the action
~make_pkt(data)~) and sends the packet into the channel. In practice, the
~rdt_send(data)~ event would result from a procedure call (for example, to
~rdt_send()~) by the upper-layer application.

On the receiving side, rdt receives a packet from the underlying channel via the
~rdt_rcv(packet)~ event, removes the data from the packet (via the action
~extract (packet, data)~ ) and passes the data up to the upper layer (via the
action ~deliver_data(data)~ ). In practice, the ~rdt_rcv(packet)~ event would
result from a procedure call (for example, to ~rdt_rcv()~) from the lower-layer
protocol.

In this simple protocol, there is no difference between a unit of data and a
packet. Also, all packet flow is from the sender to receiver; with a perfectly
reliable channel there is no need for the receiver side to provide any feedback
to the sender since nothing can go wrong! Note that we have also assumed that
the receiver is able to receive data as fast as the sender happens to send data.
Thus, there is no need for the receiver to ask the sender to slow down.

*** Reliable Data Transfer over a Channel with Bit Errors: rdt2.0
A more realistic model of the underlying channel is one in which bits in a
packet may be corrupted.

We’ll continue to assume for the moment that all transmitted packets are
received (although their bits may be corrupted) in the order in which they were
sent.

**** ARQ (Automatic Repeat reQuest) protocols
Reliable data transfer protocols based on retransmissions depending on the
control messages sent.

The types of control messages are
- positive acknowledgments
- negative acknowledgments

These control messages allow the *reciever* to let the sender know what has been
received correctly, and what has been received in error and thus requires
repeating.

three additional protocol capabilities are required in ARQ protocols to handle
the presence of bit errors:
- Error detection :: a mechanism that allows the receiver to detect when bit
  errors have occurred. Recall from the previous section that UDP uses the
  Internet checksum field for exactly this purpose. For now, we need only know
  that these techniques require that extra bits (beyond the bits of original
  data to be transferred) be sent from the sender to the receiver; these bits
  will be gathered into the packet checksum field of the rdt2.0 data packet.
- Receiver feedback :: the only way for the sender to know whether or not a
  packet was received correctly, is for the receiver to provide explicit
  feedback to the sender. The positive (~ACK~) and negative (~NAK~)
  acknowledgment replies in the message-dictation scenario are examples of such
  feedback. Our rdt2.0 protocol will similarly send ACK and NAK packets back
  from the receiver to the sender. In principle, these packets need only be one
  bit long; for example, a 0 value could indicate a NAK and a value of 1 could
  indicate an ACK.
- Retransmission :: A packet that is received in error at the receiver will be
  retransmitted by the sender.

imagen 3.10

The send side of rdt2.0 has two states.

- In the leftmost state, the send-side protocol is waiting for data to be passed
  down from the upper layer. When the ~rdt_send(data)~ event occurs, the sender
  will create a packet ( ~sndpkt~ ) containing the data to be sent, along with a
  packet checksum and then send the packet via the ~udt_send(sndpkt)~ operation.
- In the rightmost state, the sender protocol is waiting for an ACK or a NAK
  packet from the receiver.
  - If an ACK packet is received the sender knows that the most recently
    transmitted packet has been received correctly and thus the protocol returns
    to the state of waiting for data from the upper layer.
  - If a NAK is received, the protocol retransmits the last packet and waits for
    an ACK or NAK to be returned by the receiver in response to the
    retransmitted data packet.

when the sender is in the wait-for-ACK-or-NAK state, it cannot get more data
from the upper layer; that is, the ~rdt_send()~ event can not occur; that will
happen only after the sender receives an ACK and leaves this state. Thus, the
sender will not send a new piece of data until it is sure that the receiver has
correctly received the current packet. Because of this behavior, protocols such
as rdt2.0 are known as ~stop-and-wait~ protocols.


The receiver-side FSM for rdt2.0 still has a single state. On packet arrival,
the receiver replies with either an ACK or a NAK, depending on whether or not
the received packet is corrupted.

we haven’t accounted for the possibility that the ACK or NAK packet could be
corrupted. Minimally, we will need to add checksum bits to ACK/NAK packets in
order to detect such errors. The more difficult question is how the protocol
should recover from errors in ACK or NAK packets. The difficulty here is that if
an ACK or NAK is corrupted, the sender has no way of knowing whether or not the
receiver has correctly received the last piece of transmitted data.

Consider three possibilities for handling corrupted ACKs or NAKs:
- For the first possibility, consider what a human might do in the
  message-dictation scenario. If the speaker didn’t understand the “OK” or
  “Please repeat that” reply from the receiver, the speaker would probably ask,
  “What did you say?” (thus introducing a new type of sender-to-receiver packet
  to our protocol). The receiver would then repeat the reply. But what if the
  speaker’s “What did you say?” is corrupted? The receiver, having no idea
  whether the garbled sentence was part of the dictation or a request to repeat
  the last reply, would probably then respond with “What did you say?” And then,
  of course, that response might be garbled. Clearly, we’re heading down a
  difficult path.
- A second alternative is to add enough checksum bits to allow the sender not
  only to detect, but also to recover from, bit errors. This solves the
  immediate problem for a channel that can corrupt packets but not lose them.
- A third approach is for the sender simply to resend the current data packet
  when it receives a garbled ACK or NAK packet. This approach, however,
  introduces duplicate packets into the sender-to-receiver channel. The
  fundamental difficulty with duplicate packets is that the receiver doesn’t
  know whether the ACK or NAK it last sent was received correctly at the sender.
  Thus, it cannot know a priori whether an arriving packet contains new data or
  is a retransmission!

A simple solution is to add a new field to the data packet and have the sender
number its data packets by putting a ~sequence number~ into this field. The
receiver then need only check this sequence number to determine whether or not
the received packet is a retransmission. For this simple case of a stop-and-wait
protocol, a 1-bit sequence number will suffice, since it will allow the receiver
to know whether the sender is resending the previously transmitted packet (the
sequence number of the received packet has the same sequence number as the most
recently received packet) or a new packet.Since we are currently assuming a
channel that does not lose packets, ACK and NAK packets do not themselves need
to indicate the sequence number of the packet they are acknowledging. The sender
knows that a received ACK or NAK packet (whether garbled or not) was generated
in response to its most recently transmitted data packet.

imagen 3.11
imagen 3.12

The rdt2.1 sender and receiver FSMs each now have twice as many states as
before. This is because the protocol state must now reflect whether the packet
currently being sent (by the sender) or expected (at the receiver) should have a
sequence number of 0 or 1. Note that the actions in those states where a 0-
numbered packet is being sent or expected are mirror images of those where a
1-numbered packet is being sent or expected; the only differences have to do
with the handling of the sequence number.

Protocol rdt2.1 uses both positive and negative acknowledgments from the
receiver to the sender. When an out-of-order packet is received, the receiver
sends a positive acknowledgment for the packet it has received. When a corrupted
packet is received, the receiver sends a negative acknowledgment. We can
accomplish the same effect as a NAK if, instead of sending a NAK, we send an ACK
for the last correctly received packet.

A sender that receives two ACKs for the same packet (that is, receives duplicate
ACKs) knows that the receiver did not correctly receive the packet following the
packet that is being ACKed twice.

One subtle change between rtdt2.1 and rdt2.2 is that the receiver must now
include the sequence number of the packet being acknowledged by an ACK message
(this is done by including the ACK , 0 or ACK , 1 argument in ~make_pkt()~ in the
receiver FSM), and the sender must now check the sequence number of the packet
being acknowledged by a received ACK message (this is done by including the 0 or 1 argument in ~isACK()~ in the sender FSM).

*** Reliable Data Transfer over a Lossy Channel with Bit Errors: rdt3.0
Suppose now that in addition to corrupting bits, the underlying channel can lose
packets as well.

- how to detect packet loss and
- what to do when packet loss occurs.

The use of checksumming, sequence numbers, ACK packets, and retransmissions—the
techniques already developed in rdt2.2 —will allow us to answer the latter
concern.

Suppose that the sender transmits a data packet and either that packet, or the
receiver’s ACK of that packet, gets lost. In either case, no reply is
forthcoming at the sender from the receiver. If the sender is willing to wait
long enough so that it is certain that a packet has been lost, it can simply
retransmit the data packet.

But how long must the sender wait to be certain that something has been lost?
The sender must clearly wait at least as long as a round-trip delay between the
sender and receiver (which may include buffering at intermediate routers) plus
whatever amount of time is needed to process a packet at the receiver. In many
networks, this worst-case maximum delay is very difficult even to estimate, much
less know with certainty. Moreover, the protocol should ideally recover from
packet loss as soon as possible; waiting for a worst-case delay could mean a
long wait until error recovery is initiated.


If an ACK is not received within this time, the packet is retransmitted. Note
that if a packet experiences a particularly large delay, the sender may
retransmit the packet even though neither the data packet nor its ACK have been
lost. This introduces the possibility of ~duplicate data packets~ in the
sender-to-receiver channel. Happily, protocol rdt2.2 already has enough
functionality (that is, sequence numbers) to handle the case of duplicate
packets.

imagen 3.14

The sender does not know whether a data packet was lost, an ACK was lost, or if
the packet or ACK was simply overly delayed. In all cases, the action is the
same: retransmit. Implementing a time-based retransmission mechanism requires a
countdown timer that can interrupt the sender after a given amount of time has
expired. The sender will thus need to be able to (1) start the timer each time a
packet (either a first-time packet or a retransmission) is sent, (2) respond to
a timer interrupt (taking appropriate actions), and (3) stop the timer.

imagen 3.15

note that a receive time for a packet is necessarily later than the send time
for a packet as a result of transmission and propagation delays.

the send-side brackets indicate the times at which a timer is set and later
times out. Several of the more subtle aspects of this protocol are explored in
the exercises at the end of this chapter.

imagen 3.16

             