#+title:Manejo de memoria
* Traduccion de Direcciones
** Concepto

[[./img/8.1.png][8.1]]

Address translation is a simple function, illustrated in Figure
8.1. The translator takes each instruction and data memory reference
generated by a process, checks whether the address is legal, and
converts it to a physical memory address that can be used to fetch or
store instructions or data.

Goals:
- Memory protection. We need the ability to limit the access of a
  process to certain regions of memory. Often, however, we may
  want to limit access of a program to its own memory, e.g., to
  prevent a pointer error from overwriting the code region or to cause
  a trap to the debugger when the program references a specific data
  location.
- Memory sharing. We want to allow multiple processes to share
  selected regions of memory. These shared regions can be large (e.g.,
  if we are sharing a program’s code segment among multiple processes
  executing the same program) or relatively small (e.g., if we are
  sharing a common library, a file, or a shared data structure).
- Flexible memory placement. We want to allow the operating system the
  flexibility to place a process (and each part of a process) anywhere
  in physical memory; this will allow us to pack physical memory more
  efficiently.
- Sparse addresses. Many programs have multiple dynamic memory regions
  that can change in size over the course of the execution of the
  program: the heap for data objects, a stack for each thread, and
  memory mapped files.
- Runtime lookup efficiency. Hardware address translation occurs on
  every instruction fetch and every data load and store. It would be
  impractical if a lookup took, on average, much longer to execute
  than the instruction itself.
- Compact translation tables. We also want the space overhead of
  translation to be minimal; any data structures we need should be
  small compared to the amount of physical memory being managed.
- Portability. Different hardware architectures make different choices
  as to how they implement translation; if an operating system kernel
  is to be easily portable across multiple processor architectures, it
  needs to be able to map from its data structures to the specific
  capabilities of each architecture.

the process sees its own memory, using its own addresses. We will call
these virtual addresses, because they do not necessarily correspond to
any physical reality. By contrast, to the memory system, there are
only physical addresses — real locations in memory. From the memory
system perspective, it is given physical addresses and it does lookups
and stores values. The translation mechanism converts between the two
views: from a virtual address to a physical memory address.
** Traduccion de Direcciones (revisar titulo)
*** Base and Bounds
The translation box consists of two extra registers per process:
- base register specifies the start of the process’s region of
  physical memory;
- bound register specifies the extent of that region.

If the base register is added to every address generated by the
program, then we no longer need a relocating loader — the virtual
addresses of the program start from 0 and go to bound, and the
physical addresses start from base and go to base + bound.

[[./img/8.2.png]]

Since physical memory can contain several processes, the kernel resets
the contents of the base and bounds registers on each process context
switch to the appropiate values for that process.

It is not possible to prevent a program from overwriting its own code.
It is difficult to share regions of memory between two processes.
Since the memory for a process needs to be contiguous, supporting
dynamic memory regions, such as for heaps, thread stacks, or memory
mapped files, becomes difficult to impossible.

*** Memoria Segmentada                                             :segments:

[[./img/8.3.png]]

Instead of keeping only a single pair of base and bounds registers per
process, the hardware can support an array of pairs of base and bounds
registers, for each process. This is called <<<segmentation>>>.  Each
entry in the array controls a portion, or segment, of the virtual
address space.  The physical memory for each segment is stored
contiguously, but different segments can be stored at different
locations.

The high order bits of the virtual address are used to index into the
array; the rest of the address is then treated as above — added to the
base and checked against the bound stored at that index.

The operating system can assign different segments different
permissions.

Segmented memory has gaps; program memory is no longer a single
contiguous region, but instead it is a set of regions. Each different
segment starts at a new segment boundary.

If a program branches into or tries to load data from one of the gaps,
the hardware will generate an exception, trapping into the operating
system kernel this is called a <<<segmentation fault>>>, that is, a
reference outside of a legal segment of memory.

[[./img/8.4.png][shared segments]]

With segments, the operating system can allow processes to share some
regions of memory while keeping other regions protected. For example,
two processes can share a code segment by setting up an entry in their
segment tables to point to the same region of physical memory

shared library routines, such as a graphics library, can be placed
into a segment and shared between processes.

disadvantages

The principal downside of segmentation is the overhead of managing a
large number of variable size and dynamically growing memory
segments. Over time, as processes are created and finish, physical
memory will be divided into regions that are in use and regions that
are not, that is, available to be allocated to a new process. These
free regions will be of varying sizes. When we create a new segment,
we will need to find a free spot for it.

However we choose to place new segments, as more memory becomes
allocated, the operating system may reach a point where there is
enough free space for a new segment, but the free space is not
contiguous. This is called <<<external fragmentation>>>.

solution to ext fragmentation
The operating system is free to compact memory to make room without
affecting applications, because virtual addresses are unchanged when
we relocate a segment in physical memory.  Even so, <<<compaction>>>
can be costly in terms of processor overhead.


*** Memoria Paginada
With paging, memory is allocated in fixed-sized chunks called page
frames. Address translation is similar to how it works with
segmentation. There is a page table for each process whose entries
contain pointers to page frames. Because page frames are fixed-sized
and a power of two, the page table entries only need to provide the
upper bits of the page frame address, so they are more compact. There
is no need for a “bound” on the offset; the entire page in physical
memory is allocated as a unit.

[[./img/8.6.png]]

a program thinks of its memory as linear, in fact its memory can be,
and usually is, scattered throughout physical memory in a kind of
abstract mosaic. The processor will execute one instruction after
another using virtual addresses; its virtual addresses are still
linear. However, the instruction located at the end of a page will be
located in a completely different region of physical memory from the
next instruction at the start of the next page.

Paging addresses the principal limitation of segmentation: free-space
allocation is very straightforward. The operating system can represent
physical memory as a bit map, with each bit representing a physical page
frame that is either free or in use. Finding a free frame is just a matter of
finding an empty bit.

Sharing memory between processes is also convenient: we need to set the
page table entry for each process sharing a page to point to the same
physical page frame. 

A downside of paging is that while the management of physical memory
becomes simpler, the management of the virtual address space becomes
more challenging. 

[[./img/8.5.png]]

The size of the page table is proportional to the size of the virtual
address space, not to the size of physical memory.

We can reduce the space taken up by the page table by choosing a
larger page frame. How big should a page frame be? A larger page frame
can waste space if a process does not use all of the memory inside the
frame.  This is called internal fragmentation. Fixed-size chunks are
easier to allocate, but waste space if the entire chunk is not used.

*** Segmentacion Paginada
With <<<paged segmentation>>>, memory is segmented, but instead of
each segment table entry pointing directly to a contiguous region of
physical memory, each segment table entry points to a page table,
which in turn points to the memory backing that segment. The segment
table entry “bound” describes the page table length, that is, the
length of the segment in pages. Because paging is used at the lowest
level, all segment lengths are some multiple of the page size.

[[./img/8.7.png]]

*** Paginacion Multinivel
multiple levels of page tables.

[[./img/8.8.png]]

the top-level page table contains entries, each of which points to a
second-level page table whose entries are pointers to page tables.

Only the top-level page table must be filled in; the lower levels of
the tree are allocated only if those portions of the virtual address
space are in use by a particular process. Access permissions can be
specified at each level, and so sharing between processes is possible
at each level.

*** Segmentacion Paginanada Multinivel
segmented memory where each segment is managed by a multi-level page
table.

The x86 has a per-process Global Descriptor Table (<<<GDT>>>),
equivalent to a segment table. The GDT is stored in memory; each entry
(descriptor) points to the (multi-level) page table for that segment
along with the segment length and segment access permissions. To start
a process, the operating system sets up the GDT and initializes a
register, the Global Descriptor Table Register (GDTR), that contains
the address and length of the GDT.

** Eficiencia
<<<cache>>, a copy of some data that can be accessed more quickly than
the original.

*** Translation Lookaside Buffer (TLB)
A <<<translation lookaside buffer>>> (<<<TLB>>>) is a small hardware
table containing the results of recent address translations. Each
entry in the TLB maps a virtual page to a physical page.

[[./img/8.9.png]]

Instead of finding the relevant entry by a multi-level lookup or by
hashing, the TLB hardware (typically) checks all of the entries
simultaneously against the virtual page. If there is a match, the
processor uses that entry to form the physical address, skipping the
rest of the steps of address translation. This is called a <<<TLB
hit>>>. On a TLB hit, the hardware still needs to check permissions,
in case, for example, the program attempts to write to a code-only
page or the operating system needs to trap on a store instruction to a
copy-on-write page.

A <<<TLB miss>>> occurs if none of the entries in the TLB match. In
this case, the hardware does the full address translation in the way
we described above.  When the address translation completes, the
physical page is used to form the physical address, and the
translation is installed in an entry in the TLB, replacing one of the
existing entries. Typically, the replaced entry will be one that has
not been used recently.

[[./img/8.10.png]]

*** Consistencia de la TLB

- Process context switch. What happens on a process context switch?
The virtual addresses of the old process are no longer valid, and
should no longer be valid, for the new process. 

On a context switch, we need to change the hardware page table
register to point to the new process’s page table. However, the TLB
also contains copies of the old process’s page translations and
permissions. One approach is to flush the TLB (discard its contents)
on every context switch. Since emptying the cache carries a
performance penalty, modern processors have a tagged TLB.  Entries in
a tagged TLB contain the process ID that produced each translation.

With a tagged TLB, the operating system stores the current process ID
in a hardware register on each context switch. When performing a
lookup, the hardware ignores TLB entries from other processes, but it
can reuse any TLB entries that remain from the last time the current
process executed.

*** Virtually adressed caches
Another step to improving the performance of address translation is to
include a virtually addressed cache before the TLB is consulted

[[./img/8.11.png]]

A virtually addressed cache stores a copy of the contents of physical
memory, indexed by the virtual address. When there is a match, the
processor can use the data immediately, without waiting for a TLB
lookup or page table translation to generate a physical address, and
without waiting to retrieve the data from main memory.

The same consistency issues that apply to TLBs also apply to virtually
addressed caches.

*** Physically adressed caches
Many processor architectures include a physically addressed cache that
is consulted as a second-level cache after the virtually addressed
cache and TLB, but before main memory.

[[./img/8.12.png]]

Together, these physically addressed caches serve a dual purpose:
- Faster memory references. An on-chip physically addressed cache will
  have a lookup latency that is ten times (2nd level) or three times
  (3rd level) faster than main memory.
- Faster TLB misses. In the event of a TLB miss, the hardware will
  generate a sequence of lookups through its multiple levels of page
  tables. Because the page tables are stored in physical memory, they
  can be cached. Thus, even a TLB miss and page table lookup may be
  handled entirely on chip.

* Cache y Memoria Virtual                                     :virtualMemory:

#+begin_quote
A cache is a copy of a computation or data that can be accessed more
quickly than the original.
#+end_quote

Regardless of the context, all caches face three design challenges:
1. Locating the cached copy. Because caches are designed to improve
   performance, a key question is often how to quickly determine
   whether the cache contains the needed data or not.
2. Replacement policy. Most caches have physical limits on how many
   items they can store; when new data arrives in the cache, the system
   must decide which data is most valuable to keep in the cache and
   which can be replaced.
3. Coherence. How do we detect, and repair, when a cached copy becomes
   out of date? This question, cache coherence, is central to the
   design of multiprocessor and distributed systems.

<<<temporal locality>>>: programs tend to reference the same instructions
and data that they had recently accessed.

<<<spatial locality>>>: Programs tend to reference data near other data that
has been recently referenced.

* Manejo de Memoria Avanzado
 
 
 
 
 
 
 
 

