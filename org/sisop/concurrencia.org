#+title:Concurrencia
* Concurrencia y threads
We use the word concurrency to refer to multiple activities that can
happen at the same time.

[[./img/4.1.png]]

The key idea is to write a concurrent program — one with many
simultaneous activities — as a set of sequential streams of execution,
or threads, that interact and share results in very precise ways.

Threads let us define a set of tasks that run concurrently while the
code for each task is sequential. Each thread behaves as if it has its
own dedicated processor, as illustrated in Figure 4.1.

The thread abstraction lets the programmer create as many threads as
needed without worrying about the exact number of physical processors,
or exactly which processor is doing what at each instant. Of course,
threads are only an abstraction: the physical hardware has a limited
number of processors (and potentially only one!).

The operating system’s job is to provide the illusion of a nearly
infinite number of virtual processors even while the physical hardware
is more limited.
#+BEGIN_EXAMPLE
It sustains this illusion by transparently suspending and resuming
threads so that at any given time only a subset of the threads are
actively running.
#+END_EXAMPLE

** Casos de uso de threads
The intuition behind the thread abstraction is simple: in a program,
we can represent each concurrent task as a thread. Each thread
provides the abstraction of sequential execution similar to the
traditional programming model. In fact, we can think of a traditional
program as single-threaded with one logical sequence of steps as each
instruction follows the previous one.

A multi-threaded program is a generalization of the same basic
programming model. Each individual thread follows a single sequence of
steps as it executes statements, iterates through loops, calls/returns
from procedures, etc. However, a program can now have several such
threads executing at the same time.

*** Razones para usar threads
1. Program structure: expressing logically concurrent tasks. Programs
   often interact with or simulate real-world applications that have
   concurrent activities. Threads let you express an application’s
   natural concurrency by writing each concurrent task as a separate
   thread.
2. Responsiveness: shifting work to run in the background. To improve
   user responsiveness and performance, a common design pattern is to
   create threads to perform work in the background, without the user
   waiting for the result.
3. Performance: exploiting multiple processors. Programs can use
   threads on a multiprocessor to do work in parallel; they can do the
   same work in less time or more work in the same elapsed time.  An
   advantage to using threads for parallelism is that the number of
   threads need not exactly match the number of processors in the
   hardware on which it is running. The operating system transparently
   switches which threads run on which processors.
4. Performance: managing I/O devices. To do useful work, computers
   must interact with the outside world via I/O devices. By running
   tasks as separate threads, when one task is waiting for I/O, the
   processor can make progress on a different task.

The benefit of concurrency between the processor and the I/O is two-
fold:

First, processors are often much faster than the I/O systems with
which they interact, so keeping the processor idle during I/O would
waste much of its capacity. For example, the latency to read from disk
can be tens of milliseconds, enough to execute more than 10 million
instructions on a modern processor. After requesting a block from
disk, the operating system can switch to another program, or another
thread within the same program, until the disk completes and the
original thread is ready to resume.

Second, I/O provides a way for the computer to interact with external
entities, such as users pressing keys on a keyboard or a remote
computer sending network packets. The arrival of this type of I/O
event is unpredictable, so the processor must be able to work on other
tasks while still responding quickly to these external events.

** Abstraccion de Thread
A thread is a single execution sequence that represents a separately

- Single execution sequence. Each thread executes a sequence of
  instructions — assignments, conditionals, loops, procedures, and so
  on — just as in the familiar sequential programming model.
- Separately schedulable task. The operating system can run, suspend,
  or resume a thread at any time.

*** Ejecutar, Suspender y Resumir threads
La ilusion de infinitos procesadores se logra medieante la ejecucion
de instrucciones de cada thread para que cada uno pueda progresar.

To map an arbitrary set of threads to a fixed set of processors,
operating systems include a <<<thread scheduler>>> that can switch
between threads that are running and those that are ready but not
running. For example, in the previous Figure 4.1, a scheduler might
suspend thread 1 from processor 1, move it to the list of ready
threads, and then resume thread 5 by moving it from the ready list to
run on processor 1.

Switching between threads is transparent to the code being executed
within each thread. The abstraction makes each thread appear to be a
single stream of execution; this means the programmer can pay
attention to the sequence of instruction within a thread and not
whether or when that sequence may be (temporarily) suspended to let
another thread run.

Threads thus provide an execution model in which each thread runs on a
dedicated virtual processor with unpredictable and variable
speed. From the point of view of a thread’s code, each instruction
appears to execute immediately after the preceding one. However, the
scheduler may suspend a thread between one instruction and the next
and resume running it later.

[[./img/4.3.png]]

Figure 4.3 illustrates a programmer’s view of a simple program and
three (of many) possible ways the program might be executed, depending
on what the scheduler does. From the thread’s point of view, other
than the speed of execution, the alternatives are equivalent. Indeed,
the thread would typically be unaware of which of these (or other)
executions actually occurs.

[[./img/4.4.png]]
How threads are scheduled affects a thread’s interleavings with other
threads. Figure 4.4 shows some of the many possible interleavings of a
program with three threads.

** Thread API
Figure 4.5 shows a simple API for using threads. This simplified API
is based on the POSIX standard pthreads API, but it omits some POSIX
options and error handling for simplicity.

there is no image 4.5.png

A good way to understand the simple threads API is that it provides a
way to invoke an <<<asynchronous procedure call>>>.

A normal procedure call passes a set of arguments to a function, runs
the function immediately on the caller’s stack, and when the function
is completed, returns control back to the caller with the result.

An asynchronous procedure call separates the call from the return:
with thread_create, the caller starts the function, but unlike a
normal procedure call, the caller continues execution concurrently

*** Hello world multi-thread

#+BEGIN_SRC 
#include <stdio.h>
#include "thread.h"

static void go(int n);

#define NTHREADS 10
static thread_t threads[NTHREADS];

int main(int argc, char **argv) {
    int i;
    long exitValue;

    for (i = 0; i < NTHREADS; i++){
        thread_create(&(threads[i]), &go, i);
    }
    for (i = 0; i < NTHREADS; i++){
        exitValue = thread_join(threads[i]);
        printf("Thread %d returned with %ld\n", i, exitValue);
    }
    printf("Main thread done.\n");
    return 0;
}

void go(int n) {
    printf("Hello from thread %d\n", n);
    thread_exit(100 + n);
    // not reached
}	 
#+END_SRC

To illustrate how to use the simple threads API, Figure 4.6 shows a
very simple multi-threaded program written in ’C’. The main function
uses thread_create to create 10 threads. The interesting arguments are
the second and third.

- The second argument, go, is a function pointer — where the newly
  created thread should begin execution.
- The third argument, i, is passed to that function.

Thus, thread_create initializes the i’th thread’s state so that it is
prepared to call the function go with the argument i.

When the scheduler runs the i’th thread, that thread runs the function
go with the value i as an argument and prints Hello from thread i. The
thread then returns the value (i + 100) by calling thread_exit. This
call stores the specified value in a field in the thread_t object so
that thread_join can retrieve it.

*** Paralelismo Fork join
Although the interface in Figure 4.5 is simple, it is remarkably
powerful.  Many multi-threaded applications can be designed using only
these thread operations and no additional synchronization. With
fork-join parallelism, a thread can create child threads to perform
work (“fork”, or thread_create), and it can wait for their results
(“join”). Data may be safely shared between threads, provided it is
(a) written by the parent before the child thread starts or (b)
written by the child and read by the parent after the join.

If these sharing restrictions are followed, each thread executes
independently and in a deterministic fashion, unaffected by the
behavior of any other concurrently executing thread. The multiplexing
of threads onto processors has no effect other than performance.

** Estructuras de Datos de Threads y Ciclo de Vida
As we have seen, each thread represents a sequential stream of
execution.  The operating system provides the illusion that each
thread runs on its own virtual processor by transparently suspending
and resuming threads.

For the illusion to work, the operating system must precisely save and
restore the state of a thread. However, because threads run either in
a process or in the kernel, there is also shared state that is not
saved or restored when switching the processor between threads.

Thus, to understand how the operating system implements the thread
abstraction, we must define both the per-thread state and the state
that is shared among threads. Then we can describe a thread’s life
cycle — how the operating system can create, start, stop, and delete
threads to provide the abstraction.

[[./img/4.5.png]]

*** Estado por Thread y Thread Control Block (TCB)                      :TCB:
The operating system needs a data structure to represent a thread’s
state; a thread is like any other object in this regard. This data
structure is called the <<<thread control block>>> (TCB). For every
thread the operating system creates, it creates one TCB.

The thread control block holds two types of per-thread information:
1. The state of the computation being performed by the thread.
2. Metadata about the thread that is used to manage the thread.

**** Per-thread Computation State. 
To create multiple threads and to be able to start and stop each
thread as needed, the operating system must allocate space in the TCB
for the current state of each thread’s computation: a pointer to the
thread’s stack and a copy of its processor registers.

- Stack. A thread’s stack is the same as the stack for a
  single-threaded computation — it stores information needed by the
  nested procedures the thread is currently running. For example, if a
  thread calls foo(), foo() calls bar(), and bar() calls bas(), then
  the stack would contain a stack frame for each of these three
  procedures; each stack frame contains the local variables used by
  the procedure, the parameters the procedure was called with, and the
  return address to jump to when the procedure completes.
Because at any given time different threads can be in different states
  in their sequential computations — each can be in a different place
  in a different procedure called with different arguments from a
  different nesting of enclosing procedures — each thread needs its
  own stack.  When a new thread is created, the operating system
  allocates it a new stack and stores a pointer to that stack in the
  thread’s TCB. The stack is allocated in memory like any other data
  structure.
- Copy of processor registers. A processor’s registers include not
  only its general-purpose registers for storing intermediate values
  for ongoing computations, but they also include special-purpose
  registers, such as the instruction pointer and stack pointer.  
To be able to suspend a thread, run another thread, and later resume
  the original thread, the operating system needs a place to store a
  thread’s registers when that thread is not actively running. In some
  systems, the general-purpose registers for a stopped thread are
  stored on the top of the stack, and the TCB contains only a pointer
  to the stack. In other systems, the TCB contains space for a copy of
  all processor registers.

**** Per-thread Metadata. 
The TCB also includes per-thread metadata - information for managing
the thread. For example, each thread might have a thread ID,
scheduling priority, and status (e.g., whether the thread is waiting
for an event or is ready to be placed onto a processor).

*** Estado Compartido
As opposed to per-thread state that is allocated for each thread, some
state is shared between threads running in the same process or within
the operating system kernel (Figure 4.8). In particular, program code
is shared by all threads in a process, although each thread may be
executing at a different place within that code. Additionally,
statically allocated global variables and dynamically allocated heap
variables can store information that is accessible to all threads.

** Ciclo de Vida de un thread
Figure 4.9 shows the states of a thread durings its lifetime.

[[./img/4.9.png]]

*** INIT
Thread creation puts a thread into its INIT state and allocates and
initializes per-thread data structures. Once that is done, thread
creation code puts the thread into the READY state by adding the
thread to the ready list. The ready list is the set of runnable
threads that are waiting their turn to use a processor. In practice,
the operating system typically uses a more sophisticated data
structure to keep track of runnable threads, such as a priority queue.

*** READY
A thread in the READY state is available to be run but is not
currently running. Its TCB is on the ready list, and the values of its
registers are stored in its TCB. At any time, the scheduler can cause
a thread to transition from READY to RUNNING by copying its register
values from its TCB to a processor’s registers.

*** RUNNING
A thread in the RUNNING state is running on a processor. At this time,
its register values are stored on the processor rather than in the
TCB. A RUNNING thread can transition to the READY state in two ways:
- The scheduler can preempt a running thread and move it to the READY
  state by: (1) saving the thread’s registers to its TCB and (2)
  switching the processor to run the next thread on the ready list.
- A running thread can voluntarily relinquish the processor and go
  from RUNNING to READY by calling <<<yield>>> (e.g., thread_yield in
  the thread library).

*** WAITING
A thread in the WAITING state is waiting for some event.  Whereas the
scheduler can move a thread in the READY state to the RUNNING state, a
thread in the WAITING state cannot run until some action by another
thread moves it from WAITING to READY.

While a thread waits for an event, it cannot make progress; therefore,
it is not useful to run it. Rather than continuing to run the thread
or storing the TCB on the scheduler’s ready list, the TCB is stored on
the waiting list of some synchronization variable associated with the
event. When the required event occurs, the operating system moves the
TCB from the synchronization variable’s waiting list to the
scheduler’s ready list, transitioning the thread from WAITING to
READY.

*** FINISHED
A thread in the FINISHED state never runs again. The system can free
some or all of its state for other uses, though it may keep some
remnants of the thread in the FINISHED state for a time by putting the
TCB on a finished list. For example, the thread_exit call lets a
thread pass its exit value to its parent thread via
thread_join. Eventually, when a thread’s state is no longer needed
(e.g., after its exit value has been read by the join call), the
system can delete and reclaim the thread’s state.

-----

One way to understand these states is to consider where a thread’s TCB
and registers are stored, as shown in Figure 4.10. For example, all
threads in the READY state have their TCBs on the ready list and their
registers in the TCB. All threads in the RUNNING state have their TCBs
on the running list and their register values in hardware
registers. And all threads in the WAITING state have their TCBs on
various synchronization variables’ waiting lists.

-----

** Kernel Threads
- Kernel threads. The simplest case is implementing threads inside the
  operating system kernel, sharing one or more physical processors. A
  <<<kernel thread>>> executes kernel code and modifies kernel data
  structures. Almost all commercial operating systems today support
  kernel threads.
- Kernel threads and single-threaded processes. An operating system
  with kernel threads might also run some single-threaded user
  processes. As shown in Figure 4.11, these processes can invoke
  system calls that run concurrently with kernel threads inside the
  kernel.
- Multi-threaded processes using kernel threads. Most operating
  systems provide a set of library routines and system calls to allow
  applications to use multiple threads within a single user-level
  process. Figure 4.12 illustrates this case. These threads execute
  user code and access user-level data structures. They also make
  system calls into the operating system kernel. For that, they need a
  kernel interrupt stack just like a normal single-threaded process.
- User-level threads. To avoid having to make a system call for every
  thread operation, some systems support a model where user-level
  thread operations — create, yield, join, exit, and the
  synchronization routines (other chapter) — are implemented entirely
  in a user-level library, without invoking the kernel.

[[./img/4.11.png]]

[[./img/4.12.png]]

*** Creando un thread
#+BEGIN_SRC 
// func is a pointer to a procedure the thread will run.
// arg is the argument to be passed to that procedure.
void
thread_create(thread_t *thread, void (*func)(int), int arg) {
    // Allocate TCB and stack
    TCB *tcb = new TCB();

    thread->tcb = tcb;
    tcb->stack_size = INITIAL_STACK_SIZE;
    tcb->stack = new Stack(INITIAL_STACK_SIZE);

    // Initialize registers so that when thread is resumed, it will start running at
    // stub.  The stack starts at the top of the allocated region and grows down.
    tcb->sp = tcb->stack + INITIAL_STACK_SIZE;
    tcb->pc = stub;

    // Create a stack frame by pushing stub’s arguments and start address
    // onto the stack: func, arg
    *(tcb->sp) = arg;
    tcb->sp--;
    *(tcb->sp) = func;
    tcb->sp--;

    // Create another stack frame so that thread_switch works correctly.
    // This routine is explained later in the chapter.
    thread_dummySwitchFrame(tcb);

    tcb->state = READY;
    readyList.add(tcb);   // Put tcb on ready list
}

void
stub(void (*func) (int), int arg) {
    (*func) (arg);     // Execute the function func()
    thread_exit(0);    // If func () does not call exit, call it here.
}
#+END_SRC

Figure 4.13 shows the pseudo-code to allocate a new thread. The goal
of thread_create is to perform an asynchronous procedure call to func
with arg as the argument to that procedure. When the thread runs, it
will execute func(arg) concurrently with the calling thread.

There are three steps to creating a thread:
1. Allocate per-thread state. The first step in the thread constructor
   is to allocate space for the thread’s per-thread state: the TCB and
   stack.
2. Initialize per-thread state. To initialize the TCB, the thread
   constructor sets the new thread’s registers to what they need to be
   when the thread starts RUNNING. When the thread is assigned a
   processor, we want it to start running func(arg). However, instead
   of having the thread start in func, the constructor starts the
   thread in a dummy function, stub, which in turn calls func.
We need this extra step in case the func procedure returns instead of
   calling thread_exit. Without the stub, func would return to
   whatever random location is stored at the top of the stack!
   Instead, func returns to stub and stub calls thread_exit to finish
   the thread. To start at the beginning of stub, the thread
   constructor sets up the stack as if stub was just called by normal
   code; the specifics will depend on the calling convention of the
   machine. In the pseudo-code, we push stub’s two arguments onto the
   stack: func and arg. When the thread starts running, the code in
   stub will access its arguments just like a normal procedure. In
   addition, we also push a dummy stack frame for thread_switch onto
   the stack (more on this later).
3. Put TCB on ready list. The last step in creating a thread is to set
   its state to READY and put the new TCB on the ready list, enabling
   the thread to be scheduled.

*** Eliminando un thread
When a thread calls thread_exit, there are two steps to deleting the
thread:
- Remove the thread from the ready list so that it will never run
  again.
- Free the per-thread state allocated for the thread.

Although this seems easy, there is an important subtlety: if a thread
removes itself from the ready list and frees its own per-thread state,
then the program may break. For example, if a thread removes itself
from the ready list but an interrupt occurs before the thread finishes
de-allocating its state, there is a memory leak: that thread will
never resume to de- allocate its state.

Worse, suppose that a thread frees its own state? Can the thread
finish running the code in thread_exit if it does not have a stack?
What happens if an interrupt occurs just after the running thread’s
stack has been de- allocated? If the context switch code tries to save
the current thread’s state, it will be writing to de-allocated memory,
possibly to storage that another processor has re-allocated for some
other data structure. The result could be corrupted memory, where the
specific behavior depends on the precise sequence of events. Needless
to say, such a bug would be very difficult to locate.

Fortunately, there is a simple fix: a thread never deletes its own
state.  Instead, some other thread must do it. On exit, the thread
transitions to the FINISHED state, moves its TCB from the ready list
to a list of finished threads the scheduler should never run. The
thread can then safely switch to the next thread on the ready
list. Once the finished thread is no longer running, it is safe for
some other thread to free the state of the thread.

*** Context Switch de Threads                                :context:switch:
To support multiple threads, we also need a mechanism to switch which
threads are RUNNING and which are READY.

A <<<thread context switch>>> suspends execution of a currently
running thread and resumes execution of some other thread. The switch
saves the currently running thread’s registers to the thread’s TCB and
stack, and then it restores the new thread’s registers from that
thread’s TCB and stack into the processor.

We need to answer several questions:
- What triggers a context switch?
- How does a voluntary context switch work?
- How does an involuntary context switch differ from a voluntary one?
- What thread should the scheduler choose to run next?

-----

**** What Triggers a Kernel Thread Context Switch? A thread context switch
can be triggered by either a voluntary call into the thread library,
or an involuntary interrupt or processor exception.

- Voluntary. The thread could call a thread library function that
  triggers a context switch. For example the thread_yield call that
  lets the currently running thread voluntarily give up the processor
  to the next thread on the ready list. Similarly, the thread_join and
  thread_exit calls suspend execution of the current thread and start
  running a different one.
- Involuntary. An interrupt or processor exception could invoke an
  interrupt handler. The interrupt hardware saves the state of the
  running thread and executes the handler’s code. The handler can
  decide that some other thread should run, and then switch to
  it. Alternatively, if the current thread should continue running,
  the handler restores the state of the interrupted thread and resumes
  execution. Other I/O hardware events also invoke interrupt handlers.

Regardless, the thread system must save the current processor state,
so that when the current thread resumes execution, it appears to the
thread as if the event never occurred except for some time having
elapsed. This provides the abstraction of thread execution on a
virtual processor with unpredictable and variable speed.

***** Voluntary Kernel Thread Context Switch

#+BEGIN_SRC 
// We enter as oldThread, but we return as newThread.
// Returns with newThread’s registers and stack.
void thread_switch(oldThreadTCB, newThreadTCB) {
    pushad;                  // Push general register values onto the old stack.
    oldThreadTCB->sp = %esp; // Save the old thread’s stack pointer.
    %esp = newThreadTCB->sp; // Switch to the new stack.
    popad;            // Pop register values from the new stack.
    return;
}

void thread_yield() {
    TCB *chosenTCB, *finishedTCB;

    // Prevent an interrupt from stopping us in the middle of a switch.
    disableInterrupts();

    // Choose another TCB from the ready list.
    chosenTCB = readyList.getNextThread();
    if (chosenTCB == NULL) {
        // Nothing else to run, so go back to running the original thread.
    } else {
        // Move running thread onto the ready list.
        runningThread->state = ready;
        readyList.add(runningThread);
        thread_switch(runningThread, chosenTCB); // Switch to the new thread.
        runningThread->state = running;
    }

    // Delete any threads on the finished list.
    while ((finishedTCB = finishedList->getNextThread()) != NULL) {
    	delete finishedTCB->stack;
    	delete finishedTCB;
    }
    enableInterrupts();
}

// thread_create must put a dummy frame at the top of its stack:
// the return PC and space for pushad to have stored a copy of the registers.
// This way, when someone switches to a newly created thread,
// the last two lines of thread_switch work correctly.
void thread_dummySwitchFrame(newThread) {
    *(tcb->sp) = stub;      // Return to the beginning of stub.
    tcb->sp--;
    tcb->sp -= SizeOfPopad;
}
#+END_SRC

The pseudo-code for thread_yield first turns off interrupts to prevent
the thread system from attempting to make two context switches at the
same time. The pseudo-code then pulls the next thread to run off the
ready list (if any), and switches to it.

The thread_switch code may seem tricky, since it is called in the
context of the old thread and finishes in the context of the new
thread. To make this work, thread_switch saves the state of the
registers to the stack and saves the stack pointer to the TCB. It then
switches to the stack of the new thread, restores the new thread’s
state from the new thread’s stack, and returns to whatever program
counter is stored on the new stack.

A twist is that the return location may not be to thread_yield! The
return is to whatever the new thread was doing beforehand. For
example, the new thread might have been WAITING in thread_join and is
now READY to run.  The thread might have called thread_yield. Or it
might be a newly created thread just starting to run.

It is essential that any routine that causes the thread to yield or
block call thread_switch in the same way. Equally, to create a new
thread, thread_create must set up the stack of the new thread to be as
if it had suspended execution just before performing its first
instruction. Then, if the newly created thread is the next thread to
run, a thread can call thread_yield, switch to the newly created
thread, switch to its stack pointer, pop the register values off the
stack, and “return” to the new thread, even though it had never called
switch in the first place.

***** Involuntary Kernel Thread Context Switch
When an interrupt, exception, or trap interrupts a running user-level
process: hardware and software work together to save the state of the
interrupted process, run the kernel’s handler, and restore the state
of the interrupted process.

The mechanism is almost identical when an interrupt or trap triggers a
thread switch between threads in the kernel.

1. Save the state. Save the currently running thread’s registers so
   that the handler can run code without disrupting the interrupted
   thread. Hardware saves some state when the interrupt or exception
   occurs, and software saves the rest of the state when the handler
   runs.
2. Run the kernel’s handler. Run the kernel’s handler code to handle
   the interrupt or exception. Since we are already in kernel mode, we
   do not need to change from user to kernel mode in this step. We
   also do not need to change the stack pointer to the base of the
   kernel’s interrupt stack. Instead, we can just push saved state or
   handler variables onto the current stack, starting from the current
   stack pointer.
3. Restore the state. Restore the next ready thread’s registers so
   that the thread can resume running where it left off.

** Combinando Kernel Threads y Procesos de usuario Mono-thread
** Implementando Procesos Multi-thread
** Abstracciones alternativas
* Sincronizando Acceso a Objetos Compartidos
If a program has <<<independent threads>>> that operate on completely
separate subsets of memory, we can reason about each thread
separately. In this case, reasoning about independent threads differs
little from reasoning about a series of independent, single-threaded
programs.

However, most multi-threaded programs have both per-thread state
(e.g., a thread’s stack and registers) and shared state (e.g., shared
variables on the heap). <<<Cooperating threads>>> read and write
shared state.

Sharing state is useful because it lets threads communicate,
coordinate work, and share information.

The sequential model of reasoning does not work in programs with
cooperating threads, for three reasons:
1. Program execution depends on the possible interleavings of threads’
   access to shared state. For example, if two threads write a shared
   variable, one thread with the value 1 and the other with the value
   2, the final value of the variable depends on which of the threads’
   writes finishes last. 
2. Program execution can be nondeterministic. Different runs of the
   same program may produce different results. For example, the
   scheduler may make different scheduling decisions, the processor
   may run at a different frequency, or another concurrently running
   program may affect the cache hit rate. Even common debugging
   techniques — such as running a program under a debugger,
   recompiling with the -g option instead of -O, or adding a printf —
   can change how a program behaves. 
3. Compilers and processor hardware can reorder instructions. Modern
   compilers and hardware reorder instructions to improve
   performance. This reordering is generally invisible to
   single-threaded programs; compilers and processors take care to
   ensure that dependencies within a single sequence of instructions —
   that is, within a thread — are preserved. However, reordering can
   become visible when multiple threads interact through accessing
   shared variables.

We introduce a structured synchronization approach to sharing state in
multi-threaded programs which is:
(1) structure the program to facilitate reasoning about concurrency,
and
(2) use a set of standard synchronization primitives to control access
to shared state.

*** Challenges
**** Race Condition                                           :raceCondition:
A <<<race condition>> occurs when the behavior of a program depends on
the interleaving of operations of different threads. In effect, the
threads run a race between their operations, and the results of the
program execution depends on who wins the race.

**** Operaciones atomicas
Atomic operations are indivisible operations that cannot be interleaved
with or split by other operations.

**** Too much milk (Problema y solucion no optima)
We illustrate the problems with using atomic loads and stores using a
simple problem called, “Too Much Milk.”

The Too Much Milk problem models two roommates who share a
refrigerator and who — as good roommates — make sure the refrigerator
is always well stocked with milk. With such responsible roommates, the
following scenario is possible:

missig example

We can model each roommate as a thread and the number of bottles of
milk in the fridge with a variable in memory. If the only atomic
operations on shared state are atomic loads and stores to memory, is
there a solution to the Too Much Milk problem that ensures both safety
(the program never enters a bad state) and liveness (the program
eventually enters a good state)? Here, we strive for the following
properties:
- Safety: Never more than one person buys milk.
- Liveness: If milk is needed, someone eventually buys it.

Solution 1. 
- A roommate leaves a note on the fridge before going to the store. 
- To do this we set a flag when going to buy milk and to check this
  flag before going to buy milk. 

Each thread might run the following code:

#+BEGIN_SRC 
if (milk==0) {       // if no milk
    if (note==0) {   // if no note
        note = 1;    // leave note
        milk++;      // buy milk
        note = 0;    // remove note
    }
}
#+END_SRC

This implementation can violate safety, because TA might check for the
milk, then get context switched, then TB would run all its code, then
switch back to TA, and TA would go to buy more milk.

| // Thread A    | // Thread B    |
| if (milk==0) { |                |
|                | if (milk==0) { |
|                | if (note==0) { |
|                | note = 1;      |
|                | milk++;        |
|                | note = 0;      |
|                | }              |
|                | }              |
| if (note==0) { |                |
| note = 1;      |                |
| milk++;        |                |
| note = 0;      |                |
| }              |                |
| }              |                |
    
Solution 2. In solution 1, the roommate checks the note before setting
it.  This opens up the possibility that one roommate has already made
a decision to buy milk before notifying the other roommate of that
decision.  If we use two variables for the notes, a roommate can
create a note before checking the other note and the milk and making a
decision to buy.

Path A
#+BEGIN_SRC 
noteA = 1;         // leave note
if (noteB==0) {    // if no note  A1
    if (milk==0) { // if no milk  A2
        milk++;    // buy milk    A3
    }
}
noteA = 0;         // remove note A
#+END_SRC
 
Path B
#+BEGIN_SRC 
noteB = 1;         // leave note
if (noteA==0) {    // if no note  B1
    if (milk==0) { // if no milk  B2
        milk++;    // buy milk    B3
    }              //             B4
}                  //             B5
#+END_SRC

This solution is Safe but not Live, it is possible for both threads to
set their respective notes, for each thread to check the other
thread’s note, and for both threads to decide not to buy milk.

Solution 3. We ensure that at least one of the threads determines
whether the other thread has bought milk or not before deciding
whether or not to buy.

Path A
#+BEGIN_SRC
noteA = 1;         // leave note A
while (noteB==1) { // wait for no note B
    ;              // spin
}
if (milk==0) {     // if no milk M
    milk++;        // buy milk
}
noteA = 0;         // remove note A
#+END_SRC

Path B
#+BEGIN_SRC 
noteB = 1;          // leave note B
if (noteA==0) {     // if no note A
    if (milk==0) {  // if no milk
        milk++;     // buy milk
    }               //
}                   //
noteB = 0;          // remove note B
#+END_SRC
 
This solution is both Safe and Live.

**** Una Solucion Mejor
We write shared objects that use synchronization objects to coordinate
different threads’ access to shared state.

We have a primitive called a lock that only one thread at a time can
own. Then, we can solve the Too Much Milk problem by defining the
class for a Kitchen object with the following method:

#+BEGIN_SRC 
Kitchen::buyIfNeeded() {
    lock.acquire();
    if (milk == 0) {     // if no milk
        milk++;          // buy milk
    }
    lock.release();
}
#+END_SRC

*** Estructurando Objetos Compartidos
Figure 5.1 illustrates, a multi-threaded program is built using shared
objects and a set of threads that operate on them.

[[./img/5.1.png]]

<<<Shared objects>>> are objects that can be accessed safely by
multiple threads.  All shared state in a program — including variables
allocated on the heap and static, global variables — should be
encapsulated in one or more shared objects.

Since shared objects encapsulate the program’s shared state, the main
loop code that defines a thread’s high-level actions need not concern
itself with synchronization details.

A <<<synchronization variable>>> is a data structure used for
coordinating concurrent access to shared state. Both the interface and
the implementation of synchronization variables must be carefully
designed. In particular, we build shared objects using two types of
synchronization variables: locks and condition variables.

Synchronization variables coordinate access to state variables, which
are just the normal member variables of an object that you are
familiar with from single-threaded programming.  

** Locks: Exclusion Mutua                                              :lock:
A <<<lock>>> is a synchronization variable that provides <<<mutual
exclusion>>> — when one thread holds a lock, no other thread can hold
it (i.e., other threads are excluded). A program associates each lock
with some subset of shared state and requires a thread to hold the
lock when accessing that state. Then, only one thread can access the
shared state at a time.  

*** API de Locks
A lock enables mutual exclusion by providing two methods:
Lock::acquire() and Lock::release(). These methods are defined as
follows:
- A lock can be in one of two states: BUSY or FREE.
- A lock is initially in the FREE state.
- Lock::acquire waits until the lock is FREE and then atomically makes
  the lock BUSY.
- Lock::release makes the lock FREE. If there are pending acquire
  operations, this state change causes one of them to proceed.

Using locks makes solving the Too Much Milk problem trivial. Both
threads run the following code:

#+BEGIN_SRC 
lock.acquire();
if (milk == 0) {     // if no milk
    milk++;          // buy milk
}
lock.release();
#+END_SRC

Formal Properties. A lock can be defined more precisly as follows.

#+BEGIN_EXAMPLE
A thread holds a lock if it has returned from a lock’s acquire method
more often than it has returned from a lock’s release method.
#+END_EXAMPLE

#+BEGIN_EXAMPLE
A thread is attempting to acquire a lock if it has called but not yet
returned from a call to aquire on the lock.
#+END_EXAMPLE

A lock should ensure the following three properties:
1. Mutual Exclusion. At most one thread holds the lock.
2. Progress. If no thread holds the lock and any thread attempts to
   acquire the lock, then eventually some thread succeeds in acquiring
   the lock.
3. Bounded waiting. If thread T attempts to acquire a lock, then there
   exists a bound on the number of times other threads can
   successfully acquire the lock before T does.

*** Caso de estudio: Cola Limitada Thread-Safe

#+BEGIN_SRC 
// Thread-safe queue interface

const int MAX = 10;

class TSQueue {
  // Synchronization variables
    Lock lock;

  // State variables
    int items[MAX];
    int front;
    int nextEmpty;

  public:
    TSQueue();
    ~TSQueue(){};
    bool tryInsert(int item);
    bool tryRemove(int *item);
};

// Initialize the queue to empty
// and the lock to free.
TSQueue::TSQueue() {
    front = nextEmpty = 0;
}

// Try to insert an item. If the queue is
// full, return false; otherwise return true.
bool
TSQueue::tryInsert(int item) {
    bool success = false;

    lock.acquire();
    if ((nextEmpty - front) < MAX) {
        items[nextEmpty % MAX] = item;
        nextEmpty++;
        success = true;
    }
    lock.release();
    return success;
}

// Try to remove an item. If the queue is
// full, return false; otherwise return true.
bool
TSQueue::tryInsert(int item) {
    bool success = false;

    lock.acquire();
    if ((nextEmpty - front) < MAX) {
        items[nextEmpty % MAX] = item;
        nextEmpty++;
        success = true;
    }
    lock.release();
    return success;
}

// Try to remove an item. If the queue is
// empty, return false; otherwise return true.
bool
TSQueue::tryRemove(int *item) {
    bool success = false;

    lock.acquire();
    if (front < nextEmpty) {
        *item = items[front % MAX];
        front++;
        success = true;
    }
    lock.release();
    return success;
}
#+END_SRC

[[./img/5.4.png]]

*** Critical Sections
A <<<critical section>>> is a sequence of code that atomically
accesses shared state.  By ensuring that a thread holds the object’s
lock while executing any of its critical sections, we ensure that each
critical section appears to execute atomically on its shared
state.

WARNING: Put shared objects on the heap, not the stack. The compiler
allocates automatic variables (sometimes called “local variables”,
with good reason) on the stack during procedure invocation. If one
thread passes a pointer or reference to one of its automatic variables
to another thread and later returns from the procedure where the
automatic variable was allocated, then that second thread now has a
pointer into a region of the first thread’s stack that may be used for
other purposes.

** Condition Variables: Esperando por un Cambio           :conditionVariable:
<<<Condition variables>>> provide a way for one thread to wait for
another thread to take some action. For example, in the thread-safe
queue example in Figure 5.4, rather than returning an error when we
try to remove an item from an empty queue, we might wait until the
queue is non-empty, and then always return an item.

In all of these cases, we want a thread to wait for some action to
change the system state so that the thread can make progress.

#+BEGIN_SRC 
int
TSQueue::remove() {
    int item;
    bool success;

    do {
        success = tryRemove(&item);
    } until(success);
    return item;
}
#+END_SRC

*** Definicion
A <<<condition variable>>> is a synchronization object that lets a
thread efficiently wait for a change to shared state that is protected
by a lock. A condition variable has three methods:
- CV::wait(Lock *lock). This call atomically releases the lock and
  suspends execution of the calling thread, placing the calling thread
  on the condition variable’s waiting list. Later, when the calling
  thread is re-enabled, it re-acquires the lock before returning from
  the wait call.
- CV::signal(). This call takes one thread off the condition
  variable’s waiting list and marks it as eligible to run (i.e., it
  puts the thread on the scheduler’s ready list). If no threads are on
  the waiting list, signal has no effect.
- CV::broadcast(). This call takes all threads off the condition
  variable’s waiting list and marks them as eligible to run. If no
  threads are on the waiting list, broadcast has no effect.

A condition variable is used to wait for a change to shared state, and
a lock must always protect updates to shared state. Thus, the
condition variable API is designed to work in concert with locks. All
three methods (wait, signal, and broadcast) should only be called
while the associated lock is held.

#+BEGIN_SRC 
SharedObject::someMethodThatWaits() {
    lock.acquire();

    // Read and/or write shared state here.

    while (!testOnSharedState()) {
        cv.wait(&lock);
    }
    assert(testOnSharedState());

    // Read and/or write shared state here.

    lock.release();
}

SharedObject::someMethodThatSignals() {
    lock.acquire();

    // Read and/or write shared state here.

    // If state has changed in a way that
    // could allow another thread to make
    // progress, signal (or broadcast).

    cv.signal();

    lock.release();
}
#+END_SRC

In this code, the calling thread first
acquires the lock and can then read and write the shared object’s state
variables. To wait until testOnSharedState succeeds, the thread calls
wait on the shared object’s condition variable cv. This atomically puts the
thread on the waiting list and releases the lock, allowing other threads to
enter the critical section. Once the waiting thread is signaled, it re-acquires
the lock and returns from wait. The monitor can then safely test the state
variables to see if testOnSharedState succeeds. If so, the monitor performs
its tasks, releases the lock, and returns.

The method someMethodThatSignals() shows the complementary code that
causes a waiting thread to wake up. Whenever a thread changes the
shared object’s state in a way that enables a waiting thread to make
progress, the thread must signal the waiting thread using the
condition variable.

*** Propiedades
- A condition variable is memoryless.
The condition variable, has no internal state other than a queue of
waiting threads. Condition variables do not need their own state
because they are always used inside shared objects that have their own
state.

If no threads are currently on the condition variable’s waiting list,
a signal or broadcast has no effect. No thread calls wait unless it
holds the lock, checks the state variables, and finds that it needs to
wait. After signal is called, if sometime later another thread calls
wait, it will block until the next signal (or broadcast) is called,
regardless of how many times signal has been called in the past.
- CV::wait atomically releases the lock.
A thread always calls wait while holding a lock. The call to wait
atomically releases the lock and puts the thread on the condition
variable’s waiting list. 
- When a waiting thread is re-enabled via signal or broadcast, it may
not run immediately.

#+BEGIN_EXAMPLE
wait must always be called from within a loop
#+END_EXAMPLE

Because wait releases the lock, and because there is no guarantee of
atomicity between signal or broadcast and the return of a call to
wait, there is no guarantee that the checked-for state still
holds. Therefore, a waiting thread must always wait in a loop,
rechecking the state until the desired predicate holds.

#+BEGIN_SRC 
...
while (predicateOnStateVariables(...)) {
    wait(&lock);
}
...
#+END_SRC

and not

#+BEGIN_SRC 
...
if (predicateOnStateVariables(...)) {
    wait(&lock);
}
...
#+END_SRC

*** Ciclo de Vida (Actualizado)
A RUNNING thread that calls wait is put in the WAITING state. This is
typically implemented by moving the thread control block (TCB) from
the ready list to the condition variable’s list of waiting
threads. Later, when some RUNNING thread calls signal or broadcast on
that condition variable, one (if signal) or all (if broadcast) of the
TCBs on that condition variable’s waiting list are moved to the ready
list. This changes those threads from the WAITING state to the READY
state. At some later time, the scheduler selects a READY thread and
runs it by moving it to the RUNNING state.  Eventually, the signaled
thread runs.

Locks are similar. A lock acquire on a busy lock puts the caller into
the WAITING state, with the caller’s TCB on a list of waiting TCBs
associated with the lock. Later, when the lock owner calls release,
one waiting TCB is moved to the ready list, and that thread
transitions to the READY state.

Notice that threads that are RUNNING or READY have their state located
at a pre-defined, “global” location: the CPU (for a RUNNING thread) or
the scheduler’s list of ready threads (for a READY thread). However,
threads that are WAITING typically have their state located on some
per-lock or per-condition-variable queue of waiting threads. Then, a
signal, broadcast, or release call can easily find and re-enable a
waiting thread for that particular condition variable or lock.

** Semaforos
* Sincronizacion
** Deadlock                                                        :deadlock:
A <<<deadlock>>> is a cycle of waiting among a set of threads, where
each thread waits for some other thread in the cycle to take some
action.

Deadlock can occur in many different situations, but one of the
simplest is mutually recursive locking:

#+BEGIN_SRC 
// Thread A

lock1.acquire();
lock2.acquire();
lock2.release();
lock1.release();

// Thread B

lock2.acquire();
lock1.acquire();
lock1.release();
lock2.release();
#+END_SRC

We can also get into deadlock with two locks and a condition variable,
shown below:

#+BEGIN_SRC
// Thread A

lock1.acquire();
...
lock2.acquire();
while (need to wait) {
    cv.wait(&lock2);
}
...
lock2.release();
...
lock1.release();
// Thread B

lock1.acquire();
...
lock2.acquire();
...
cv.signal();
lock2.release();
...
lock1.release();
#+END_SRC

In <<<nested waiting>>>, one shared object calls into another shared
object while holding the first object's lock, and then waits on a
condition variable.

*** Condiciones para un Deadlock
1. Bounded resources. There are a finite number of threads that can
   simultaneously use a resource.
2. No preemption. Once a thread acquires a resource, its ownership
   cannot be revoked until the thread acts to release it.
3. Wait while holding. A thread holds one resource while waiting for
   another. This condition is sometimes called multiple independent
   requests because it occurs when a thread first acquires one
   resource and then tries to acquire another.
4. Circular waiting. There is a set of waiting threads such that each
   thread is waiting for a resource held by another.

*** Prevencion de Deadlocks
1. Exploit or limit the behavior of the program. Often, we can change
   the behavior of a program to prevent one of the four necessary
   conditions for deadlock, and thereby eliminate the possibility of
   deadlock.
2. Predict the future. If we can know what threads may or will do,
   then we can avoid deadlock by having threads wait (e.g., thread 2
   can wait at step 2 above) before they would head into a possible
   deadlock.
3. Detect and recover. Another alternative is to allow threads to
   recover or “undo” actions that take a system into a deadlock.

** Starvation
In <<<starvation>>>, a thread fails to make progress for an indefinite
period of time. Deadlock is a form of starvation but with the stronger
condition: a group of threads forms a cycle where none of the threads
make progress because each thread is waiting for some other thread in
the cycle to take action. Thus, deadlock implies starvation, but
starvation does not imply deadlock.

* Scheduling                                                     :scheduling:
When there are multiple things to do, how do you choose which one to
do first?

When there are more runnable threads than processors, the <<<processor
scheduling policy>>> determines which threads to run first.

There is no one right answer; rather, any scheduling policy poses a
complex set of tradeoffs between various desirable properties.

Our discussion also assumes the scheduler has the ability to
<<<preempt>>> the processor and give it to some other task. Preemption
can happen either because of a timer interrupt, or because some task
arrives on the ready list with a higher priority than the current
task, at least according to some scheduling policy.

** defs
A <<<workload>>> is a set of tasks for some system to perform, along
with when each task arrives and how long each task takes to
complete. In other words, the workload defines the input to a
scheduling algorithm. Given a workload, a processor scheduler decides
when each task is to be assigned the processor.

Response time

turn around time

** Policies
*** FIFO 
Do each task in the order in which it arrives.

FIFO minimizes overhead, switching between tasks only when each one
completes.

[[./img/7.1.png]]

If a task with very little work to do happens to land in
line behind a task that takes a very long time, then the system will
seem very inefficient.

*** SJF (Shortest Job First)
Suppose we could know how much time each task needed at the processor.

SJF is pessimal for variance in response time. By doing the shortest
tasks as quickly as possible, SJF necessarily does longer tasks as
slowly as possible.

SJF can suffer from starvation and frequent context switches. If
enough short tasks arrive, long tasks may never complete. Whenever a
new task on the ready list is shorter than the remaining time left on
the currently scheduled task, the scheduler will switch to the new
task. If this keeps happening indefinitely, a long task may never
finish.

*** Round Robin
addresses starvation 

tasks take turns running on the processor for a limited period of
time. The scheduler assigns the processor to the first task in the
ready list, setting a timer interrupt for some delay, called the
<<<time quantum>>>. At the end of the quantum, if the task has not
completed, the task is preempted and the processor is given to the
next task in the ready list.  The preempted task is put back on the
ready list where it can wait its next turn. With Round Robin, there is
no possibility that a task will starve.

[[./img/7.2.png]]

One consideration is overhead: if we have too short a time quantum,
the processor will spend all of its time switching and getting very
little useful work done. If we pick too long a time quantum, tasks
will have to wait a long time until they get a turn.

[[./img/7.3.png]]

Figure 7.3 illustrates what happens for FIFO, SJF, and Round
Robin when several tasks start at roughly same time and are of the
same length. Round Robin will rotate through the tasks, doing a bit of
each, finishing them all at roughly the same time.

*** Min Max Fairness

*** MLFQ (Multi-Level Feedback Queue)
Goals:
- Responsiveness. Run short tasks quickly, as in SJF.
- Low Overhead. Minimize the number of preemptions, as in FIFO, and
  minimize the time spent making scheduling decisions.
- Starvation-Freedom. All tasks should make progress, as in Round
  Robin.
- Background Tasks. Defer system maintenance tasks, such as disk
  defragmentation, so they do not interfere with user work.
- Fairness. Assign (non-background) processes approximately their
  max-min fair share of the processor.

MFQ is an extension of Round Robin. Instead of only a single queue,
MFQ has multiple Round Robin queues, each with a different priority
level and time quantum. Tasks at a higher priority level preempt lower
priority tasks, while tasks at the same level are scheduled in Round
Robin fashion.  Further, higher priority levels have shorter time
quanta than lower levels.

Tasks are moved between priority levels to favor short tasks over long
ones. A new task enters at the top priority level. Every time the task
uses up its time quantum, it drops a level; every time the task yields
the processor because it is waiting on I/O, it stays at the same level
and if the task completes it leaves the system.

[[./img/7.5.png]]

*** MLFQ Multi procesador
Exsite una MLFQ para cada procesador.

Each processor uses <<<affinity scheduling>>>: once a thread is
scheduled on a processor, it is returned to the same processor when it
is re-scheduled, maximizing cache reuse. Each processor looks at its
own copy of the queue for new work to do; this can mean that some
processors can idle while others have work waiting to be
done. Rebalancing occurs only if the queue lengths are persistent
enough to compensate for the time to reload the cache for the migrated
threads.

 
 
 
 
 
 
 
 












