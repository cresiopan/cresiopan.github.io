#+title:Normalidad y Teorema Central del Límite
* La distribución normal
** Presentación
**** Definición 1.1.
La función definida por}
\varphi (x) =}
1
\sqrt{}
2 \pi
e
−x
2
/{2}
(1)
se llama la función densidad normal{; su integral
\Phi(x) =
1
\sqrt{}
2 \pi
Z
x
−\infty
e
−t
2
/{2}
dt (2)
es la función distribución normal.
Folclore. Se sabe que la función e}
−x
2
no admite una primitiva que pueda expresarse medi
ante un número finito de funciones elementales: x
\nu
, sen(x), cos(x), a
x
, etc.... (Ver Piskunov,
N., (1983). cálculo diferencial e integral, tomo I, Mir, Moscú). Sin embargo, usando técnicas
de cambio de variables bidimensionales se puede demostrar que
R
\infty
−\infty
\varphi (x) dx = 1.
La función \Phi(x) crece desde 0 hasta 1. Su gráfico es una curva con forma de S con
\Phi(−x) = 1 − \Phi(x). (3)
2
−4 −3 −2 −1 0 1 2 3 4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
(a)
−4 −3 −2 −1 0 1 2 3 4
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(b)
Figura 1: (a) La función densidad normal \varphi(x) :=
1
\sqrt{}
2 \pi
e
−x
2
/{2}
; (b) La función distribución
normal \Phi(x) =
1
\sqrt{}
2 \pi
R
x
−\infty
e
−t
2
/{2}
dt
Tablas. La tabla de valores de la función distribución normal se puede consultar en la}
mayoría de los libros sobre probabilidad y/o estadística. En general se tabulan los valores
de \Phi(x) para x = d
0
+
d
1
10
+
d
2
100
, donde d
0
\in \{0, 1, 2, 3\} y d}
1
, d
2
\in \{0, 1, 2, \dots , 9}\. Las filas}
de la tabla están indexadas por los números d
0
. d
1
y sus columnas por los números 0.0d
2
:
en l a posición (d
0
. d
1
, 0.0}d
2
) de la tabla se encuentra el valor \Phi(d
0
. d
1
d
2
). Por ejemplo, si
se consulta la tabla del libro de Feller, W. (1968). An Introduction to Probability Theory}
and it s Applications, en fila 1.2 y columna de 0.08 puede leerse 0.8997, lo que significa que}
\Phi(1.28) = 0.8997.
En el Cuadro 1.1 reproducimos algunos de los valores de la tabla del Feller:
**** Lema 1.2. Para cada x > 0 valen las siguientes desigualdades:}
\varphi (x)

1
x
−
1
x
3

< 1 − \Phi(x) < \varphi (x)

1
x

. (4)
3
x 1.28 1.64 1.96 2.33 2.58 3.09 3.29}
\Phi(x) 0.8997 0.9495 0.975 0.9901 0.9951 0.9990 0.9995
Cuadro 1: En la tabla se muestran algunos valores de \Phi(x) :=
1
\sqrt{}
2 \pi
R
x
−\infty
e
−t
2
/{2}
dt.
**** Demostración
Usando que}
d
dx
\varphi (x) = −} x\varphi (x) es fácil ver que las derivadas de los miembros}
de las desigualdades (4) satisfacen:
d
dx

\varphi (x)

1
x
−
1
x
3

= −{\varphi}(x)

1 −}
3
x
4

.
d
dx
[1 − \Phi(x)] = −{\varphi}(x).
d
dx

\varphi (x)

1
x

= −{\varphi}(x)

1 +
1
x
2

.
Por lo tanto,
d
dx

−{\varphi (x)

1
x
−
1
x
3

<
d
dx
[\Phi(x) − 1] <}
d
dx

−{\varphi (x)

1
x

(5)
Las desigualdades (4) se obtienen integrando desde x hasta \infty}.
**** Nota Bene
De las desigualdades (4) se infiere un método de cálculo para aproximar los}
valores de 1 − \Phi(x): promediando los valores de los extremos de las desigualdades se obtiene
una aproximación cuyo error absoluto es menor que la semi-diferencia entre ambos:




1 − \Phi(x) − \varphi}(x)

1
x
−
1
2x
3





\leq
\varphi (x)
2x
3
. (6)
De la desigualdad (6) se puede ver que la aproximación
\Phi(x) \approx 1 − \varphi}(x)

1
x
−
1
2x
3

(7)
es prácticamente inútil para valores /pequeños/ de x (i.e., x \in (0, 1]) pero va mejorando a
medida que los valores de x /crecen/  . Usando la aproximación dada en (7) se obtienen las
siguientes aproximaciones
x
1.28 1.64 1.96 2.33 2.58 3.09 3.29
\Phi(x) 0.90454 0.94839 0.97406 0.98970 0.99487 0.99896 0.99948
|{error}| \leq
0.04192 0.01178 0.00388 0.00104 0.00041 0.00005 0.00002
Cuadro 2: Algunos valores de \Phi(x) obtenidos mediante la estimación (7).
Nota histórica La distribución normal fue descubierta por De Moivre en 1733 como re
sultado de analizar la forma límite de la distribución binomial simétrica y redescubierta
nuevamente por Gauss (1809) y Laplace (1812) quienes la estudiaron en relación con sus tra
bajos sobre la teoría de los errores de observación. Laplace dio, además, el primer enunciado
(incompleto) del teorema central del límite. (Ver Cramer, H., (1970). Métodos matemáticos}
de estadística , Aguilar, Madrid.)
4
** Cuentas con normales
Sean \mu \in \Re y \sigma > 0 arbitrarios, pero fijos. Se dice que la variable aleatoria X tiene
distribución normal de parámetros \mu y \sigma}
2
y se denota X \sim N}(\mu, \sigma
2
) si la función densidad
de X es de la forma
\varphi
\mu,\sigma
2
(x) =
1
\sigma
\sqrt{}
2 \pi
exp

−
(x − \mu)
2
2 \sigma
2

. (8)
**** Nota Bene
Un hecho importante sobre las variables aleatorias normales es que si X tiene}
distribución normal N(\mu, \sigma
2
), entonces
Z =}
X − \mu
\sigma
(9)
tiene distribución normal N(0, 1). En efecto,
\mathbb{P}(Z \leq z) = \mathbb{P}((X − \mu)/\sigma \leq z) = \mathbb{P}(X \leq z\sigma + \mu)
=
1
\sigma
\sqrt{}
2 \pi
Z
z\sigma{+}\mu
−\infty
exp

−
(x − \mu)
2
2 \sigma
2

dx
=
1
\sqrt{}
2 \pi
Z
z
−\infty
e
−
1
2
t
2
dt por sustitución x = t\sigma + \mu.}
Este hecho significa que si trasladamos el origen de las abscisas en \mu y cambiamos la escala
de manera tal que \sigma represente la unidad de medida, la distribución normal N(\mu, \sigma
2
) se
transforma en la distribución normal N(0, 1). Su importancia práctica radica en que permite
reducir el cálculo de probabilidades de las distribuciones normales N(\mu, \sigma
2
) al de la distribu
ción normal N(0, 1). Motivo por el cual esta última recibe el nombre de normal está ndar (o
típica). Más precisamente, si X tiene distribución normal N(\mu, \sigma
2
), su función de distribu
ción podrá reducirse a la función de distribución normal \Phi(·) definida en (2) de la siguiente
manera:
\mathbb{P}(X \leq x) = P

X − \mu
\sigma
\leq
x − \mu
\sigma

= P

Z \leq}
x − \mu
\sigma

= \Phi

x − \mu
\sigma

. (10)
La identidad (10) resume toda la información probabilísticamente relevante sobre la variable
aleatoria X \sim N}(\mu, \sigma
2
) y permite calcular (con ayuda de la tabla de la función de distribución
normal \Phi(·)) la probabilidad de que la variable X se encuentre en cualquier intervalo prefijado
de antemano:
\mathbb{P}(a < X < b) = \Phi}

b − \mu
\sigma

− \Phi}

a − \mu
\sigma

. (11)
En particular, cuando el intervalo (a, b) es simétrico con respecto a \mu, l as cantidades a y b se
pueden expresar en la forma a = \mu − \epsilon, b = \mu + \epsilon, donde \epsilon > 0, y la fórmula (11) adopta la
forma
\mathbb{P}(|X − \mu}| < \epsilon}) = \Phi}

\epsilon
\sigma

− \Phi}

−
\epsilon
\sigma

= 2\Phi

\epsilon
\sigma

− 1. (12)
5
Significado de los parámetros \mu y \sigma}
2
. La relación (9) dice que si X es una variable}
aleatoria con distribución normal de parámetros \mu y \sigma}
2
, entonces X = \sigmaZ + \mu donde Z es
una variable con distribución normal estándar. Cálculos de rutina muestran que E[Z] = 0
y V(Z) = 1, lo que permite deducir que la media y la varianza de la N(\mu, \sigma }
2
) son \mu y \sigma
2
,
respectivamente.
** Ejemplos
**** Ejemplo 1.3.
Una maquina produce ejes cuyos diámetros X tienen distribución normal de}
media \mu = 10 mm y varianza \sigma}
2
= 0.25 mm. Un eje se considera defectuoso si X < 9.5 mm.
Cuál es la probabilidad de que un eje elegido al azar resulte defectuoso?
Solución: El problema se resuelve calculando \mathbb{P}(X < 9.5). Poniendo \mu = 10 y \sigma = 0.5 en}
la fórmula (10) obtenemos \mathbb{P}(X < 9.5) = \Phi

9.5{−}10
0.5

= \Phi (−}1) = 0.1587.
Curva peligrosa. De inmediato podría surgir una objeción al uso de la distribución nor
mal N(10, 0.25) para modelar el diámetro de los ejes. Al fin y al cabo, los diámetros deben
ser positivos y la distribución normal adopta valores positivos y negativos. Sin embargo, el
modelo anterior asigna una probabilidad despreciable al evento X < 0. En efecto,\mathbb{P}(X < 0) =
P

X{−{10
0.5
<
0{−}10
0.5

= \mathbb{P}(Z < −} 20) = \Phi (−}20) = 1 − \Phi(20). De acuerdo con la estimación (6)
tenemos que 1 −}\Phi(20) \approx \varphi}(20)

1
20
−
1
2{·}20
3

= O(10
−{89}
). Este tipo de situación es habitual en
la práctica. Se tiene una variable aleatoria X de la que se sabe que no puede tomar valores
negativos (p.ej. una distancia, una longitud, un área, un peso, una temperatura, un precio,
etc.) y se la modela utilizando una distribución normal N(\mu, \sigma
2
); motivados, por ejemplo,
por cuestiones de simetría. En principio, el modelo podrá ser perfectamente válido sie mpre
y cuando los valores de los parámetros \mu y \sigma}
2
sean tales que la probabilidad \mathbb{P}(X < 0) sea
prácticamente 0.
**** Nota Bene sobre grandes desvíos. Sea X una variable aleatoria con distribución normal}
de media \mu y varianza \sigma}
2
. Sea t > 0, utilizando la fórmula (12) podemos ver que
p
t
:= \mathbb{P}(|X − \mu}| > t\sigma}) = 1 − \mathbb{P}(|X − \mu}| \leq t\sigma}) = 1 −}

2\Phi

t\sigma
\sigma

− 1}

= 2 (1 − \Phi (t)) .
Usando la tabla de la distribución normal \Phi(·) se puede ver que p
1
= 0.3174, p
2
= 0.0454,
p
3
= 0.0028. Estos probabilidades admiten la siguiente interpretación: cerca del 32 % de los
valores de una variable X \sim N}(\mu, \sigma
2
) se desvían de su media en más de \sigma}; solamente cerca
de un 5 % lo hacen en más de 2{\sigma y solamente cerca de un 3 % en más de 3 \sigma . Esto da lugar
a que en la mayor parte de los problemas de la práctica se consideren casi imposibles las
desviaciones respecto de la media \mu que superen 3{\sigma y se consideren limitados por el intervalo
[\mu − 3{\sigma, \mu + 3 \sigma ] todos los valores prácticamente p osibles de la variable X.
**** Ejemplo 1.4.
Sea X una variable aleatoria con distribución normal de media \mu = 3 y}
varianza \sigma}
2
= 4. ¿Cuál es la probabilidad de que X sea no menor que 1 y no mayor que 7?
Solución: Poner \mu = 3 y \sigma = 2 en la fórmula (11) y usar la tabla de la distribución normal}
\Phi(·): \mathbb{P}(1 \leq X \leq 7) = \Phi

7{−}3
2

− \Phi}

1{−}3
2

= \Phi(2) − \Phi(−}1) = 0.9773 − 0.1587 = 0.8186.
6
** Suma de normales independientes
**** Lema 1.5. Sean X}
1
y X_2
dos variables aleatorias independientes con distribución nor
mal N(\mu
1
, \sigma
2
1
) y N}(\mu
2
, \sigma
2
2
), respectivamente. Entonces X}
1
+ X_2
tiene distribución normal
N

\mu
1
+ \mu}
2
, \sigma
2
1
+ \sigma}
2
2

.
**** Demostración
Observando que X}
1
+ X_2
= (X_1
− \mu
1
) + (X_2
− \mu
2
) + \mu}
1
+ \mu}
2
el problema se
reduce a considerar el caso \mu}
1
= \mu}
2
= 0. La prueba se obtiene mostrando que la convolución de
las densidades f
1
(x
1
) =
1
\sqrt{}
2{\pi\sigma}
1
exp

−x
2
1
/{2}\sigma
2
1

y f
2
(x
2
) =
1
\sqrt{}
2{\pi\sigma}
2
exp

−x
2
2
/{2}\sigma
2
2

es la densidad
normal de media \mu}
1
+ \mu}
2
y varianza \sigma}
2
= \sigma}
2
1
+ \sigma}
2
2
. Por definición
(f
1
∗ f
2
)(x) =
Z
\infty
−\infty
f
1
(x − y)f
2
(y) =
1
2{\pi\sigma}
1
\sigma
2
Z
\infty
−\infty
exp

−
(x − y)
2
2 \sigma
2
1
−
y
2
2 \sigma
2
2

dy (13)
El resultado se obtendrá mediante un poco de álgebra, bastante paciencia, y un cambio de
variables en la integral del lado derecho de la identidad (13).
exp

−
(x − y)
2
2 \sigma
2
1
−
y
2
2 \sigma
2
2

= exp
−
1
2

\sigma
\sigma
1
\sigma
2
y −}
\sigma
2
\sigma\sigma
1
x

2
−
x
2
2 \sigma
2
!
= exp
−
1
2

\sigma
\sigma
1
\sigma
2
y −}
\sigma
2
\sigma\sigma
1
x

2
!
exp

−
x
2
2 \sigma
2

La primera igualdad se obtuvo completando cuadrados respecto de y en la expresión −}
(x{−}y)
2
2 \sigma
2
1
−
y
2
2 \sigma
2
2
y reagrupando algunos términos. Mediante el cambio de variables z =
\sigma
\sigma
1
\sigma
2
y −}
\sigma
2
\sigma\sigma
1
x, cuya}
diferencial es de la forma dz =
\sigma
\sigma
1
\sigma
2
dy, se puede ver que}
(f
1
∗ f
2
)(x) =
1
2{\pi\sigma}
exp

−
x
2
2 \sigma
2

Z
\infty
−\infty
exp

−
z
2
2

dz =}
1
\sqrt{}
2{\pi \sigma}
exp

−
x
2
2 \sigma
2

.
Este resultado se puede generalizar para una suma de n variables aleatorias independientes:
Sean X_1
, X_2
, \dots , X
n
variables aleatorias independientes con distribuciones normales: X
i
\sim
N(\mu }
i
, \sigma
2
i
), 1 \leq i \leq n. Entonces,
n
X
{i=1}
X
i
\sim N
n
X
{i=1}
\mu
i
,
n
X
{i=1}
\sigma
2
i
!
.
La prueba se obtiene por inducción y utilizando la siguiente propiedad /hereditaria/ de
familias de variables aleatorias independientes (cuya prueba puede verse en el Capítulo 1
del libro de Durrett, R.(1996): Probability Theory and Examples): Si X}
1
, X_2
, \dots , X
n
son
variables aleatorias independientes, entonces funciones (medibles) de familias disjunta s de las
X
i
también son independientes.
**** Nota Bene
Observando que para cada a \in \Re y X \sim N}(\mu, \sigma
2
) resulta que aX \sim N}(a\mu, a}
2
\sigma
2
)
se obtiene el siguiente resultado:
7
**** Teorema 1.6. Sean X}
1
, X_2
, \dots , X
n
variables aleato rias independientes con distribuciones
normales: X
i
\sim N(\mu }
i
, \sigma
2
i
), 1 \leq i \leq n y sean a
1
, a
2
, \dots , a
n
números reales cualesquiera.
Entonces,
n
X
{i=1}
a
i
X
i
\sim N
n
X
{i=1}
a
i
\mu
i
,
n
X
{i=1}
a
2
1
\sigma
2
i
!
.
* Génesis de la distribución normal
** Teorema límite de De Moivre - Laplace
En 1733, De Moivre observó que la distribución binomial correspondiente a la cantidad
de é xitos, S_n
, en n ensayos de Bernoulli simétricos tiene la forma límite de una campana.
Esta observación fue la clave que le permitió descubrir la famosa campana de Gauss y allanar
el camino que lo condujo a establecer la primera versión del Teorema Ce ntral del Límite{: la
convergencia de la distribución Binomial(n, 1 / 2) a la distribución normal estándar. En 1801,
Laplace refinó y generalizó este resultado al caso de la distribución Binomial(n, p). El Teorema
de De Moivre-Laplace, que enunciamos más abajo, mejora sustancialmente la Ley débil de los
grandes números porque proporciona una estimación mucho más precisa de las probabilidades
P

|
S_n
n
− p| \leq \epsilon}

.
0 2 4 6 8 10 12 14 16
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
Figura 2: Relación entre la distribución Binomial simétrica y la distribución normal. La prob
abilidad de que ocurran k éxitos en n ensayos de Bernoulli está representada por un segmento
paralelo al eje de las abscisas localizado en la ordenada k de al tura igual a \mathbb{P}(S_n
= k). La curva
continua /aproxima/ los valores de \mathbb{P}(S_n
= k). Observar que dichas probabilidades también
se pueden representar como áreas de re ctángulos de altura \mathbb{P}(S_n
= k) y de base unitaria
centrada en k.
8
**** Teorema 2.1 (Teorema límite de De Moivre-Laplace). Consideramos una sucesión de en-
sayos de Bernoulli independi entes . Sean p la probabilidad de éxito en cada ensayo y S_n
la
cantida d de é xitos observados en los primeros n ensayos. Para cualqu ier x \in \Re vale que
\lim_{n  \rightarrow \infty}
P
S_n
− np}
p
np(1 − p)
\leq x
!
= \Phi(x), (14)
donde \Phi(x) :=}
R
x
−\infty
1
\sqrt{}
2 \pi
e
−t
2
/{2}
dt es la función distribución normal estándar.
**** Demostración
Ver Capítulo VII de Feller, W., (1971). An Introduction to Probability
Theory a nd Its Applications, Vol. I, John Wiley & Sons, New York.
¿Qué significa el Teorema Límite de De Moivre-Laplace? Para c ontestar esta pre
gunta vamos a reconstruir las ideas principales de su génesis. En otras palabras, vamos a
(re)construir el Teorema. La clave de la construcción está /embutida/ en la Figura 2. La im
agen permite /capturar de inmediato/ la existencia de una forma lí mite para la distribución
Binomial en el caso simétrico p = 1 / 2.
Paso 1. El primer paso en la dirección del Teorema de De Moivre consiste en darse cuenta}
que la Figura 2 señala la existencia de una forma límite. En una primera fase (completa
mente abstracta) podemos conjeturar que /"{la distribuc ión binomial simétrica tiene una forma}
asintótica . En otras palabras, cuando la cantidad de ensayos de Bernoulli es suficientemente
grande, salvo traslaciones y cambios de escala apropiados, la distribución Binomial se parece
a una función continua par, \varphi (x) , cuyo gráfico tiene la forma de una campana.{''}
Paso 2. El segundo paso consiste en precisar la naturaleza de la traslación y los cambios de}
escala que permiten /capturar/ esa forma límite. Si se reﬂexiona sobre el significado de la
media y la varianza de una variable aleatoria, parece claro que la forma límite se obtendrá cen
trando la variable S_n
en su valor medio, E[S_n
] =
1
2
n, y adoptando como unidad de medida}
la desviación típica de los valores observados respecto de dicho valor, \sigma(S_n
) =
1
2
\sqrt{}
n. El sig
nificado geométrico de esta transformación consiste en (1) trasladar el origen de las abscisas
en
1
2
n y (2) dividirlas por}
1
2
\sqrt{}
n. Para que las áreas de los rectángulos sigan representando}
probabilidades, las ordenadas deben multiplicarse p or el mismo número. Este paso permite
enunciar la siguiente versión mejorada de la conjetura inicial: /"{existe una función continua}
\varphi (x) tal que
\mathbb{P}(S_n
= k) =

n
k

1
2

n
\sim
1
1
2
\sqrt{}
n
\varphi
k −}
1
2
n
1
2
\sqrt{}
n
!
, (15)
siempre y cuando n sea su ficienteme nte grande.{''}
Paso 3. Establecida la conjetura el problema consiste en /descubrir/ la expresión de la función}
\varphi (x) y en precisar cuál es el sentido de la relación aproximada que aparece en (15). En este}
punto no queda otra que /arremangarse y meter la mano en el barro/  . Como resultado se
obtiene que la expresión de la función \varphi(x) es
\varphi (x) =}
1
\sqrt{}
2 \pi
exp

−
x
2
2

y que la relación \sim vale para valores de k del orden de
\sqrt{}
n y significa que el cociente de los}
dos lados tiende a 1 cuando n \rightarrow \infty} .
9
\hypertarget{pfa}
**** Nota Bene
La relación (15) expresa matemáticamente un hecho que se observa claramente}
en la Figura 2: la campana /pasa/ por los puntos de base k y altura \mathbb{P}(S_n
= k). Conviene
observar que la expresión que aparece en el lado derecho de la relación (15) es la función
de densidad de la normal N}

1
2
n,
1
4
n

evaluada en x = k. En la práctica, esto significa que
para obtener una buena aproximación de la probabilidad de observar k éxitos en n ensayos de
Bernoulli independientes, basta con evaluar la densidad de la normal N}

1
2
n,
1
4
n

en x = k.
Sin temor a equivocarnos, podemos resumir estas observaciones mediante una expresión de
la forma S_n
\sim N (E[S_n
], V}(S_n
)).
Paso 4. Observar que para cada x}
1
< x
2
vale que
P
x
1
\leq
S_n
−
1
2
n
1
2
\sqrt{}
n
\leq x
2
!
= P

1
2
n + x
1
1
2
\sqrt{}
n \leq S_n
\leq
1
2
n + x
2
1
2
\sqrt{}
n

=
X
x
1
1
2
\sqrt{}
n{\leq}j{\leq}x
2
1
2
\sqrt{}
n
P

S_n
=
1
2
n + j

\approx
X
x
1
\leq{jh}\leqx
2
h\varphi(jh) , (16)
donde h =
2
\sqrt{}
n
y la suma se realiza sobre todos los enteros j tales que x
1
\leq jh \leq x
2
. Cada
uno de los sumandos que aparecen en el lado derecho de la aproximación (16) es el área de
un rectángulo de base [kh, (k + 1)h y altura \varphi(kh). Como la función \varphi(·) es continua, para
valores pequeños de h la suma total de las áreas de los rectángulo debe estar próxima del área
bajo la curva de la densidad normal entre x
1
y x
2
. Por lo tanto, debe valer lo siguiente
\lim_{n  \rightarrow \infty}
P
x
1
\leq
S_n
−
1
2
n
1
2
\sqrt{}
n
\leq x
2
!
=
Z
x
2
x
1
\varphi (t) dt = \Phi(x
2
) − \Phi(x
1
). (17)
Este paso puede hacerse formalmente preciso /arremangandose y metiendo la mano en .../
**** Nota Bene
La variable aleatoria que aparece dentro de la probabilidad del l ado izquierdo}
de (17)
S
∗
n
=
S_n
−
1
2
n
1
2
\sqrt{}
n
=
S_n
− E[S
n
]
\sigma (S_n
)
(18)
es una medida de la desviación de S_n
respecto de la media E[S_n
] en unidades de la desviación
típica \sigma(S_n
). El teorema límite de De Moivre-Laplace significa que cuando se considera una
cantidad n (suficientemente grande) de ensayos de Bernoulli independientes, la distribución de
la variable aleatoria S}
∗
n
es /prácticamente indistinguible/ de la distribución normal estándar
N(0, 1).
Comentario sobre prueba del Teorema 2.1. Si se sigue con cuidado la demostración}
presentada por Feller se puede ver que las herramientas principales de la prueba son el desar
rollo de Taylor (1712) de la función log(1 + t) = t + O(t
2
) y la fórmula asintótica de Stirling
(1730) para los números factoriales n! \sim}
\sqrt{}
2{\pin n}
n
e
−n
. Partiendo de la función de probabilidad
de la Binomial(n, 1 / 2) se /deduce/ la expresión de la función densidad normal (
\sqrt{}
2 \pi)
−{1}
e
−x
2
/{2}
:
el factor (
\sqrt{}
2 \pi)
−{1}
proviene de la fórmula de Stirling y el factor e
−x
2
/{2}
del desarrollo de Tay
lor. Dejando de lado los recursos técnicos utilizados en la prueba, se observa que las ideas
involucradas son simples y /recorren el camino del descubrimiento/ de De Moivre (1733).
10
\hypertarget{pfb}
**** Ejemplo 2.2.
Se lanza 40 veces una moneda honesta. Hallar la probabilidad de que se}
obtengan exactamente 20 caras. Usar l a aproximación normal y compararla con la solución
exacta.
Solución: La cantidad de caras en 40 lanzamientos de una moneda honesta, S
40
, es una
variable Binomial de parámetros n = 40 y p = 1 / 2. La aproximación normal (15) establece
que
\mathbb{P}(S
40
= 20) \sim}
1
1
2
\sqrt{}
40
\varphi(0) =}
1
\sqrt{}
20 \pi
= 0.12615\dots
El resultado exacto es
\mathbb{P}(X = 20) =}

40
20

1
2

40
= 0.12537\dots
**** Ejemplo 2.3.
Se dice que los recién nacidos de madres fumadoras tienden a ser más pequeños}
y propensos a una variedad de dolencias. Se conjetura que además parecen deformes. A un
grupo de enfermeras se les mostró una selección de fotografías de bebés, la mitad de los
cuales nacieron de madres fumadoras; las enfermeras fueron invitadas a juzgar a partir de la
apariencia de cada uno si la madre era fumadora o no. En 1500 ensayos se obtuvieron 910
respuestas correctas. La conjetura es plausible?
Solución: Aunque superficial, un argumento atendible consiste en afirmar que, si todos los}
bebés parecen iguales, la cantidad de repuestas correctas S_n
en n ensayos es una variable
aleatoria con distribución Binomial (n, 1 / 2). Entonces, para n grande
P
S_n
−
1
2
n
1
2
\sqrt{}
n
> 3}
!
= 1 − P}
S_n
−
1
2
n
1
2
\sqrt{}
n
\leq 3}
!
\approx 1 − \Phi(3) \approx
1
1000
por el Teorema límite de De Moivre-Laplace. Para los valores dados de S_n
,
S_n
−
1
2
n
1
2
\sqrt{}
n
=
910 − 750
5
\sqrt{}
15
\approx 8.
Se podría decir que el evento \{X −}
1
2
n >
3
2
\sqrt{}
n{\} es tan improbable que su ocurrencia arroja
dudas sobre la suposición origi nal de que los bebés parecen iguales. Este argumento otorgaría
cierto grado de credibilidad a la c onjetura enunciada.
Comentarios sobre el caso general
1. En el caso general, la probabilidad de éxito en cada ensayo de Bernoulli individual es}
p \in (0}, 1). Si S_n
es la cantidad de éxitos observados en los primeros n ensayos, entonces
E[S_n
] = np y V(S_n
) = np(1 − p). Por lo tanto, la variable aleatoria
S
∗
n
:=
S_n
− np}
p
np(1 − p)
(19)
es una medida de la desviación de S_n
respecto de la media E[S_n
] = np en unidades de la
desviación típica \sigma(S_n
) =
p
np(1 − p). El teorema límite de De Moivre-Laplace significa}
11
\hypertarget{pfc}
que cuando se considera una cantidad n (suficientemente grande) de ensayos de Bernoulli
independientes, la distribución de la variable aleatoria S}
∗
n
es /prácticamente indistinguible/
de la distribución normal estándar N(0, 1).
2. Técnicamente la prueba del teorema se puede hacer recurriendo a las mismas herramientas}
utilizadas en la prueba del caso simétrico, pero los cálculos involucrados son más complica
dos. Sin embargo, el resultado también es claro si se observan las gráficas de la distribución
Binomial(n, p). En la Figura 3 se ilustra el caso n = 16 y p = 1 / 4. Nuevamente es /evidente/
que la forma límite de distribución Binomial debe ser la distribución normal.
0 2 4 6 8 10 12 14 16
0
0.05
0.1
0.15
0.2
Figura 3: Gráfica de la función de probabilidad binomial con n = 16 y p = 1 / 4. Cerca
del término central m = np = 4, salvo un cambio de escala (cuya unidad de medida es
p
np(1 − p) =}
\sqrt{}
3) la gráfica es /indistinguible/ de la gráfica de la densidad normal.
3. De la Figura 3 debería estar claro que, para n suficientemente grande, debe valer lo siguiente}
\mathbb{P}(S_n
= k) =

n
k

p
k
(1 − p)
n{−}k
\sim
1
p
np(1 − p)
\varphi
k − np
p
np(1 − p)
!
. (20)
**** Ejemplo 2.4.
Para el caso ilustrado en la Figura 3: n = 16 y p = 1 / 4, la aproximación (20)
es bastante buena, incluso con un valor de n peque ño. Para k = 0, \dots 4 las probabilidades
\mathbb{P}(S_n
= 4+k) son 0.2252, 0.1802, 0.1101, 0.0524, 0.0197. Las aproximaciones correspondientes
son 0.2303, 0.1950, 0.1183, 0.0514, 0.0160.
**** Nota Bene
El Teorema límite de De Moivre-Laplace justifica el uso de los métodos de la}
curva normal para aproximar probabilidades relacionadas con ensayos de Bernoulli con prob
abilidad de éxito p. La experiencia /indica/  que la aproximación es bastante buena siempre
que np > 5 cuando p \leq 1 / 2, y n(1 − p) cuando p > 1 / 2. Un valor muy pequeño de p junto
con un valor de n moderado darán lugar a una media pequeña y con ello se obtendrá una
12
\hypertarget{pfd}
distribución asimétrica. La mayor parte de la distribución se acumulará alrededor de 0, im
pidiendo con ello que una curva normal se le ajuste bien. Si la media se aparta por lo menos
5 unidades de una y otra extremidad, la distribución tiene suficiente espacio para que resulte
bastante simétrica. (Ver la Figura 4).
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
(a)
0 1 2 3 4 5 6 7 8 9 10
0
0.1
0.2
0.3
0.4
0.5
(b)
0 1 2 3 4 5 6 7 8 9 10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
(c)
0 1 2 3 4 5 6 7 8 9 10
0
0.05
0.1
0.15
0.2
0.25
0.3
(d)
0 1 2 3 4 5 6 7 8 9 10
0
0.05
0.1
0.15
0.2
0.25
(e)
0 1 2 3 4 5 6 7 8 9 10
0
0.05
0.1
0.15
0.2
0.25
(f)
Figura 4: Comparación entre la distribución Binomial(10, p) y su aproximación por la normal
para distintos valores de p (a) p = 0.025; (b) p = 0.05; (c) p = 0.1; (d) p = 0.2; (e) p = 0.4;
(f) p = 0.5.
**** Ejemplo 2.5 (Encuesta electoral). Queremos estimar la proporción del electorado que pre
tende votar a un cierto candidato. Para ello consideramos que el voto de cada elector tiene
una distribución Bernoulli de parámetro p. Concretamente, queremos encontrar un tamaño
muestral n suficiente para que con una certeza del 99.99 % podamos garantizar un error máxi
mo de 0.02 entre el verdadero valor de p y la proporción muestral S_n
/n. En otras palabras,}
queremos encontrar n tal que
P





S_n
n
− p




\leq 0.02}

\geq 0.9999. (21)
Para acotar la incerteza usaremos la aproximación por la normal provista por el teorema límite
de De Moivre - Laplace. Para ello, en lugar de observar la variable S_n
, debemos observar la
variable normalizada S}
∗
n
:= (S_n
− np) /}
p
np(1 − p). En primer lugar observamos que, como}
consecuencia del teorema límite, tenemos la siguiente aproximación
P





S_n
− np}
p
np(1 − p)





\leq a
!
\approx \Phi(−{a) − \Phi(a) = 2\Phi(a) − 1 (22)
13
\hypertarget{pfe}
o lo que es equivalente
P




S_n
n
− p




\leq
a
p
p(1 − p)
\sqrt{}
n
!
\approx 2\Phi(a) − 1. (23)
Como el verdadero valor de p es desconocido, la fórmula (23) no puede aplicarse directamente
ya que no se conoce el valor de
p
p(1 − p). Sin embargo, es fácil ver que}
p
p(1 − p) \leq 1}/{2 y}
por lo tanto
P





S_n
n
− p




\leq
a
2
\sqrt{}
n

\geq P




S_n
n
− p




\leq
a
p
p(1 − p)
\sqrt{}
n
!
\approx 2\Phi(a) − 1. (24)
Esta última relación es la herramienta con la que podemos resolver nuestro problema.
En primer lugar tenemos que resolver la ecuación 2\Phi(a) − 1 = 0.9999 o la ecuación
equivalente \Phi(a) =
1.9999
2
= 0.99995. La solución de está ecuación se obtiene consultando una
tabla de la distribución normal: a = 3.9. Reemplazando este valor de a en (24) obtenemos
P





S_n
n
− p




\leq
3.9
2
\sqrt{}
n

\geq 0.9999.
En segundo lugar tenemos que encontrar los valores de n que satisfacen la desigualdad
3.9
2
\sqrt{}
n
\leq 0.02. (25)
Es fácil ver que n satisface la desigualdad (25) si y solo si
n \geq}

3.9
0.04

2
= (97.5)
2
= 9506.2
El problema está resuelto.
* Teorema central del límite
Los teoremas sobre normalidad asintótica de sumas de variables aleatorias se llaman Teo
remas Centrales del Límite. El Teorema límite de De Moivre - Laplace es un Teorema Central
del Límite para variables aleatorias independientes con distribución Bernoulli(p). Una versión
más general es la siguiente:
**** Teorema 3.1 (Teorema Central del Límite). Sea X
1
, X_2
, \dots una sucesión de variables aleato
rias independientes idénticamente distribuidas, cada una con media \mu y varianza \sigma}
2
. Entonces
la distribución de
P
n
{i=1}
X
i
− n\mu}
\sigma
\sqrt{}
n
tiende a la normal estándar cuando n \rightarrow \infty} . Esto es,
\lim_{n  \rightarrow \infty}
P

P
n
{i=1}
X
i
− n\mu}
\sigma
\sqrt{}
n
\leq x

= \Phi(x),
donde \Phi(x) :=}
R
x
−\infty
1
\sqrt{}
2 \pi
e
−t
2
/{2}
dt es la función de distribución de una normal de media 0 y}
varianza 1}.
14
\hypertarget{pff}
**** Demostración
Ver Capítulo XV de Feller, W., (1971). An Introduction to Probability
Theory a nd Its Applications, Vol. II, John Wiley & Sons, New York.
**** Corolario 3.2. Sea X}
1
, X_2
, \dots una sucesión de variables aleatorias independientes idénti
camente distribuidas, cada una con media \mu y varianza \sigma}
2
. Si n es suficientemente grande,
para cada valor a > 0 vale la siguiente aproximación
P





1
n
n
X
{i=1}
X
i
− \mu}





\leq a
\sigma
\sqrt{}
n
!
\approx 2\Phi(a) − 1 (26)
**** Demostración
El teorema central del límite establece que si n es suficientemente grande,}
entonces para c ada x \in \Re vale que
P

P
n
{i=1}
X
i
− n\mu}
\sigma
\sqrt{}
n
\leq x

\approx \Phi(x) (27)
De la aproximación (27) se deduce que para cada valor a > 0
P





P
n
{i=1}
X
i
− n\mu}
\sigma
\sqrt{}
n




\leq a

\approx \Phi(a) − \Phi(−{a) = 2\Phi(a) − 1. (28)
El resultado se obtiene de (28) observando que




P
n
{i=1}
X
i
− n\mu}
\sigma
\sqrt{}
n




=
n
\sigma
\sqrt{}
n





1
n
n
X
{i=1}
X
i
− \mu}





=
\sqrt{}
n
\sigma





1
n
n
X
{i=1}
X
i
− \mu}





. (29)
**** Nota Bene
Para los usos prácticos, especialmente en estadística, el resultado límite en}
sí mismo no es de interés primordial. Lo que interesa es usarlo como una aproximación con
valores finitos de n. Aunque no es posible dar un enunciado consiso sobre cuan buena es la
aproximación, se pueden dar algunas pautas generales y examinando algunos casos especiales
se puede tener alguna idea más precisa del comportamiento de cuan buena es la aproximación.
Qué tan rápido la aproximación es buena depende de la distribución de los sumandos. Si
la distribución es bastante simétrica y sus colas decaen rápidamente, la aproximación es
buena para valores relativamente pequeños de n. Si la distribución es muy asimétrica o si
sus col as decaen muy lentamente, se necesitan valores grandes de n para obtener una buena
aproximación.
** Ejemplos
**** Ejemplo 3.3 (Suma de uniformes). Puesto que la distribución uniforme sobre}

−
1
2
,
1
2

tiene
media 0 y varianza
1
12
, la suma de 12 variables independientes U}

−
1
2
,
1
2

tiene media 0 y
varianza 1. La distribución de esa suma está muy cerca de la normal.
**** Ejemplo 3.4.
Para simplificar el cálculo de una suma se redondean todos los números al}
entero más cercano. Si el error de redondeo se puede representar como una variable aleatoria
U

−
1
2
,
1
2

y se suman 12 números, ¿cuál es la probabilidad de que el error de redondeo exceda
1?
15
−4 −3 −2 −1 0 1 2 3 4
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
(a)
−3 −2 −1 0 1 2 3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
(b)
Figura 5: (a) Comparación entre un histograma de 1000 valores, cada uno de l os cuales es la
suma de 12 variables uniformes U}

−
1
2
,
1
2

, y la función densidad normal; (b) Comparación
entre la función de distribución empírica correspondiente a 1000 valores de la suma de 12
uniformes U}

−
1
2
,
1
2

y la función de distribución normal. El ajuste es sorprendentemente
bueno, especialmente si se tiene en cuenta que 12 no se considera un número muy grande.
Solución: El error de redondeo cometido al sumar 12 números se representa por la suma}
P
12
{i=1}
X
i
de 12 variables aleatorias independientes X_1
, \dots , X_12
cada una con distribución uni
forme sobre el intervalo

−
1
2
,
1
2

. El error de r edondeo excede 1 si y solamente si



P
12
{i=1}
X
i



> 1.
Puesto que E[X
i
] = 0 y V(X
i
) =
1
12
de acuerdo con el teorema central del límite tenemos que
la distribución de
P
12
{i=1}
X
i
− 12{E[X}
i
]
p
12{V(X
i
)
=
12
X
{i=1}
X
i
se puede aproximar por la distribución normal estándar. En consecuencia,
P





12
X
{i=1}
X
i





> 1}
!
= 1 − P}





12
X
{i=1}
X
i





\leq 1}
!
\approx 1 − (\Phi(1) − \Phi(−{1))
= 1 − (2\Phi(1) − 1) = 2 − 2\Phi(1) = 0.3173\dots
16
**** Ejemplo 3.5 (Suma de exponenciales). La suma S_n
de n variables aleatorias independientes
exponenciales de intensidad \lambda = 1 obedece a una distribución gamma, S_n
\sim \Gamma(n, 1). En la}
siguiente figura se comparan, para distintos valores de n, la función de distribución de la suma
estandarizada
S_n
−{E[S
n
]
\sqrt{}
V(S_n
)
con la función de distribución normal estándar.
−3 −2 −1 0 1 2 3
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Figura 6: La normal estándar (sólida) y las funciones de distribución de las variables \Gamma(n, 1)
estandarizadas para n = 5 (punteada), n = 10 (quebrada y punteada) y n = 30 (quebrada).
**** Ejemplo 3.6.
La distribución de Poisson de media \lambda se puede aproximar por la normal para}
valores grandes de \lambda}: si N \sim Poisson(\lambda), entonces
N − \lambda
\sqrt{}
\lambda
\approx N(0, 1).
**** Ejemplo 3.7.
Si la emisi ón de una cierta clase de partículas obedece a un proceso de Poisson}
de intensidad 900 por hora, ¿cuál es la probabilidad de que se emitan más de 950 partículas
en una hora determinada?
Solución: Sea N una variable Poisson de media 900. Calculamos \mathbb{P}(N > 950) estandarizan
do
\mathbb{P}(N > 950) = P

N − 900
\sqrt{}
900
>
950 − 900
\sqrt{}
900

\approx 1 − \Phi}

5
3

= 0.04779.
**** Ejemplo 3.8.
El tiempo de vida de una batería es una variable aleatoria de media 40 horas}
y desvío 20 horas. Una batería se usa hasta que falla, momento en el cual se la reemplaza por
17
una nueva. Suponiendo que se dispone de un stock de 25 baterías, cuyos tiempos de vida son
independientes, aproximar la probabilidad de que pueda obtenerse un uso superior a las 1100
horas.
Solución: Si ponemos X}
i
para denotar el tiempo de vida de la i-ésima batería puesta en
uso, lo que buscamos es el valor de p = \mathbb{P}(X_1
+ \cdots + X_25
> 1000), que puede aproximarse de}
la siguiente manera:
p = P}
P
25
{i=1}
X
i
− 1000}
20
\sqrt{}
25
>
1100 − 1000
20
\sqrt{}
25
!
\approx 1 − \Phi(1) = 0.1587.
**** Ejemplo 3.9.
El peso W (en toneladas) que puede re sistir un puente sin sufrir daños es
tructurales es una variable aleatoria con distribución normal de media 1400 y desvío 100. El
peso (en toneladas) de cada camión de are na es una variable aleatoria de media 22 y desvío
0.25. Calcular la probabilidad de que ocurran daños estructurales cuando hay 64 camiones de
arena sobre el tablero del puente.
Solución: Ocurren daños estructurales cuando la suma de los pesos de los 64 camiones,}
X_1
, \dots , X
64
, supera al peso W . Por el teorema central del límite, la distribución de la suma
P
64
{i=1}
X
i
es aproximadamente una normal de media 1408 y desvío 2. En consecuencia, W −
P
64
{i=1}
X
i
se distribuye (aproximadamente) como una normal de media 1400 − 1408 = −}8 y
varianza 10000 + 4 = 10004. Por lo tanto,
P
64
X
{i=1}
X
i
> W
!
= P
W −}
64
X
{i=1}
X
i
< 0}
!
= P
W −}
P
64
{i=1}
X
i
+ 8
\sqrt{}
10004
<
8
\sqrt{}
10004
!
\approx \Phi(0.07998\dots) = 0.5318\dots}
**** Ejercicios adicionales
1. Un astronauta deberá permanecer 435 días en el espacio y tiene que optar entre dos}
alternativas. Utilizar 36 tanques de oxígeno de tipo A o 49 tanques de oxigeno de tipo B.
Cada tanque de oxígeno de tipo A tiene un rendimiento de media 12 días y desvío 1 / 4. Cada
tanque de oxígeno de tipo B tiene un rendimiento de media de 8, 75 días y desvío 25 / 28.
¿Qué alternativa es la más conveniente?
2. 432 números se redondean al entero más cercano y se suman. Suponiendo que los errores}
individuales de r edondeo se distribuyen uniformemente sobre el i ntervalo (−}0.5, 0.5), aproxi
mar la probabilidad de que la suma de los números redondeados difiera de la suma exacta en
más de 6.
3. Dos aerolíneas $A$ y $B$ que ofrecen idéntico servicio para viajar de Buenos Aires a San Pablo}
compiten por la misma población de 400 clientes, cada uno de los cuales elige una aerolínea
al azar. ¿Cuál es la probabilidad de que la línea A tenga más clientes que sus 210 asientos?
18
* Distribuciones relacionadas con la Normal
En esta sección se presentan tres distribuciones de probabilidad relacionadas con la dis
tribución normal: las distribuciones \Chi}
2
, t y F . Esas distribuciones aparecen en muchos prob
lemas estadísticos.
** \Chi^2 (chi-cuadrado)
**** Definición 4.1 (Distribución chi-cuadrado con un grado de libertad). Si Z es una una vari
able aleatoria con distribución normal estándar, la distribución de U = Z}
2
se llama la dis
tribución chi-cuadrado con 1 grado de libertad.
0 1 2 3 4 5
0
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
Figura 7: Gráfico de la función densidad de probabilidad de la distribución \Chi}
2
1
.
Caracterización de la distribución \Chi}
2
1
. La función de distribución de la variable U = Z
2
es F}
U
(u) = \mathbb{P}(Z}
2
\leq u), donde Z es N(0, 1). Para cada u > 0, vale que}
F(x) = \mathbb{P}(Z
2
\leq u) = \mathbb{P}(}|{Z}| \leq
\sqrt{}
u) = \mathbb{P}(−
\sqrt{}
u \leq Z \leq}
\sqrt{}
u) =}
Z
\sqrt{}
u
−
\sqrt{}
u
1
\sqrt{}
2 \pi
e
−z
2
/{2}
dz.
Usando el teorema fundamental del cálculo integral y la regla de la cadena obtenemos que
para cada u > 0 vale que
f
U
(u) =
d
du
F
U
(u) =
d
du
Z
\sqrt{}
u
−
\sqrt{}
u
1
\sqrt{}
2 \pi
e
−z
2
/{2}
dz
=
1
\sqrt{}
2 \pi

e
− (
\sqrt{}
u)
2
/{2}
d
du
(
\sqrt{}
u) − e
− (−
\sqrt{}
u)
2
/{2}
d
du
(−}
\sqrt{}
u)

=
1
\sqrt{}
2 \pi

e
−{u/{2
1
2
\sqrt{}
u
+ e
−{u/{2
1
2
\sqrt{}
u

=
1
\sqrt{}
2 \pi

e
−{u/{2
1
\sqrt{}
u

=
(1 / 2)
1
2
\sqrt{}
\pi

u
−{1 / 2}
e
−(1 / 2)u}

=
(1 / 2)
1
2
\sqrt{}
\pi
u
1
2
−{1}
e
−(1 / 2)u}
. (30)
19
La última expresión que aparece en el lado derecho de la identidad (30) es la expresión de la
densidad de la distribución \Gamma

1
2
,
1
2

. Por lo tanto,
\Chi
2
1
= \Gamma

1
2
,
1
2

.
**** Nota Bene
Notar que si X \sim N}(\mu, \sigma
2
), entonces
X{−}\mu
\sigma
\sim N(0, 1), y por lo tanto}

X{−}\mu
\sigma

2
\sim
\Chi
2
1
.
**** Definición 4.2 (Distribución chi-cuadrado). Si U
1
, U
2
, \dots , U
n
son variables aleatorias inde
pendientes, cada una con distribución \Chi}
2
1
, la distribución de V =
P
n
{i=1}
U
i
se llama distribución
chi-cuadrado con n grados de libertad y se denota \Chi
2
n
.
Caracterización de la distribución chi-cuadrado. La distribución \Chi
2
n
es un caso par
ticular de la distribución Gamma. Más precisamente,
\Chi
2
n
= \Gamma

n
2
,
1
2

.
Basta recordar que la suma de variables \Gamma i.i.d. también es \Gamma. En particular, la función
densidad de V es
f
V
(v) =
(1 / 2)
n
2
\Gamma

n
2

v
n
2
−{1}
e
−
1
2
v
1\{v > 0}\}.
**** Nota Bene
La distribución \Chi
2
n
no es simétrica.
0 5 10 15 20 25
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
Figura 8: Gráfico de la función densidad de probabilidad de la distribución \Chi}
2
7
.
20
** t de Student
**** Definición 4.3 (La distribución t de Student). Sean Z y U variables aleatorias independientes
con d ist ribuc iones N(0, 1) y \Chi}
2
n
, respectivamente. La distribución de la variable
T =}
Z
p
U/n
se llama distribución t de Student con n grados de libertad y se denota mediante t_n
.
La función densidad de la t de Student con n grados de libertad es
f
T
(t) =
\Gamma

{n+1}
2

\sqrt{}
n\pi{\Gamma}

n
2


1 +
t
2
n

−
{n+1}
2
.
La fórmula de la densidad se obtiene por los métodos estándar desarrollados en las notas
sobre transformaciones de variables.
−5 −4 −3 −2 −1 0 1 2 3 4 5
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
Figura 9: Comparación de la función densidad de probabilidad de una distribución t
7
(línea
solida) con la de la distribución N(0, 1) (línea punteada).
**** Observación 4.4. Notar que la densidad de t}
n
es simétrica respecto del origen. Cuando la
cantidad de grados de libertad, n , es grande la distribución t_n
se aproxima a la la distribución
N(0, 1); de hecho para más de 20 o 30 grados de libertad, las distribuciones son muy cercanas.
.
** F de Fisher
**** Definición 4.5 (Distribución F). Sean U y V variables aleatorias independientes con dis-
tribuciones \Chi}
2
m
y \Chi}
2
n
, respectivamente. La distribución de la variable
W =}
U/m
V/n
21
se llama distribución F con m y n grados de libertad y se denot a por F}
m, n
.
La función densidad de W es
f
W
(w) =
\Gamma

m{+}n
2

\Gamma

m
2

\Gamma

n
2


m
n

m
2
w
m
2
−{1}

1 +
m
n
w

−
m{+}n
2
1\{w \geq 0\}.
W es el cociente de dos variables aleatorias independientes, y su densidad se obtiene usando}
los métodos estándar desarrollados en las notas sobre transformaciones de variables.
**** Nota Bene
Se puede mostrar que, para n > 2, E[W ] = n/(n − 2). De las definiciones de}
las distribuciones t y F , se deduce que el cuadrado de una variable aleatoria t_n
se distribuye
como una F}
1,n}
.
0 1 2 3 4 5 6 7
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figura 10: Gráfico típico de la función densidad de probabilidad de una distribución F .
¿Cómo usar las tablas de las distribuciones F ? Para cada \alpha \in (0, 1), sea F
\alpha,m,n
el
punto del semieje positivo de las abscisas a cuya derecha la distribución F}
m,n
acumula una
probabilidad \alpha}:
\mathbb{P}(F
m,n
> F
\alpha,m,n
) = \alpha.
**** Observación 4.6. Notar que de las igualdades}
\alpha = P}

U/m
V/n
> F
\alpha,m,n

= P

V/n
U/m
<
1
F
\alpha,m,n

= 1 − P}

V/n
U/m
\geq
1
F
\alpha,m,n

se deduce que
F
1{−{\alpha,n,m
=
1
F
\alpha,m,n
. (31)
22
En los manuales de estadística se pueden consultar las tablas de los valores F}
\alpha,m,n
para
diferentes valores de m, n y \alpha \in \}0.01, 0.05{\}. Por ejemplo, según la tabla que tengo a mi
disposición
1
\mathbb{P}(F
9, 9
> 3.18) = 0.05 y \mathbb{P}(F
9, 9
> 5.35) = 0.01}
Usando esa información queremos hallar valores φ}
1
y φ}
2
tales que
\mathbb{P}(F
9, 9
> φ
2
) = 0.025 y \mathbb{P}(F}
9, 9
< φ
1
) = 0.025.
El valor de φ}
2
se obtiene por interpolación líneal entre los dos puntos dados en la tabla:
A = (3.18, 0.05) y B = (5.35, 0.01). La ecuación de la rec ta que pasa por ellos es y − 0.01 =}
−
0.04
2.17
(x −} 5.35). En consecuencia, φ}
2
será la solución de la ecuación 0.025 −}0.01 = −}
0.04
2.17
(φ}
2
−
5.35). Esto es, φ}
2
= 4.5362.
El valor de φ}
1
se obtiene observando que la ecuación \mathbb{P}(F}
9, 9
< φ
1
) = 0.025 es equivalente
a la ecuación \mathbb{P}(1{/F}
9, 9
> 1}/φ
1
) = 0.025. Por definición, la distribución de 1{/F}
9, 9
coincide con
la de F}
9, 9
. En consecuencia, φ}
1
debe satisfacer la ecuación \mathbb{P}(F}
9, 9
> 1}/φ
1
) = 0.025. Por lo
tanto, φ}
1
= 1 / 4.5362 = 0.2204.
* Bibliografía consultada
Para redactar estas notas se consultaron los siguientes libros:
1. Cramer, H.: Métodos matemáticos de estadística. Aguilar,
   Madrid. (1970)
2. Durrett R.: Probability. Theory and Examples. Duxbury Press,
   Belmont. (1996)
3. Feller, W.: An introduction to Probability Theory and Its
   Applications. Vol. 1. John Wiley & Sons, New York. (1968)
4. Feller, W.: An introduction to Probability Theory and Its
   Applications. Vol. 2. John Wiley & Sons, New York. (1971)
5. Hoel P. G.: Introducción a la estadística matemática. Ariel,
   Barcelona. (1980)
6. Piskunov, N. : Cálculo diferencial e integral, tomo I. Mir, Moscú
 (1983)
7. Rice, J. A.: Mathematical Statistics and Data Analysis. Duxbury
   Press, Belmont. (1995)
8. Ross, S. M: Introduction to Probability and Statistics for
   Engineers and Scientists. Elsevier Academic Press, San
   Diego. (2004)
9. Ross, S.: Introduction to Probability Mo del s. Academic Press, San Diego. (2007)
10. Introducción a la estadística matemática. Ariel,
    Barcelona. (1980).



 
 
